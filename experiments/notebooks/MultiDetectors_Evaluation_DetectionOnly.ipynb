{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Statistical utilities\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Minimal model support for streaming detectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# River drift detectors (streaming methods)\n",
    "from river.drift import ADWIN\n",
    "from river.drift.binary import DDM, EDDM, FHDDM, HDDM_A, HDDM_W\n",
    "\n",
    "# River datasets for benchmark generation (SEA, Hyperplane, etc.)\n",
    "from river.datasets import synth\n",
    "\n",
    "# Drift detection modules\n",
    "sys.path.insert(0, os.path.abspath('../backup'))\n",
    "from shape_dd import shape, shape_snr_adaptive\n",
    "from d3 import d3\n",
    "from dawidd import dawidd\n",
    "from gen_data import gen_random\n",
    "from mmd import mmd\n",
    "from ow_mmd import mmd_ow, shapedd_ow_mmd, shapedd_ow_mmd_buffer\n",
    "from ks import ks\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Data stream configuration\n",
    "STREAM_SIZE = 2000\n",
    "RANDOM_SEED = 42  # Fixed seed for reproducibility (deprecated - use RANDOM_SEEDS instead)\n",
    "\n",
    "# ============================================================================\n",
    "# RELIABILITY CONFIGURATION (Multiple Independent Runs)\n",
    "# ============================================================================\n",
    "# Statistical validation requires multiple runs with different random seeds\n",
    "# Benchmark standard: 30-500 runs (we use 30 for 80% statistical power)\n",
    "N_RUNS = 10  # Number of independent runs (minimum for statistical validity)\n",
    "RANDOM_SEEDS = [42 + i * 137 for i in range(N_RUNS)]  # Prime spacing avoids correlation\n",
    "\n",
    "\n",
    "# Detection parameters (following run__drift_detection.py style)\n",
    "CHUNK_SIZE = 150        # Detection window size\n",
    "OVERLAP = 100           # Overlap between windows\n",
    "SHAPE_L1 = 50          # ShapeDD reference window\n",
    "SHAPE_L2 = 150         # ShapeDD test window (matches CHUNK_SIZE)\n",
    "SHAPE_N_PERM = 2500    # ShapeDD permutation count\n",
    "COOLDOWN = 75          # Minimum samples between detections\n",
    "\n",
    "# SPECTRA-DRIFT parameters\n",
    "SPECTRA_WINDOW = 500         # Window size for spectral analysis\n",
    "SPECTRA_K = None             # Number of neighbors (None = auto sqrt(window_size))\n",
    "SPECTRA_EIGENVALUES = 10  # Number of eigenvalues to extract\n",
    "SPECTRA_ALPHA = 0.01         # False positive rate (auto-calibrated threshold)\n",
    "\n",
    "# Streaming detector configuration (minimal model support)\n",
    "INITIAL_TRAINING_SIZE = 500    # Initial batch for model training\n",
    "PREQUENTIAL_WINDOW = 100       # Window for prequential accuracy\n",
    "\n",
    "\n",
    "# Window-based methods\n",
    "WINDOW_METHODS = [\n",
    "    'D3',           # Margin density drift detector\n",
    "    'DAWIDD',       # Distance-aware windowed drift detector\n",
    "    'MMD',          # Maximum Mean Discrepancy\n",
    "    'KS',           # Kolmogorov-Smirnov test\n",
    "    'ShapeDD',      # Original method\n",
    "    'ShapeDD_SNR_Adaptive', # SNR-Aware Hybrid\n",
    "    'MMD_OW',       # Optimally-Weighted MMD estimator                  \n",
    "    'ShapeDD_OW_MMD', # ShapeDD + OW-MMD Hybrid\n",
    "]\n",
    "\n",
    "# Streaming methods (require model for accuracy signal)\n",
    "STREAMING_METHODS = [\n",
    "    # 'ADWIN',        # Adaptive Windowing\n",
    "    # 'DDM',          # Drift Detection Method\n",
    "    # 'EDDM',         # Early Drift Detection Method\n",
    "    # 'HDDM_A',       # Hoeffding's Drift Detection Method (Average)\n",
    "    # 'HDDM_W',       # Hoeffding's Drift Detection Method (Weighted)\n",
    "]\n",
    "\n",
    "\n",
    "DATASET_CATALOG = {\n",
    "    # ========================================================================\n",
    "    # SUDDEN DRIFT DATASETS (8 datasets)\n",
    "    # Classic benchmarks with abrupt concept switches\n",
    "    # ground_truth_type: \"exact\" = known positions, \"estimated\" = heuristic, \"none\" = unknown\n",
    "    # ========================================================================\n",
    "    \"standard_sea\": {\n",
    "        \"enabled\": True,\n",
    "        \"type\": \"standard_sea\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"ground_truth_type\": \"exact\",  # Synthetic - exact drift positions known\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"enhanced_sea\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"enhanced_sea\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"scale_factors\": (1.8, 1.5, 2.0),\n",
    "            \"shift_amounts\": (5.0, 4.0, 8.0)\n",
    "        }\n",
    "    },\n",
    "    \"stagger\": {\n",
    "        \"enabled\": True,\n",
    "        \"type\": \"stagger\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"ground_truth_type\": \"exact\",  # Synthetic - exact drift positions known\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"hyperplane\": {\n",
    "        \"enabled\": True,\n",
    "        \"type\": \"hyperplane\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"ground_truth_type\": \"exact\",  # Synthetic - exact drift positions known\n",
    "        \"params\": {\n",
    "            \"n_features\": 3\n",
    "        }\n",
    "    },\n",
    "    \"gen_random_mild\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"gen_random\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"dims\": 5,\n",
    "            \"intens\": 0.125,\n",
    "            \"dist\": \"unif\",\n",
    "            \"alt\": False\n",
    "        }\n",
    "    },\n",
    "    \"gen_random_moderate\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"gen_random\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"dims\": 5,\n",
    "            \"intens\": 0.25,\n",
    "            \"dist\": \"unif\",\n",
    "            \"alt\": False\n",
    "        }\n",
    "    },\n",
    "    \"gen_random_severe\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"gen_random\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"dims\": 5,\n",
    "            \"intens\": 1,\n",
    "            \"dist\": \"unif\",\n",
    "            \"alt\": True\n",
    "        }\n",
    "    },\n",
    "    \"gen_random_ultra_severe\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"gen_random\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"dims\": 5,\n",
    "            \"intens\": 2,\n",
    "            \"dist\": \"unif\",\n",
    "            \"alt\": True\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GRADUAL DRIFT DATASETS (4 datasets)\n",
    "    # Smooth blending transitions between concepts\n",
    "    # ========================================================================\n",
    "    \n",
    "    \"sea_gradual\": {\n",
    "        \"enabled\": True,  # ENABLED: Representative gradual drift dataset\n",
    "        \"type\": \"sea_gradual\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"ground_truth_type\": \"exact\",  # Synthetic - exact transition start positions known\n",
    "        \"params\": {\n",
    "            \"transition_width\": 450  # OPTIMIZED: 50% of segment (909 samples)\n",
    "        }\n",
    "    },\n",
    "    \"hyperplane_gradual\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"hyperplane_gradual\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"n_features\": 10  # Continuous drift (no discrete transition)\n",
    "        }\n",
    "    },\n",
    "    \"agrawal_gradual\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"agrawal_gradual\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"transition_width\": 450  # OPTIMIZED: 50% of segment (909 samples)\n",
    "        }\n",
    "    },\n",
    "    \"circles_gradual\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"circles_gradual\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"transition_width\": 400  # OPTIMIZED: 44% of segment (more stable)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # INCREMENTAL DRIFT DATASETS (2 datasets) - MOA Standard\n",
    "    # Continuous cluster boundary movement\n",
    "    # ========================================================================\n",
    "    \n",
    "    \"rbf_slow\": {\n",
    "        \"enabled\": True,  # ENABLED: Representative incremental drift dataset\n",
    "        \"type\": \"rbf\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"ground_truth_type\": \"estimated\",  # Continuous drift - positions are estimates\n",
    "        \"params\": {\n",
    "            \"n_centroids\": 50,    # MOA standard\n",
    "            \"speed\": 0.0001       # Slow continuous drift\n",
    "        }\n",
    "    },\n",
    "    \"rbf_fast\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"rbf\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"n_centroids\": 50,    # MOA standard\n",
    "            \"speed\": 0.001        # Fast continuous drift (10× faster)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # REAL-WORLD DATASETS (1 dataset)\n",
    "    # Natural concept drift from real-world processes\n",
    "    # NO GROUND TRUTH - Use for qualitative analysis only\n",
    "    # ========================================================================\n",
    "    \n",
    "    \"electricity\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"electricity\",\n",
    "        \"n_drift_events\": 5,      # Estimated (no ground truth available)\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"electricity_sorted\": {\n",
    "        \"enabled\": True,\n",
    "        \"type\": \"electricity_sorted\",\n",
    "        \"n_drift_events\": 5,\n",
    "        \"ground_truth_type\": \"estimated\",  # Semi-real: drift positions are heuristic\n",
    "        \"params\": {\n",
    "            \"sort_feature\": \"nswdemand\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SEMI-REAL DATASETS - Controlled Drift from Real Data\n",
    "    # Real-world features with known drift positions (sorted by feature)\n",
    "    # ========================================================================\n",
    "    \n",
    "    \"covertype_sorted\": {\n",
    "        \"enabled\": False,  # DISABLED: Covtype not available in river library\n",
    "        \"type\": \"covertype_sorted\",\n",
    "        \"n_drift_events\": 5,      # Controllable - adjust as needed\n",
    "        \"ground_truth_type\": \"estimated\",  # Semi-real: drift positions are heuristic\n",
    "        \"params\": {\n",
    "            \"sort_feature\": \"Elevation\"  # Creates natural drift by terrain\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STATIONARY DATASETS (2 datasets) - False Positive Analysis\n",
    "    # No drift - for statistical calibration validation\n",
    "    # ========================================================================\n",
    "    \n",
    "    \"stagger_none\": {\n",
    "        \"enabled\": True,  # ENABLED: False positive calibration (no drift baseline)\n",
    "        \"type\": \"stagger\",\n",
    "        \"n_drift_events\": 0,      # NO DRIFT - stationary\n",
    "        \"ground_truth_type\": \"exact\",  # Synthetic - known to have NO drift\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"gen_random_none\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"gen_random\",\n",
    "        \"n_drift_events\": 0,      # NO DRIFT - stationary\n",
    "        \"params\": {\n",
    "            \"dims\": 5,\n",
    "            \"intens\": 0,          # Zero intensity = no drift\n",
    "            \"dist\": \"unif\",\n",
    "            \"alt\": False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SINE FAMILY: Classification Reversal + Noise Robustness Tests\n",
    "    # ========================================================================\n",
    "    \"sine1\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"sine1\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"sine2\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"sine2\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"sinirrel1\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"sinirrel1\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"sinirrel2\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"sinirrel2\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {}\n",
    "    },\n",
    "\n",
    "    # ========================================================================\n",
    "    # RBF AND LED: Complex Distributions\n",
    "    # ========================================================================\n",
    "    \"rbfblips\": {\n",
    "        \"enabled\": True,\n",
    "        \"type\": \"rbfblips\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"n_centroids\": 50,\n",
    "            \"n_features\": 10\n",
    "        }\n",
    "    },\n",
    "    \"led_abrupt\": {\n",
    "        \"enabled\": False,\n",
    "        \"type\": \"led_abrupt\",\n",
    "        \"n_drift_events\": 10,\n",
    "        \"params\": {\n",
    "            \"has_noise\": False\n",
    "        }\n",
    "    },\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "enabled_datasets = [k for k, v in DATASET_CATALOG.items() if v['enabled']]\n",
    "sudden_datasets = [k for k in enabled_datasets if 'gradual' not in k and 'rbf' not in k and 'electricity' not in k and 'covertype' not in k and 'none' not in k]\n",
    "gradual_datasets = [k for k in enabled_datasets if 'gradual' in k]\n",
    "incremental_datasets = [k for k in enabled_datasets if 'rbf' in k]\n",
    "realworld_datasets = [k for k in enabled_datasets if 'electricity' in k or 'covertype' in k]\n",
    "stationary_datasets = [k for k in enabled_datasets if 'none' in k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: DATASET GENERATION (Multiple Benchmark Datasets)\n",
    "# ============================================================================\n",
    "\n",
    "def generate_standard_sea_stream(total_size, n_drift_events, seed=42):\n",
    "    \"\"\"\n",
    "    Standard SEA benchmark with multiple drifts.\n",
    "    Creates sudden drifts by switching between SEA variants.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    variants = [0, 1, 2, 3]  # SEA has 4 variants\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "\n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "        variant = variants[seg_idx % len(variants)]\n",
    "\n",
    "        stream = synth.SEA(seed=seed + seg_idx * 100, variant=variant)\n",
    "        for i, (x, y) in enumerate(stream.take(size)):\n",
    "            X_list.append(list(x.values()))\n",
    "            y_list.append(y)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_enhanced_sea_stream(total_size, n_drift_events, seed=42,\n",
    "                                  scale_factors=(1.8, 1.5, 2.0),\n",
    "                                  shift_amounts=(5.0, 4.0, 8.0)):\n",
    "    \"\"\"Enhanced SEA with multiple drifts and transformations.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    variants = [0, 1, 2, 3]\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "\n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "        variant = variants[seg_idx % len(variants)]\n",
    "\n",
    "        stream = synth.SEA(seed=seed + seg_idx * 100, variant=variant)\n",
    "\n",
    "        for i, (x, y) in enumerate(stream.take(size)):\n",
    "            x_vals = list(x.values())\n",
    "\n",
    "            # Apply transformations to alternate segments\n",
    "            if seg_idx % 2 == 1:\n",
    "                x_vals = [x_vals[j] * scale_factors[j] + shift_amounts[j]\n",
    "                         for j in range(len(x_vals))]\n",
    "\n",
    "            X_list.append(x_vals)\n",
    "            y_list.append(y)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_stagger_stream(total_size, n_drift_events, seed=42):\n",
    "    \"\"\"STAGGER concepts with multiple sudden drifts.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "\n",
    "    X_segments, y_segments = [], []\n",
    "\n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "\n",
    "        X_seg = np.random.randn(size, 5)\n",
    "\n",
    "        # Different concepts for each segment\n",
    "        if seg_idx % 3 == 0:\n",
    "            X_seg[:, 0] += 2.0\n",
    "            y_seg = (X_seg[:, 0] + X_seg[:, 1] > 1.5).astype(int)\n",
    "        elif seg_idx % 3 == 1:\n",
    "            X_seg[:, 0] -= 2.0\n",
    "            y_seg = (X_seg[:, 0] * X_seg[:, 1] > 0).astype(int)\n",
    "        else:\n",
    "            X_seg[:, 1] += 1.5\n",
    "            y_seg = (X_seg[:, 1] + X_seg[:, 2] > 0.5).astype(int)\n",
    "\n",
    "        X_segments.append(X_seg)\n",
    "        y_segments.append(y_seg)\n",
    "\n",
    "    X = np.vstack(X_segments)\n",
    "    y = np.hstack(y_segments)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_hyperplane_stream(total_size, n_drift_events, seed=42, n_features=10):\n",
    "    \"\"\"Rotating Hyperplane with multiple drifts.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "\n",
    "    mag_changes = [0.0001, 0.01, 0.005, 0.015]  # Alternate rotation speeds\n",
    "\n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "        mag_change = mag_changes[seg_idx % len(mag_changes)]\n",
    "\n",
    "        stream = synth.Hyperplane(seed=seed + seg_idx * 100, n_features=n_features,\n",
    "                                   n_drift_features=2, mag_change=mag_change,\n",
    "                                   noise_percentage=0.05)\n",
    "\n",
    "        for i, (x, y) in enumerate(stream.take(size)):\n",
    "            X_list.append(list(x.values()))\n",
    "            y_list.append(y)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_genrandom_stream(total_size, n_drift_events, seed=42,\n",
    "                               dims=5, intens=0.125, dist=\"unif\", alt=False):\n",
    "    \"\"\"Custom synthetic data using gen_random with multiple drifts.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X, y_drift_labels = gen_random(\n",
    "        number=n_drift_events,\n",
    "        dims=dims,\n",
    "        intens=intens,\n",
    "        dist=dist,\n",
    "        alt=alt,\n",
    "        length=total_size\n",
    "    )\n",
    "\n",
    "    # Find actual drift positions\n",
    "    drift_indices = np.where(np.diff(y_drift_labels) != 0)[0] + 1\n",
    "    drift_positions = drift_indices.tolist()\n",
    "\n",
    "    # Generate synthetic binary classification labels\n",
    "    # Use simple threshold on first feature\n",
    "    y = (X[:, 0] > np.median(X[:, 0])).astype(int)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# GRADUAL DRIFT DATASET GENERATORS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_sea_gradual_stream(total_size, n_drift_events, seed=42, transition_width=1000):\n",
    "    \"\"\"\n",
    "    SEA benchmark with GRADUAL drifts (smooth transitions between variants).\n",
    "    \n",
    "    Instead of instant variant switches, blends samples from old→new concept\n",
    "    over transition_width samples.\n",
    "    \n",
    "    Args:\n",
    "        transition_width: Number of samples for gradual transition (default: 1000)\n",
    "                         During transition, samples are blended: \n",
    "                         Start: 100% old, 0% new → End: 0% old, 100% new\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    variants = [0, 1, 2, 3]\n",
    "\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    # Generate segments with gradual transitions\n",
    "    for seg_idx in range(n_drift_events + 1):\n",
    "        old_variant = variants[seg_idx % len(variants)]\n",
    "        new_variant = variants[(seg_idx + 1) % len(variants)]\n",
    "        \n",
    "        if seg_idx == 0:\n",
    "            # First segment - no transition at start\n",
    "            stream = synth.SEA(seed=seed + seg_idx * 100, variant=old_variant)\n",
    "            for i, (x, y) in enumerate(stream.take(segment_size)):\n",
    "                X_list.append(list(x.values()))\n",
    "                y_list.append(y)\n",
    "        else:\n",
    "            # Gradual transition segment\n",
    "            transition_start = len(X_list)\n",
    "            \n",
    "            # Generate samples from both concepts\n",
    "            stream_old = synth.SEA(seed=seed + seg_idx * 100, variant=old_variant)\n",
    "            stream_new = synth.SEA(seed=seed + (seg_idx + 1) * 100, variant=new_variant)\n",
    "            \n",
    "            samples_old = list(stream_old.take(segment_size))\n",
    "            samples_new = list(stream_new.take(segment_size))\n",
    "            \n",
    "            for i in range(segment_size):\n",
    "                # Calculate blend ratio (linear interpolation)\n",
    "                if i < transition_width:\n",
    "                    # Gradual transition: old → new\n",
    "                    alpha = i / transition_width  # 0 → 1\n",
    "                    \n",
    "                    x_old = np.array(list(samples_old[i][0].values()))\n",
    "                    x_new = np.array(list(samples_new[i][0].values()))\n",
    "                    y_old = samples_old[i][1]\n",
    "                    y_new = samples_new[i][1]\n",
    "                    \n",
    "                    # Blend features\n",
    "                    x_blended = (1 - alpha) * x_old + alpha * x_new\n",
    "                    \n",
    "                    # Blend labels probabilistically\n",
    "                    if np.random.rand() < alpha:\n",
    "                        y_blended = y_new\n",
    "                    else:\n",
    "                        y_blended = y_old\n",
    "                    \n",
    "                    X_list.append(x_blended.tolist())\n",
    "                    y_list.append(y_blended)\n",
    "                else:\n",
    "                    # After transition - pure new concept\n",
    "                    X_list.append(list(samples_new[i][0].values()))\n",
    "                    y_list.append(samples_new[i][1])\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_hyperplane_gradual_stream(total_size, n_drift_events, seed=42, n_features=10):\n",
    "    \"\"\"\n",
    "    Rotating Hyperplane with CONTINUOUS gradual drift.\n",
    "    \n",
    "    Uses very small mag_change to create smooth, continuous rotation\n",
    "    instead of sudden jumps between segments.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    # Use VERY SMALL mag_change for gradual rotation\n",
    "    # This creates continuous drift throughout the stream\n",
    "    stream = synth.Hyperplane(\n",
    "        seed=seed, \n",
    "        n_features=n_features,\n",
    "        n_drift_features=5,  # More features drifting for observable change\n",
    "        mag_change=0.0001,   # VERY small = gradual\n",
    "        sigma=0.1,           # Small noise\n",
    "        noise_percentage=0.05\n",
    "    )\n",
    "\n",
    "    # Generate full stream (drift happens continuously)\n",
    "    for i, (x, y) in enumerate(stream.take(total_size)):\n",
    "        X_list.append(list(x.values()))\n",
    "        y_list.append(y)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    # Estimate drift positions (evenly spaced)\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_agrawal_gradual_stream(total_size, n_drift_events, seed=42, transition_width=1000):\n",
    "    \"\"\"\n",
    "    Agrawal generator with GRADUAL transitions between classification functions.\n",
    "    \n",
    "    Agrawal has 10 different classification functions. We gradually blend\n",
    "    between them over transition_width samples.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    # Cycle through classification functions (0-9)\n",
    "    for seg_idx in range(n_drift_events + 1):\n",
    "        old_func = seg_idx % 10\n",
    "        new_func = (seg_idx + 1) % 10\n",
    "        \n",
    "        if seg_idx == 0:\n",
    "            # First segment - no transition\n",
    "            stream = synth.Agrawal(seed=seed + seg_idx * 100, classification_function=old_func)\n",
    "            for i, (x, y) in enumerate(stream.take(segment_size)):\n",
    "                X_list.append(list(x.values()))\n",
    "                y_list.append(y)\n",
    "        else:\n",
    "            # Gradual transition segment\n",
    "            stream_old = synth.Agrawal(seed=seed + seg_idx * 100, classification_function=old_func)\n",
    "            stream_new = synth.Agrawal(seed=seed + (seg_idx + 1) * 100, classification_function=new_func)\n",
    "            \n",
    "            samples_old = list(stream_old.take(segment_size))\n",
    "            samples_new = list(stream_new.take(segment_size))\n",
    "            \n",
    "            for i in range(segment_size):\n",
    "                if i < transition_width:\n",
    "                    # Gradual transition via probabilistic label selection\n",
    "                    alpha = i / transition_width\n",
    "                    \n",
    "                    # Use old sample's features but blend labels probabilistically\n",
    "                    x_features = list(samples_old[i][0].values())\n",
    "                    y_old = samples_old[i][1]\n",
    "                    y_new = samples_new[i][1]\n",
    "                    \n",
    "                    # Blend labels probabilistically\n",
    "                    if np.random.rand() < alpha:\n",
    "                        y_blended = y_new\n",
    "                    else:\n",
    "                        y_blended = y_old\n",
    "                    \n",
    "                    X_list.append(x_features)\n",
    "                    y_list.append(y_blended)\n",
    "                else:\n",
    "                    # Pure new concept\n",
    "                    X_list.append(list(samples_new[i][0].values()))\n",
    "                    y_list.append(samples_new[i][1])\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_circles_gradual_stream(total_size, n_drift_events, seed=42, transition_width=500):\n",
    "    \"\"\"\n",
    "    Circles dataset with GRADUAL drifts (circles move/resize smoothly).\n",
    "    \n",
    "    Classic synthetic benchmark: 2D data with circular decision boundaries\n",
    "    that gradually move and resize over transition windows.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    # Define circle configurations (center_x, center_y, radius)\n",
    "    circles = [\n",
    "        (0.25, 0.25, 0.15),  # Circle 1\n",
    "        (0.75, 0.25, 0.15),  # Circle 2\n",
    "        (0.25, 0.75, 0.15),  # Circle 3\n",
    "        (0.75, 0.75, 0.15),  # Circle 4\n",
    "    ]\n",
    "    \n",
    "    for seg_idx in range(n_drift_events + 1):\n",
    "        # Get current and next circle configuration\n",
    "        old_circle = circles[seg_idx % len(circles)]\n",
    "        new_circle = circles[(seg_idx + 1) % len(circles)]\n",
    "        \n",
    "        for i in range(segment_size):\n",
    "            # Generate random point in [0, 1] × [0, 1]\n",
    "            x = np.random.rand()\n",
    "            y = np.random.rand()\n",
    "            \n",
    "            if seg_idx == 0 or i >= transition_width:\n",
    "                # Use current circle (before transition or after transition complete)\n",
    "                if seg_idx == 0:\n",
    "                    cx, cy, r = old_circle\n",
    "                else:\n",
    "                    cx, cy, r = new_circle\n",
    "            else:\n",
    "                # Gradual transition - interpolate circle parameters\n",
    "                alpha = i / transition_width\n",
    "                \n",
    "                cx = (1 - alpha) * old_circle[0] + alpha * new_circle[0]\n",
    "                cy = (1 - alpha) * old_circle[1] + alpha * new_circle[1]\n",
    "                r = (1 - alpha) * old_circle[2] + alpha * new_circle[2]\n",
    "            \n",
    "            # Classification: inside circle = class 1, outside = class 0\n",
    "            distance = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
    "            label = 1 if distance <= r else 0\n",
    "            \n",
    "            X_list.append([x, y])\n",
    "            y_list.append(label)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INCREMENTAL DRIFT DATASET GENERATORS (NEW - Following MOA/River Standards)\n",
    "# ============================================================================\n",
    "\n",
    "def generate_rbf_stream(total_size, n_drift_events, seed=42, n_centroids=50, speed=0.0001):\n",
    "    \"\"\"\n",
    "    Random RBF with moving centroids (INCREMENTAL/CONTINUOUS drift).\n",
    "    \n",
    "    STANDARD CONFIGURATION (following MOA papers):\n",
    "    - 50 centroids (standard benchmark setting)\n",
    "    - 10 features\n",
    "    - 2 classes\n",
    "    - Speed: 0.0001 (slow), 0.001 (moderate), 0.01 (fast)\n",
    "    - All centroids drift continuously\n",
    "    \n",
    "    This simulates INCREMENTAL drift where cluster boundaries move\n",
    "    continuously over time (different from sudden concept switches).\n",
    "    \n",
    "    Reference:\n",
    "        \"MOA: Massive Online Analysis\" (Bifet et al., 2010)\n",
    "        Standard RBF generator configuration for drift benchmarking\n",
    "    \n",
    "    Args:\n",
    "        speed: Drift speed (0.0001=slow, 0.001=moderate, 0.01=fast)\n",
    "        n_centroids: Number of RBF centroids (default: 50, MOA standard)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    # STANDARD MOA CONFIGURATION\n",
    "    # 50 centroids, 10 features, all centroids drift\n",
    "    stream = synth.RandomRBFDrift(\n",
    "        seed_model=seed,\n",
    "        seed_sample=seed + 1000,\n",
    "        n_classes=2,                    # Binary classification (standard)\n",
    "        n_features=10,                  # 10 features (MOA standard)\n",
    "        n_centroids=n_centroids,        # 50 centroids (standard)\n",
    "        change_speed=speed,             # Drift speed parameter\n",
    "        n_drift_centroids=n_centroids   # ALL centroids drift (maximum drift)\n",
    "    )\n",
    "    \n",
    "    # Generate stream\n",
    "    for i, (x, y) in enumerate(stream.take(total_size)):\n",
    "        X_list.append(list(x.values()))\n",
    "        y_list.append(y)\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    # Estimate drift positions (continuous drift, so evenly spaced markers)\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_electricity_stream(total_size, n_drift_events, seed=42):\n",
    "    \"\"\"\n",
    "    Electricity (Elec2) - Real-world electricity price prediction dataset.\n",
    "    \n",
    "    REAL-WORLD DRIFT BENCHMARK (most cited in literature):\n",
    "    - 45,312 total instances (Australian NSW Electricity Market, 1996-1998)\n",
    "    - Binary classification: Price UP or DOWN\n",
    "    - 7 features: day, time, demand, supply indicators\n",
    "    - Natural concept drift from market expansion\n",
    "    - 30-minute intervals (48 instances per day)\n",
    "    \n",
    "    We don't know exactly when/where drifts occur in this real-world data.\n",
    "    \n",
    "    Use for:\n",
    "    - Qualitative validation (detection patterns, stability, false positives)\n",
    "    - Real-world robustness testing\n",
    "    \n",
    "    Do NOT use for:\n",
    "    - Quantitative F1/MTTD metrics (no ground truth)\n",
    "    \n",
    "    Reference:\n",
    "        \"How good is the Electricity benchmark for evaluating concept drift adaptation\"\n",
    "        (Harries, 1999; used in 500+ papers)\n",
    "    \n",
    "    Args:\n",
    "        total_size: Number of samples to extract (default: 10000)\n",
    "        n_drift_events: Estimated number of drift events (for compatibility)\n",
    "    \"\"\"\n",
    "    from river.datasets import Elec2\n",
    "    \n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    # Load Elec2 from River\n",
    "    stream = Elec2()\n",
    "    \n",
    "    # Extract first total_size samples\n",
    "    for i, (x, y) in enumerate(stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        X_list.append(list(x.values()))\n",
    "        y_list.append(1 if y == 'UP' else 0)  # Convert UP/DOWN to 1/0\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    # ⚠️ NO GROUND TRUTH - Estimate drift positions (heuristic only)\n",
    "    # Literature suggests drift from market expansion, but exact locations unknown\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    return X, y, drift_positions\n",
    "\n",
    "def generate_electricity_sorted_stream(total_size, n_drift_events, seed=42, sort_feature=\"nswdemand\"):\n",
    "    \"\"\"\n",
    "    Electricity (Elec2) - SEMI-REAL dataset with CONTROLLED drift.\n",
    "    \n",
    "    Strategy: Sort by demand/price to create natural concept drift as\n",
    "    market behavior changes with load levels (real economic phenomenon).\n",
    "    \n",
    "    DATASET INFO:\n",
    "    - 45,312 total instances (Australian NSW Electricity Market)\n",
    "    - 7 features: day, period, nswprice, nswdemand, vicprice, vicdemand, transfer\n",
    "    - Binary classification: Price UP or DOWN\n",
    "    - Sorting by demand creates natural drift (different pricing at different loads)\n",
    "    \n",
    "    Reference:\n",
    "        Harries (1999), used in 500+ concept drift papers\n",
    "    \n",
    "    Args:\n",
    "        total_size: Number of samples to extract\n",
    "        n_drift_events: Number of drift events (controllable!)\n",
    "        seed: Random seed\n",
    "        sort_feature: Feature to sort by (default: nswdemand)\n",
    "    \"\"\"\n",
    "    from river.datasets import Elec2\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Feature name mapping for Elec2\n",
    "    feature_names = ['day', 'period', 'nswprice', 'nswdemand', 'vicprice', 'vicdemand', 'transfer']\n",
    "    \n",
    "    # Load Elec2\n",
    "    data = []\n",
    "    for i, (x, y) in enumerate(Elec2()):\n",
    "        if i >= min(total_size * 2, 45312):  # Buffer for sampling, max is full dataset\n",
    "            break\n",
    "        row = list(x.values())\n",
    "        # Get sort feature value\n",
    "        sort_idx = feature_names.index(sort_feature) if sort_feature in feature_names else 3  # default to nswdemand\n",
    "        sort_val = row[sort_idx]\n",
    "        label = 1 if y == 'UP' else 0\n",
    "        data.append((sort_val, row, label))\n",
    "    \n",
    "    # Sort by the chosen feature to create controlled drift\n",
    "    data.sort(key=lambda t: t[0])\n",
    "    \n",
    "    # Sample evenly to get exact total_size\n",
    "    if len(data) > total_size:\n",
    "        step = len(data) // total_size\n",
    "        sampled = [data[i * step] for i in range(total_size)]\n",
    "    else:\n",
    "        sampled = data[:total_size]\n",
    "    \n",
    "    X = np.array([s[1] for s in sampled])\n",
    "    y = np.array([s[2] for s in sampled])\n",
    "    \n",
    "    # Calculate drift positions (concept changes at feature boundaries)\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    return X, y, drift_positions\n",
    "\n",
    "def generate_covertype_sorted_stream(total_size, n_drift_events, seed=42, sort_feature=\"Elevation\"):\n",
    "    \"\"\"\n",
    "    Forest Covertype - SEMI-REAL dataset with CONTROLLED drift.\n",
    "    \n",
    "    Strategy: Sort by elevation to create natural concept drift as\n",
    "    forest type changes with altitude (real ecological phenomenon).\n",
    "    \n",
    "    This is the standard \"semi-real\" benchmark approach - real data\n",
    "    reordered to create known drift positions.\n",
    "    \n",
    "    DATASET INFO:\n",
    "    - 581,012 total instances (Covertype from UCI)\n",
    "    - 54 features (10 numerical + 44 binary wilderness/soil type)\n",
    "    - 7 forest cover types → converted to binary\n",
    "    - Sorting by elevation creates natural concept drift\n",
    "    \n",
    "    Reference:\n",
    "        Blackard & Dean (1999), UCI ML Repository\n",
    "        Used in 200+ concept drift papers\n",
    "    \n",
    "    Args:\n",
    "        total_size: Number of samples to extract\n",
    "        n_drift_events: Number of drift events (controllable!)\n",
    "        seed: Random seed\n",
    "        sort_feature: Feature to sort by (default: Elevation)\n",
    "    \"\"\"\n",
    "    from river.datasets import Covtype\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Load Covertype dataset (not pre-normalized, we'll normalize later)\n",
    "    data = []\n",
    "    for i, (x, y) in enumerate(Covtype()):\n",
    "        if i >= min(total_size * 3, 100000):  # Buffer for sampling, cap at 100k\n",
    "            break\n",
    "        row = list(x.values())\n",
    "        # Get elevation (first numerical feature in Covertype)\n",
    "        # Feature order: Elevation, Aspect, Slope, etc.\n",
    "        elevation = row[0] if sort_feature == \"Elevation\" else row[1]\n",
    "        data.append((elevation, row, y))\n",
    "    \n",
    "    # Sort by elevation to create controlled drift\n",
    "    data.sort(key=lambda t: t[0])\n",
    "    \n",
    "    # Sample evenly to get exact total_size\n",
    "    if len(data) > total_size:\n",
    "        step = len(data) // total_size\n",
    "        sampled = [data[i * step] for i in range(total_size)]\n",
    "    else:\n",
    "        sampled = data[:total_size]\n",
    "    \n",
    "    X = np.array([s[1] for s in sampled])\n",
    "    # Convert multi-class to binary (cover type 1-2 vs 3-7)\n",
    "    y = np.array([1 if s[2] <= 2 else 0 for s in sampled])\n",
    "    \n",
    "    # Normalize features (Covtype is not pre-normalized)\n",
    "    # Use StandardScaler approach: (x - mean) / std\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0)\n",
    "    X_std[X_std == 0] = 1  # Avoid division by zero for constant features\n",
    "    X = (X - X_mean) / X_std\n",
    "    \n",
    "    # Calculate drift positions (concept changes at elevation boundaries)\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UNIFIED DATASET GENERATOR (Updated with NEW datasets)\n",
    "# ============================================================================\n",
    "\n",
    "def generate_drift_stream(dataset_config, total_size=10000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate drift stream for specified dataset type.\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Classification labels\n",
    "        drift_positions: List of drift point indices\n",
    "        info: Dataset metadata (ALWAYS includes: name, type, n_samples, n_features,\n",
    "              n_drifts, drift_positions, dims, intens, dist)\n",
    "    \"\"\"\n",
    "    dataset_type = dataset_config['type']\n",
    "    n_drift_events = dataset_config['n_drift_events']\n",
    "    params = dataset_config.get('params', {})\n",
    "\n",
    "    print(f\"  Generating {dataset_type} with {n_drift_events} drift events...\")\n",
    "\n",
    "    if dataset_type == \"standard_sea\":\n",
    "        X, y, drift_positions = generate_standard_sea_stream(total_size, n_drift_events, seed)\n",
    "        info = {\n",
    "            'name': 'Standard SEA',\n",
    "            'features': 3,\n",
    "            'dims': 3,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'N/A',\n",
    "            'drift_type': 'sudden'\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"enhanced_sea\":\n",
    "        scale_factors = params.get('scale_factors', (1.8, 1.5, 2.0))\n",
    "        shift_amounts = params.get('shift_amounts', (5.0, 4.0, 8.0))\n",
    "        X, y, drift_positions = generate_enhanced_sea_stream(total_size, n_drift_events, seed,\n",
    "                                                              scale_factors, shift_amounts)\n",
    "        info = {\n",
    "            'name': 'Enhanced SEA',\n",
    "            'features': 3,\n",
    "            'dims': 3,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'N/A',\n",
    "            'drift_type': 'sudden'\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"stagger\":\n",
    "        X, y, drift_positions = generate_stagger_stream(total_size, n_drift_events, seed)\n",
    "        info = {\n",
    "            'name': 'STAGGER',\n",
    "            'features': 5,\n",
    "            'dims': 5,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'N/A',\n",
    "            'drift_type': 'sudden'\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"hyperplane\":\n",
    "        n_features = params.get('n_features', 10)\n",
    "        X, y, drift_positions = generate_hyperplane_stream(total_size, n_drift_events, seed, n_features)\n",
    "        info = {\n",
    "            'name': 'Hyperplane',\n",
    "            'features': n_features,\n",
    "            'dims': n_features,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'N/A',\n",
    "            'drift_type': 'sudden'\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"gen_random\":\n",
    "        dims = params.get('dims', 5)\n",
    "        intens = params.get('intens', 0.125)\n",
    "        dist = params.get('dist', 'unif')\n",
    "        alt = params.get('alt', False)\n",
    "        X, y, drift_positions = generate_genrandom_stream(total_size, n_drift_events, seed,\n",
    "                                                          dims, intens, dist, alt)\n",
    "        info = {\n",
    "            'name': 'gen_random',\n",
    "            'features': dims,\n",
    "            'dims': dims,\n",
    "            'intens': intens,\n",
    "            'dist': dist,\n",
    "            'drift_type': 'sudden'\n",
    "        }\n",
    "\n",
    "    # ========================================================================\n",
    "    # GRADUAL DRIFT DATASETS\n",
    "    # ========================================================================\n",
    "    \n",
    "    elif dataset_type == \"sea_gradual\":\n",
    "        transition_width = params.get('transition_width', 1000)\n",
    "        X, y, drift_positions = generate_sea_gradual_stream(total_size, n_drift_events, seed, transition_width)\n",
    "        info = {\n",
    "            'name': 'SEA Gradual',\n",
    "            'features': 3,\n",
    "            'dims': 3,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'N/A',\n",
    "            'drift_type': 'gradual',\n",
    "            'transition_width': transition_width\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"hyperplane_gradual\":\n",
    "        n_features = params.get('n_features', 10)\n",
    "        X, y, drift_positions = generate_hyperplane_gradual_stream(total_size, n_drift_events, seed, n_features)\n",
    "        info = {\n",
    "            'name': 'Hyperplane Gradual',\n",
    "            'features': n_features,\n",
    "            'dims': n_features,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'N/A',\n",
    "            'drift_type': 'gradual',\n",
    "            'transition_width': 'continuous'\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"agrawal_gradual\":\n",
    "        transition_width = params.get('transition_width', 1000)\n",
    "        X, y, drift_positions = generate_agrawal_gradual_stream(total_size, n_drift_events, seed, transition_width)\n",
    "        info = {\n",
    "            'name': 'Agrawal Gradual',\n",
    "            'features': 9,\n",
    "            'dims': 9,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'N/A',\n",
    "            'drift_type': 'gradual',\n",
    "            'transition_width': transition_width\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"circles_gradual\":\n",
    "        transition_width = params.get('transition_width', 500)\n",
    "        X, y, drift_positions = generate_circles_gradual_stream(total_size, n_drift_events, seed, transition_width)\n",
    "        info = {\n",
    "            'name': 'Circles Gradual',\n",
    "            'features': 2,\n",
    "            'dims': 2,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'N/A',\n",
    "            'drift_type': 'gradual',\n",
    "            'transition_width': transition_width\n",
    "        }\n",
    "\n",
    "    # ========================================================================\n",
    "    # INCREMENTAL DRIFT DATASETS (NEW)\n",
    "    # ========================================================================\n",
    "    \n",
    "    elif dataset_type == \"rbf\":\n",
    "        n_centroids = params.get('n_centroids', 50)\n",
    "        speed = params.get('speed', 0.0001)\n",
    "        X, y, drift_positions = generate_rbf_stream(total_size, n_drift_events, seed, n_centroids, speed)\n",
    "        info = {\n",
    "            'name': f'RBF (speed={speed})',\n",
    "            'features': 10,\n",
    "            'dims': 10,\n",
    "            'intens': f'speed={speed}',\n",
    "            'dist': 'RBF',\n",
    "            'drift_type': 'incremental',\n",
    "            'speed': speed,\n",
    "            'n_centroids': n_centroids\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"electricity\":\n",
    "        X, y, drift_positions = generate_electricity_stream(total_size, n_drift_events, seed)\n",
    "        info = {\n",
    "            'name': 'Electricity (Elec2)',\n",
    "            'features': 7,\n",
    "            'dims': 7,\n",
    "            'intens': 'N/A (real-world)',\n",
    "            'dist': 'Real-world',\n",
    "            'drift_type': 'real-world',\n",
    "            'has_ground_truth': False\n",
    "        }\n",
    "\n",
    "\n",
    "    elif dataset_type == \"electricity_sorted\":\n",
    "        sort_feature = params.get('sort_feature', 'nswdemand')\n",
    "        X, y, drift_positions = generate_electricity_sorted_stream(\n",
    "            total_size, n_drift_events, seed, sort_feature\n",
    "        )\n",
    "        info = {\n",
    "            'name': 'Electricity (Sorted)',\n",
    "            'features': 7,\n",
    "            'dims': 7,\n",
    "            'intens': 'N/A (semi-real)',\n",
    "            'dist': 'Real-world sorted',\n",
    "            'drift_type': 'semi-real',\n",
    "            'has_ground_truth': True,\n",
    "            'sort_feature': sort_feature\n",
    "        }\n",
    "\n",
    "    # ========================================================================\n",
    "    # SEMI-REAL DATASETS: Covertype with Controlled Drift\n",
    "    # ========================================================================\n",
    "\n",
    "    elif dataset_type == \"covertype_sorted\":\n",
    "        sort_feature = params.get('sort_feature', 'Elevation')\n",
    "        X, y, drift_positions = generate_covertype_sorted_stream(\n",
    "            total_size, n_drift_events, seed, sort_feature\n",
    "        )\n",
    "        info = {\n",
    "            'name': 'Covertype (Sorted)',\n",
    "            'features': 54,\n",
    "            'dims': 54,\n",
    "            'intens': 'N/A (semi-real)',\n",
    "            'dist': 'Real-world sorted',\n",
    "            'drift_type': 'semi-real',\n",
    "            'has_ground_truth': True,\n",
    "            'sort_feature': sort_feature\n",
    "        }\n",
    "\n",
    "    # ========================================================================\n",
    "    # NEW DATASETS: Sine Family\n",
    "    # ========================================================================\n",
    "\n",
    "    elif dataset_type == \"sine1\":\n",
    "        X, y, drift_positions = generate_sine1_stream(total_size, n_drift_events, seed)\n",
    "        info = {\n",
    "            'name': 'Sine1',\n",
    "            'features': 2,\n",
    "            'dims': 2,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'Uniform',\n",
    "            'drift_type': 'sudden',\n",
    "            'has_ground_truth': True\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"sine2\":\n",
    "        X, y, drift_positions = generate_sine2_stream(total_size, n_drift_events, seed)\n",
    "        info = {\n",
    "            'name': 'Sine2',\n",
    "            'features': 2,\n",
    "            'dims': 2,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'Uniform',\n",
    "            'drift_type': 'sudden',\n",
    "            'has_ground_truth': True\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"sinirrel1\":\n",
    "        X, y, drift_positions = generate_sinirrel1_stream(total_size, n_drift_events, seed)\n",
    "        info = {\n",
    "            'name': 'SINIRREL1',\n",
    "            'features': 4,\n",
    "            'dims': 4,\n",
    "            'intens': 'N/A (50% noise)',\n",
    "            'dist': 'Uniform',\n",
    "            'drift_type': 'sudden',\n",
    "            'has_ground_truth': True\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"sinirrel2\":\n",
    "        X, y, drift_positions = generate_sinirrel2_stream(total_size, n_drift_events, seed)\n",
    "        info = {\n",
    "            'name': 'SINIRREL2',\n",
    "            'features': 4,\n",
    "            'dims': 4,\n",
    "            'intens': 'N/A (50% noise)',\n",
    "            'dist': 'Uniform',\n",
    "            'drift_type': 'sudden',\n",
    "            'has_ground_truth': True\n",
    "        }\n",
    "\n",
    "    # ========================================================================\n",
    "    # NEW DATASETS: RBF and LED\n",
    "    # ========================================================================\n",
    "\n",
    "    elif dataset_type == \"rbfblips\":\n",
    "        n_centroids = params.get('n_centroids', 50)\n",
    "        n_features = params.get('n_features', 10)\n",
    "        X, y, drift_positions = generate_rbfblips_stream(total_size, n_drift_events, seed, n_centroids, n_features)\n",
    "        info = {\n",
    "            'name': f'RBFblips (c={n_centroids})',\n",
    "            'features': n_features,\n",
    "            'dims': n_features,\n",
    "            'intens': f'{n_centroids} centroids',\n",
    "            'dist': 'RBF',\n",
    "            'drift_type': 'sudden',\n",
    "            'has_ground_truth': True\n",
    "        }\n",
    "\n",
    "    elif dataset_type == \"led_abrupt\":\n",
    "        has_noise = params.get('has_noise', False)\n",
    "        X, y, drift_positions = generate_led_abrupt_stream(total_size, n_drift_events, seed, has_noise)\n",
    "        info = {\n",
    "            'name': 'LED_abrupt' + (' (noisy)' if has_noise else ''),\n",
    "            'features': 7,\n",
    "            'dims': 7,\n",
    "            'intens': 'N/A',\n",
    "            'dist': 'Binary',\n",
    "            'drift_type': 'sudden',\n",
    "            'has_ground_truth': True\n",
    "        }\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset_type: {dataset_type}\")\n",
    "\n",
    "    # Add common fields\n",
    "    info['type'] = dataset_type\n",
    "    info['n_samples'] = len(X)\n",
    "    info['n_features'] = X.shape[1]\n",
    "    info['n_drifts'] = len(drift_positions)\n",
    "    info['drift_positions'] = drift_positions\n",
    "\n",
    "    return X, y, drift_positions, info\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# NEW DATASETS: Sine Family (Classification Reversal + Noise)\n",
    "# ========================================================================\n",
    "\n",
    "def generate_sine1_stream(total_size, n_drift_events, seed=42):\n",
    "    \"\"\"\n",
    "    Sine1 Dataset: y < sin(x) classification that REVERSES at drift points.\n",
    "\n",
    "    Tests: Classification reversal drift (different from SEA's threshold shift)\n",
    "    - Before drift: y = 1 if point is BELOW sin(x) curve\n",
    "    - After drift:  y = 1 if point is ABOVE sin(x) curve\n",
    "\n",
    "    This is a more dramatic P(Y|X) change than SEA's threshold shift.\n",
    "\n",
    "    Args:\n",
    "        total_size: Total number of samples\n",
    "        n_drift_events: Number of drift events\n",
    "        seed: Random seed\n",
    "\n",
    "    Returns:\n",
    "        X: Features (2D: x and y coordinates)\n",
    "        y: Labels (binary classification)\n",
    "        drift_positions: List of drift positions\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "\n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "\n",
    "        # Generate random points in [0, 2π] × [0, 1]\n",
    "        x_vals = np.random.uniform(0, 2 * np.pi, size)\n",
    "        y_vals = np.random.uniform(0, 1, size)\n",
    "\n",
    "        # Compute sin curve\n",
    "        sin_vals = np.sin(x_vals)\n",
    "\n",
    "        # Classification rule: REVERSES at each drift\n",
    "        if seg_idx % 2 == 0:\n",
    "            # Concept 0: Points BELOW sine curve are positive\n",
    "            labels = (y_vals < (sin_vals + 1) / 2).astype(int)  # Normalize sin to [0,1]\n",
    "        else:\n",
    "            # Concept 1: Points ABOVE sine curve are positive (REVERSED!)\n",
    "            labels = (y_vals >= (sin_vals + 1) / 2).astype(int)\n",
    "\n",
    "        # Stack features: [x, y]\n",
    "        X_seg = np.column_stack([x_vals, y_vals])\n",
    "        X_list.append(X_seg)\n",
    "        y_list.append(labels)\n",
    "\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.hstack(y_list)\n",
    "    \n",
    "    return X, y, drift_positions\n",
    "\n",
    "def generate_sine2_stream(total_size, n_drift_events, seed=42):\n",
    "    \"\"\"\n",
    "    Sine2 Dataset: y < 0.5 + 0.3*sin(3πx) classification that REVERSES.\n",
    "\n",
    "    Similar to Sine1 but with:\n",
    "    - Higher frequency (3π instead of 1)\n",
    "    - Offset boundary (0.5)\n",
    "    - Amplitude scaling (0.3)\n",
    "\n",
    "    Args:\n",
    "        total_size: Total number of samples\n",
    "        n_drift_events: Number of drift events\n",
    "        seed: Random seed\n",
    "\n",
    "    Returns:\n",
    "        X: Features (2D: x and y coordinates)\n",
    "        y: Labels (binary classification)\n",
    "        drift_positions: List of drift positions\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "\n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "\n",
    "        # Generate random points in [0, 2π] × [0, 1]\n",
    "        x_vals = np.random.uniform(0, 2 * np.pi, size)\n",
    "        y_vals = np.random.uniform(0, 1, size)\n",
    "\n",
    "        # Compute boundary: 0.5 + 0.3*sin(3πx)\n",
    "        boundary = 0.5 + 0.3 * np.sin(3 * np.pi * x_vals)\n",
    "\n",
    "        # Classification rule: REVERSES at each drift\n",
    "        if seg_idx % 2 == 0:\n",
    "            # Concept 0: Points BELOW boundary are positive\n",
    "            labels = (y_vals < boundary).astype(int)\n",
    "        else:\n",
    "            # Concept 1: Points ABOVE boundary are positive (REVERSED!)\n",
    "            labels = (y_vals >= boundary).astype(int)\n",
    "\n",
    "        # Stack features: [x, y]\n",
    "        X_seg = np.column_stack([x_vals, y_vals])\n",
    "        X_list.append(X_seg)\n",
    "        y_list.append(labels)\n",
    "\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.hstack(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "def generate_sinirrel1_stream(total_size, n_drift_events, seed=42):\n",
    "    \"\"\"\n",
    "    SINIRREL1: Sine1 + 2 IRRELEVANT random features (noise robustness test).\n",
    "\n",
    "    Tests: How well methods handle irrelevant features\n",
    "    - Features 0-1: Same as Sine1 (relevant)\n",
    "    - Features 2-3: Uniformly random noise (irrelevant)\n",
    "\n",
    "    This tests if methods waste capacity on noise dimensions.\n",
    "\n",
    "    Args:\n",
    "        total_size: Total number of samples\n",
    "        n_drift_events: Number of drift events\n",
    "        seed: Random seed\n",
    "\n",
    "    Returns:\n",
    "        X: Features (4D: 2 relevant + 2 irrelevant)\n",
    "        y: Labels (binary classification)\n",
    "        drift_positions: List of drift positions\n",
    "    \"\"\"\n",
    "    # Generate Sine1 first\n",
    "    X_sine1, y, drift_positions = generate_sine1_stream(total_size, n_drift_events, seed)\n",
    "\n",
    "    # Add 2 irrelevant random features\n",
    "    np.random.seed(seed + 999)\n",
    "    noise_features = np.random.uniform(0, 1, (total_size, 2))\n",
    "\n",
    "    X = np.hstack([X_sine1, noise_features])\n",
    "\n",
    "    print(f\"  SINIRREL1 (SUDDEN + NOISE): {X.shape[0]} samples, {X.shape[1]} features (2 relevant + 2 irrelevant)\")\n",
    "    print(f\"    {len(drift_positions)} drifts, Noise: 50% of features\")\n",
    "    return X, y, drift_positions\n",
    "\n",
    "def generate_sinirrel2_stream(total_size, n_drift_events, seed=42):\n",
    "    \"\"\"\n",
    "    SINIRREL2: Sine2 + 2 IRRELEVANT random features (noise robustness test).\n",
    "\n",
    "    Tests: Noise robustness with complex sine boundary\n",
    "    - Features 0-1: Same as Sine2 (relevant)\n",
    "    - Features 2-3: Uniformly random noise (irrelevant)\n",
    "\n",
    "    Args:\n",
    "        total_size: Total number of samples\n",
    "        n_drift_events: Number of drift events\n",
    "        seed: Random seed\n",
    "\n",
    "    Returns:\n",
    "        X: Features (4D: 2 relevant + 2 irrelevant)\n",
    "        y: Labels (binary classification)\n",
    "        drift_positions: List of drift positions\n",
    "    \"\"\"\n",
    "    # Generate Sine2 first\n",
    "    X_sine2, y, drift_positions = generate_sine2_stream(total_size, n_drift_events, seed)\n",
    "\n",
    "    # Add 2 irrelevant random features\n",
    "    np.random.seed(seed + 999)\n",
    "    noise_features = np.random.uniform(0, 1, (total_size, 2))\n",
    "\n",
    "    X = np.hstack([X_sine2, noise_features])\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "def generate_rbfblips_stream(total_size, n_drift_events, seed=42, n_centroids=50, n_features=10):\n",
    "    \"\"\"\n",
    "    RBFblips: Random RBF clusters with SUDDEN drift (blips).\n",
    "\n",
    "    Tests: Non-linear, high-dimensional distributions with abrupt cluster changes\n",
    "    - Uses Radial Basis Functions (RBF) to create complex decision boundaries\n",
    "    - Each drift event: centroids jump to new positions (sudden drift)\n",
    "    - Different from incremental RBF (which has continuous centroid movement)\n",
    "\n",
    "    Args:\n",
    "        total_size: Total number of samples\n",
    "        n_drift_events: Number of drift events\n",
    "        seed: Random seed\n",
    "        n_centroids: Number of RBF centroids\n",
    "        n_features: Feature dimensionality\n",
    "\n",
    "    Returns:\n",
    "        X: Features (n_features dimensions)\n",
    "        y: Labels (binary classification)\n",
    "        drift_positions: List of drift positions\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "\n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "\n",
    "        # Use River's RandomRBF generator with DIFFERENT seed per segment\n",
    "        stream = synth.RandomRBF(\n",
    "            seed_model=seed + seg_idx * 1000,  # Different centroids per segment\n",
    "            seed_sample=seed + seg_idx * 100,\n",
    "            n_classes=2,\n",
    "            n_features=n_features,\n",
    "            n_centroids=n_centroids\n",
    "        )\n",
    "\n",
    "        # Generate samples from this RBF configuration\n",
    "        for i, (x, y) in enumerate(stream.take(size)):\n",
    "            X_list.append(list(x.values()))\n",
    "            y_list.append(y)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n",
    "def generate_led_abrupt_stream(total_size, n_drift_events, seed=42, has_noise=False):\n",
    "    \"\"\"\n",
    "    LED_abrupt: 7-segment LED digit prediction with SUDDEN function changes.\n",
    "\n",
    "    Tests: Discrete binary features with abrupt concept drift\n",
    "    - 7 binary features (LED segments: a-g)\n",
    "    - Predicts digit displayed on LED\n",
    "    - Each drift: LED encoding function changes\n",
    "\n",
    "    Different from STAGGER:\n",
    "    - LED: 7 binary features, digit classification (10 classes → binary)\n",
    "    - STAGGER: 3 categorical features, logical rule changes\n",
    "\n",
    "    Args:\n",
    "        total_size: Total number of samples\n",
    "        n_drift_events: Number of drift events\n",
    "        seed: Random seed\n",
    "        has_noise: Whether to add noise to LED (default: False for clean signal)\n",
    "\n",
    "    Returns:\n",
    "        X: Features (7 binary features)\n",
    "        y: Labels (binary: even vs odd digit)\n",
    "        drift_positions: List of drift positions\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "\n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "\n",
    "        # Use River's LED generator\n",
    "        stream = synth.LED(\n",
    "            seed=seed + seg_idx * 100,\n",
    "            noise_percentage=0.1 if has_noise else 0.0,\n",
    "            irrelevant_features=has_noise  # Add irrelevant features if noise=True\n",
    "        )\n",
    "\n",
    "        # Generate samples\n",
    "        for i, (x, y_digit) in enumerate(stream.take(size)):\n",
    "            X_list.append(list(x.values())[:7])  # First 7 features are LED segments\n",
    "\n",
    "            # Convert 10-class digit to binary (even vs odd)\n",
    "            # This changes the classification function at each drift\n",
    "            if seg_idx % 2 == 0:\n",
    "                # Concept 0: Even digits are positive\n",
    "                y_binary = 1 if y_digit % 2 == 0 else 0\n",
    "            else:\n",
    "                # Concept 1: Odd digits are positive (REVERSED!)\n",
    "                y_binary = 1 if y_digit % 2 == 1 else 0\n",
    "\n",
    "            y_list.append(y_binary)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    return X, y, drift_positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: UTILITY FUNCTIONS (Standard Drift Detection - NO TN)\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_beta_score(precision: float, recall: float, beta: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Calculate β-score (F-beta score) - Original ShapeDD paper metric.\n",
    "    \n",
    "    β-score = (1 + β²) * (precision * recall) / (β² * precision + recall)\n",
    "    \n",
    "    β=0.5: Emphasizes precision (minimizes false alarms) - Original paper uses this\n",
    "    β=1.0: Standard F1-score (equal weight)\n",
    "    β=2.0: Emphasizes recall (catches all drifts)\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    beta_squared = beta ** 2\n",
    "    numerator = (1 + beta_squared) * precision * recall\n",
    "    denominator = beta_squared * precision + recall\n",
    "    \n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def calculate_detection_metrics_enhanced(detections, true_drifts, stream_length,\n",
    "                                         acceptable_delta=150):\n",
    "    \"\"\"\n",
    "    Calculate detection performance metrics following standard drift detection practice.\n",
    "\n",
    "    This implementation follows the event-based approach used in most drift detection\n",
    "    papers, which do NOT compute True Negatives (TN) due to definitional ambiguity.\n",
    "\n",
    "    Implements metrics from:\n",
    "    - Basseville & Nikiforov (1993): MTFA, MTR\n",
    "    - Bifet et al. (2020): F1 as primary metric\n",
    "    - Standard drift detection practice: Precision, Recall, MTTD\n",
    "\n",
    "    Args:\n",
    "        detections: List of detected drift positions\n",
    "        true_drifts: List of true drift positions\n",
    "        stream_length: Total length of data stream\n",
    "        acceptable_delta: Acceptable delay window (default: 150)\n",
    "\n",
    "    Returns:\n",
    "        dict: Comprehensive metrics including:\n",
    "            - Basic: TP, FP, FN\n",
    "            - Primary: Precision, Recall, F1\n",
    "            - Temporal: MTTD, Median TTD, MTFA, MTR, MDR\n",
    "            - Other: Detection rate, n_detections\n",
    "    \"\"\"\n",
    "    detections = sorted([int(d) for d in detections])\n",
    "\n",
    "    # Handle no-drift case\n",
    "    if not true_drifts or len(true_drifts) == 0:\n",
    "        fp = len(detections)\n",
    "\n",
    "        return {\n",
    "            # Basic counts\n",
    "            'tp': 0, 'fp': fp, 'fn': 0,\n",
    "\n",
    "            # Primary metrics\n",
    "            'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0,\n",
    "\n",
    "            # Temporal metrics\n",
    "            'mttd': float('inf'),  # Mean Time To Detection\n",
    "            'median_ttd': float('inf'),  # Median TTD (more robust)\n",
    "            'mtfa': stream_length / fp if fp > 0 else float('inf'),  # Mean Time Between False Alarms\n",
    "            'mtr': 0.0,  # Mean Time Ratio\n",
    "            'mdr': 0.0,  # Missed Detection Rate\n",
    "\n",
    "            # Additional\n",
    "            'detection_rate': 0.0,\n",
    "            'false_alarm_rate': fp / stream_length if stream_length > 0 else 0.0,\n",
    "            'n_detections': fp,\n",
    "            'n_true_drifts': 0\n",
    "        }\n",
    "\n",
    "    # Convert true_drifts to list\n",
    "    if isinstance(true_drifts, (int, float)):\n",
    "        true_drifts = [int(true_drifts)]\n",
    "    else:\n",
    "        true_drifts = [int(d) for d in true_drifts]\n",
    "\n",
    "    # ==================================================================\n",
    "    # STEP 1: Match detections to true drifts\n",
    "    # ==================================================================\n",
    "    matched_detections = set()\n",
    "    per_drift_delays = []\n",
    "\n",
    "    for true_drift in true_drifts:\n",
    "        # Find all detections within acceptable window\n",
    "        valid_detections = [(d, abs(d - true_drift)) for d in detections\n",
    "                           if abs(d - true_drift) <= acceptable_delta\n",
    "                           and d not in matched_detections]\n",
    "\n",
    "        if valid_detections:\n",
    "            # Match to closest detection\n",
    "            closest_det, delay = min(valid_detections, key=lambda x: x[1])\n",
    "            matched_detections.add(closest_det)\n",
    "            per_drift_delays.append(delay)\n",
    "\n",
    "    # ==================================================================\n",
    "    # STEP 2: Calculate basic confusion matrix (NO TN)\n",
    "    # ==================================================================\n",
    "    tp = len(matched_detections)  # True Positives\n",
    "    fn = len(true_drifts) - tp     # False Negatives\n",
    "    fp = len(detections) - len(matched_detections)  # False Positives\n",
    "\n",
    "    # ==================================================================\n",
    "    # STEP 3: Primary Detection Metrics\n",
    "    # ==================================================================\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    beta_score = calculate_beta_score(precision, recall, beta=0.5)  # Original paper metric\n",
    "\n",
    "    # ==================================================================\n",
    "    # STEP 4: Temporal Metrics\n",
    "    # ==================================================================\n",
    "\n",
    "    # Mean Time To Detection (MTTD)\n",
    "    mttd = np.mean(per_drift_delays) if per_drift_delays else float('inf')\n",
    "\n",
    "    # Median Time To Detection (more robust to outliers)\n",
    "    median_ttd = np.median(per_drift_delays) if per_drift_delays else float('inf')\n",
    "\n",
    "    # Mean Time Between False Alarms (MTFA)\n",
    "    # Source: Basseville & Nikiforov (1993)\n",
    "    # This is the standard way to measure false alarm rate in drift detection\n",
    "    # without needing True Negatives\n",
    "    mtfa = stream_length / fp if fp > 0 else float('inf')\n",
    "\n",
    "    # False Alarm Rate (alarms per sample)\n",
    "    # Alternative representation of MTFA\n",
    "    false_alarm_rate = fp / stream_length if stream_length > 0 else 0.0\n",
    "\n",
    "    # Mean Time Ratio (MTR)\n",
    "    # Combines detection rate, MTTD, and MTFA into single score\n",
    "    # Source: Basseville & Nikiforov (1993)\n",
    "    if mttd > 0 and mttd != float('inf'):\n",
    "        mtr = (recall * mtfa) / mttd\n",
    "    else:\n",
    "        mtr = 0.0\n",
    "\n",
    "    # Missed Detection Rate (MDR)\n",
    "    # Emphasizes cost of missed drifts\n",
    "    mdr = fn / (tp + fn) if (tp + fn) > 0 else 0.0  # Same as 1 - recall\n",
    "\n",
    "    # ==================================================================\n",
    "    # STEP 5: Additional Metrics\n",
    "    # ==================================================================\n",
    "    detection_rate = tp / len(true_drifts)\n",
    "\n",
    "    # ==================================================================\n",
    "    # RETURN: Standard drift detection metrics (NO TN)\n",
    "    # ==================================================================\n",
    "    return {\n",
    "        # Basic confusion matrix (TP, FP, FN only - NO TN)\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "\n",
    "        # Primary detection metrics (STANDARD)\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "            'beta_score': beta_score,\n",
    "\n",
    "        # Temporal metrics (STANDARD)\n",
    "        'mttd': mttd,              # Mean Time To Detection\n",
    "        'median_ttd': median_ttd,  # Median TTD (robust)\n",
    "        'mtfa': mtfa,              # Mean Time Between False Alarms ⭐ Replaces FPR\n",
    "        'mtr': mtr,                # Mean Time Ratio\n",
    "        'mdr': mdr,                # Missed Detection Rate\n",
    "\n",
    "        # Additional metrics\n",
    "        'detection_rate': detection_rate,\n",
    "        'false_alarm_rate': false_alarm_rate,  # FP per sample (alternative to MTFA)\n",
    "        'n_detections': len(detections),\n",
    "        'n_true_drifts': len(true_drifts)\n",
    "    }\n",
    "\n",
    "\n",
    "# For backward compatibility - simple wrapper\n",
    "def calculate_detection_metrics(detections, true_drifts, acceptable_delta=150):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Use calculate_detection_metrics_enhanced instead.\n",
    "    This wrapper maintains compatibility with old code.\n",
    "\n",
    "    Note: Returns subset of metrics (without MTFA, MTR, etc.)\n",
    "    \"\"\"\n",
    "    detections = sorted([int(d) for d in detections])\n",
    "\n",
    "    if not true_drifts or len(true_drifts) == 0:\n",
    "        return {\n",
    "            'tp': 0, 'fp': len(detections), 'fn': 0,\n",
    "            'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0,\n",
    "            'mttd': float('inf'),\n",
    "            'detection_rate': 0.0,\n",
    "            'n_detections': len(detections)\n",
    "        }\n",
    "\n",
    "    # Convert to list\n",
    "    if isinstance(true_drifts, (int, float)):\n",
    "        true_drifts = [int(true_drifts)]\n",
    "    else:\n",
    "        true_drifts = [int(d) for d in true_drifts]\n",
    "\n",
    "    # Match detections to true drifts\n",
    "    matched_detections = set()\n",
    "    per_drift_delays = []\n",
    "\n",
    "    for true_drift in true_drifts:\n",
    "        valid_detections = [(d, abs(d - true_drift)) for d in detections\n",
    "                           if abs(d - true_drift) <= acceptable_delta\n",
    "                           and d not in matched_detections]\n",
    "\n",
    "        if valid_detections:\n",
    "            closest_det, delay = min(valid_detections, key=lambda x: x[1])\n",
    "            matched_detections.add(closest_det)\n",
    "            per_drift_delays.append(delay)\n",
    "\n",
    "    # Calculate metrics\n",
    "    tp = len(matched_detections)\n",
    "    fn = len(true_drifts) - tp\n",
    "    fp = len(detections) - len(matched_detections)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    beta_score = calculate_beta_score(precision, recall, beta=0.5)  # Original paper metric\n",
    "\n",
    "    mttd = np.mean(per_drift_delays) if per_drift_delays else float('inf')\n",
    "    detection_rate = tp / len(true_drifts)\n",
    "\n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "            'beta_score': beta_score,\n",
    "        'mttd': mttd,\n",
    "        'detection_rate': detection_rate,\n",
    "        'n_detections': len(detections)\n",
    "    }\n",
    "\n",
    "def create_sliding_windows(X, chunk_size, overlap):\n",
    "    \"\"\"Create sliding windows over the data stream.\"\"\"\n",
    "    shift = chunk_size - overlap\n",
    "    windows = []\n",
    "    indices = []\n",
    "\n",
    "    for i in range(0, len(X) - chunk_size + 1, shift):\n",
    "        windows.append(X[i:i+chunk_size])\n",
    "        indices.append(i + chunk_size // 2)  # Center of window\n",
    "\n",
    "    return windows, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: DRIFT DETECTION EVALUATION (Buffer-based for ShapeDD)\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_drift_detector(method_name, X, true_drifts, chunk_size=150, overlap=100):\n",
    "    \"\"\"\n",
    "    Evaluate drift detector on a stream (NO MODEL ADAPTATION).\n",
    "\n",
    "    Two approaches:\n",
    "    1. ShapeDD methods: Use BUFFER-BASED approach\n",
    "       - Maintain rolling buffer of samples (750 samples)\n",
    "       - Run ShapeDD on buffer periodically\n",
    "       - Check recent chunks within buffer for drift\n",
    "       - Scalable for large streams (only processes 750 samples at a time)\n",
    "\n",
    "    2. Other methods: Use SLIDING WINDOW approach\n",
    "       - Process stream in overlapping windows\n",
    "       - Run detector on each window\n",
    "    \n",
    "    IMPORTANT: All ShapeDD methods now use CONSISTENT window sizes:\n",
    "    - L1 = 50 (reference window)\n",
    "    - L2 = 150 (test window)\n",
    "    \n",
    "    This ensures fair comparison and isolates algorithmic improvements\n",
    "    from window size effects.\n",
    "    \"\"\"\n",
    "    print(f\"Running: {method_name}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    detections = []\n",
    "    last_detection = -10**9\n",
    "\n",
    "    # METHOD 1: Buffer-based approach for ShapeDD methods\n",
    "    if 'ShapeDD' in method_name:\n",
    "\n",
    "        # Configuration\n",
    "        BUFFER_SIZE = 750           # Large rolling buffer\n",
    "        CHECK_FREQUENCY = 150       # How often to check for drift\n",
    "\n",
    "        # Rolling buffer (stores recent samples)\n",
    "        buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "        print(f\"  Buffer size: {BUFFER_SIZE} samples\")\n",
    "        print(f\"  Check frequency: every {CHECK_FREQUENCY} samples\")\n",
    "        print(f\"  Window sizes: L1={SHAPE_L1}, L2={SHAPE_L2} (CONSISTENT for fair comparison)\")\n",
    "        print(f\"  Processing stream...\")\n",
    "\n",
    "        # Process stream sample by sample\n",
    "        for idx in range(len(X)):\n",
    "            # Add sample to buffer\n",
    "            buffer.append({'x': X[idx], 'idx': idx})\n",
    "\n",
    "            # Check for drift periodically (every CHECK_FREQUENCY samples)\n",
    "            if len(buffer) >= BUFFER_SIZE and idx % CHECK_FREQUENCY == 0:\n",
    "\n",
    "                # Step 1: Extract buffer data\n",
    "                buffer_list = list(buffer)\n",
    "                buffer_X = np.array([item['x'] for item in buffer_list])  # Shape: (BUFFER_SIZE, n_features)\n",
    "                buffer_indices = np.array([item['idx'] for item in buffer_list])\n",
    "\n",
    "                try:\n",
    "                    if method_name == 'ShapeDD':\n",
    "                        # Original ShapeDD (no adaptive features)\n",
    "                        shp_results = shape(buffer_X, SHAPE_L1, SHAPE_L2, SHAPE_N_PERM)\n",
    "\n",
    "                    elif method_name == 'ShapeDD_SNR_Adaptive':\n",
    "                        shp_results = shape_snr_adaptive(buffer_X, SHAPE_L1, SHAPE_L2, SHAPE_N_PERM)\n",
    "\n",
    "                    elif method_name == 'ShapeDD_OW_MMD':\n",
    "                        shp_results = shapedd_ow_mmd_buffer(buffer_X, SHAPE_L1, SHAPE_L2, gamma='auto')\n",
    "\n",
    "                    # Step 3: Check recent chunk within buffer for drift\n",
    "                    # Look at last CHECK_FREQUENCY samples in buffer\n",
    "                    chunk_start = max(0, len(buffer_X) - CHECK_FREQUENCY)\n",
    "                    recent_pvalues = shp_results[chunk_start:, 2]  # p-values for recent chunk\n",
    "\n",
    "                    # Step 4: Check if drift detected\n",
    "                    min_pvalue = recent_pvalues.min()\n",
    "                    trigger = min_pvalue < 0.05  # Significance threshold\n",
    "\n",
    "                    if trigger:\n",
    "                        # Find exact position of drift in buffer\n",
    "                        drift_pos_in_chunk = int(np.argmin(recent_pvalues))\n",
    "                        drift_idx = int(buffer_indices[chunk_start + drift_pos_in_chunk])\n",
    "\n",
    "                        # Record detection (with cooldown to avoid duplicates)\n",
    "                        if drift_idx - last_detection >= COOLDOWN:\n",
    "                            detections.append(drift_idx)\n",
    "                            last_detection = drift_idx\n",
    "                            print(f\"    [Sample {idx}] DRIFT DETECTED at position {drift_idx} (p-value: {min_pvalue:.6f})\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass  # Skip failed detections\n",
    "\n",
    "    # METHOD 2: Sliding window approach for other methods\n",
    "    else:\n",
    "        # Create sliding windows\n",
    "        windows, window_centers = create_sliding_windows(X, chunk_size, overlap)\n",
    "        print(f\"  Processing {len(windows)} windows...\")\n",
    "\n",
    "        for window_idx, (window, center_idx) in enumerate(zip(windows, window_centers)):\n",
    "            try:\n",
    "                # Method-specific detection\n",
    "                if method_name == 'D3':\n",
    "                    score = d3(window)\n",
    "                    trigger = score > 0.7\n",
    "\n",
    "                elif method_name == 'DAWIDD':\n",
    "                    _, p_value = dawidd(window, 'rbf')\n",
    "                    trigger = p_value < 0.05\n",
    "\n",
    "                elif method_name == 'MMD':\n",
    "                    stat, p_value = mmd(window)\n",
    "                    trigger = p_value < 0.05\n",
    "\n",
    "                elif method_name == 'KS':\n",
    "                    p_value = ks(window)\n",
    "                    trigger = p_value < 0.05\n",
    "\n",
    "                elif method_name == 'MMD_OW':\n",
    "                    # Optimally-Weighted MMD\n",
    "                    stat, threshold = mmd_ow(window, gamma='auto')\n",
    "                    trigger = stat > threshold\n",
    "\n",
    "                else:\n",
    "                    trigger = False\n",
    "\n",
    "                # Record detection if triggered and outside cooldown\n",
    "                if trigger and (center_idx - last_detection >= COOLDOWN):\n",
    "                    detections.append(center_idx)\n",
    "                    last_detection = center_idx\n",
    "\n",
    "            except Exception as e:\n",
    "                pass  # Skip failed detections\n",
    "\n",
    "    # Calculate metrics\n",
    "    end_time = time.time()\n",
    "    metrics = calculate_detection_metrics_enhanced(\n",
    "        detections, true_drifts, len(X)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'detections': detections,\n",
    "        'stream_size': len(X),\n",
    "        'runtime_s': end_time - start_time,\n",
    "        **metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5B: STREAMING DETECTOR EVALUATION (MINIMAL MODEL SUPPORT)\n",
    "# ============================================================================\n",
    "\n",
    "def create_lightweight_classifier():\n",
    "    \"\"\"Create a simple classifier for generating accuracy signals.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "    ])\n",
    "\n",
    "def evaluate_streaming_detector(method_name, X, y, true_drifts):\n",
    "    \"\"\"\n",
    "    Evaluate streaming detector with MINIMAL model support.\n",
    "\n",
    "    NOTE: Model is used ONLY to generate accuracy signals for drift detection.\n",
    "    NO adaptation or retraining is performed (detection-only focus).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING: {method_name} (Streaming)\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create detector\n",
    "    if method_name == 'ADWIN':\n",
    "        detector = ADWIN(delta=0.002)\n",
    "    elif method_name == 'DDM':\n",
    "        detector = DDM()\n",
    "    elif method_name == 'EDDM':\n",
    "        detector = EDDM(alpha=0.95, beta=0.9)\n",
    "    elif method_name == 'HDDM_A':\n",
    "        detector = HDDM_A()\n",
    "    elif method_name == 'HDDM_W':\n",
    "        detector = HDDM_W()\n",
    "    elif method_name == 'FHDDM':\n",
    "        detector = FHDDM(short_window_size=20)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown streaming detector: {method_name}\")\n",
    "\n",
    "    # Train initial model on first batch\n",
    "    training_end = INITIAL_TRAINING_SIZE\n",
    "    model = create_lightweight_classifier()\n",
    "    model.fit(X[:training_end], y[:training_end])\n",
    "\n",
    "    print(f\"  Initial training: {INITIAL_TRAINING_SIZE} samples\")\n",
    "\n",
    "    # Tracking\n",
    "    detections = []\n",
    "    last_detection = -10**9\n",
    "    recent_correct = deque(maxlen=PREQUENTIAL_WINDOW)\n",
    "    accuracy_buffer = deque(maxlen=30)\n",
    "\n",
    "    # Determine signal type\n",
    "    continuous_detectors = {'ADWIN', 'HDDM_A'}\n",
    "    signal_type = 'continuous' if method_name in continuous_detectors else 'binary'\n",
    "\n",
    "    print(f\"  Processing stream (signal type: {signal_type})...\")\n",
    "\n",
    "    # Stream evaluation (detection only, NO retraining)\n",
    "    for idx in range(training_end, len(X)):\n",
    "        # Make prediction\n",
    "        y_pred = model.predict(X[idx].reshape(1, -1))[0]\n",
    "        is_correct = (y_pred == y[idx])\n",
    "\n",
    "        recent_correct.append(is_correct)\n",
    "        accuracy_buffer.append(is_correct)\n",
    "\n",
    "        # Update detector with accuracy/error signal\n",
    "        if signal_type == 'continuous':\n",
    "            # Use accuracy as signal\n",
    "            if len(accuracy_buffer) >= 10:\n",
    "                signal = float(np.mean(accuracy_buffer))\n",
    "            else:\n",
    "                signal = float(is_correct)\n",
    "        else:\n",
    "            # Use error as signal (binary)\n",
    "            signal = bool(1 - is_correct)\n",
    "\n",
    "        detector.update(signal)\n",
    "\n",
    "        # Check for drift\n",
    "        if detector.drift_detected and (idx - last_detection >= COOLDOWN):\n",
    "            detections.append(idx)\n",
    "            last_detection = idx\n",
    "\n",
    "    # Calculate metrics\n",
    "    end_time = time.time()\n",
    "    metrics = calculate_detection_metrics(detections, true_drifts)\n",
    "\n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'paradigm': 'streaming',\n",
    "        'detections': detections,\n",
    "        'runtime_s': end_time - start_time,\n",
    "        **metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: COMPREHENSIVE BENCHMARK (WINDOW + STREAMING)\n",
    "# ============================================================================\n",
    "# MODIFIED: Multiple independent runs for statistical validation\n",
    "# Each run uses a different random seed to ensure independence\n",
    "\n",
    "enabled_datasets = [(name, config) for name, config in DATASET_CATALOG.items()\n",
    "                    if config['enabled']]\n",
    "\n",
    "all_results = []\n",
    "dataset_summaries = []\n",
    "\n",
    "# Calculate expected totals for validation\n",
    "n_methods = len(WINDOW_METHODS) + len(STREAMING_METHODS)\n",
    "n_datasets = len(enabled_datasets)\n",
    "expected_experiments = N_RUNS * n_datasets * n_methods\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"BENCHMARK CONFIGURATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  N_RUNS: {N_RUNS}\")\n",
    "print(f\"  Datasets: {n_datasets} ({', '.join([d[0] for d in enabled_datasets])})\")\n",
    "print(f\"  Window methods: {len(WINDOW_METHODS)} ({', '.join(WINDOW_METHODS)})\")\n",
    "print(f\"  Streaming methods: {len(STREAMING_METHODS)} ({', '.join(STREAMING_METHODS) if STREAMING_METHODS else 'None'})\")\n",
    "print(f\"  Expected total experiments: {expected_experiments}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "benchmark_start_time = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# OUTER LOOP: Multiple Independent Runs\n",
    "# ============================================================================\n",
    "for run_idx, seed in enumerate(RANDOM_SEEDS, 1):\n",
    "    run_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# RUN {run_idx}/{N_RUNS} (seed={seed})\")\n",
    "    print(f\"# Progress: {run_idx/N_RUNS*100:.1f}% complete | Experiments so far: {len(all_results)}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    for dataset_idx, (dataset_name, dataset_config) in enumerate(enabled_datasets, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"RUN {run_idx}/{N_RUNS} | DATASET {dataset_idx}/{len(enabled_datasets)}: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Generate dataset with THIS RUN's seed\n",
    "        X, y, true_drifts, info = generate_drift_stream(\n",
    "            dataset_config,\n",
    "            total_size=STREAM_SIZE,\n",
    "            seed=seed  # <-- Different seed per run\n",
    "        )\n",
    "\n",
    "        dataset_results = []\n",
    "\n",
    "        # Evaluate window-based methods\n",
    "        for method in WINDOW_METHODS:\n",
    "            result = evaluate_drift_detector(\n",
    "                method, X, true_drifts,\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                overlap=OVERLAP\n",
    "            )\n",
    "\n",
    "            # Add metadata (including run information)\n",
    "            result['paradigm'] = 'window'\n",
    "            result['dataset'] = dataset_name\n",
    "            result['n_features'] = info['n_features']\n",
    "            result['n_drifts'] = info['n_drifts']\n",
    "            result['drift_positions'] = true_drifts\n",
    "            result['intens'] = info['intens']\n",
    "            result['dims'] = info['dims']\n",
    "            result['ground_truth_type'] = dataset_config.get('ground_truth_type', 'unknown')\n",
    "            \n",
    "            # NEW: Add run tracking for statistical analysis\n",
    "            result['run_id'] = run_idx\n",
    "            result['seed'] = seed\n",
    "\n",
    "            dataset_results.append(result)\n",
    "            all_results.append(result)\n",
    "\n",
    "        # Evaluate streaming methods\n",
    "        for method in STREAMING_METHODS:\n",
    "            result = evaluate_streaming_detector(\n",
    "                method, X, y, true_drifts\n",
    "            )\n",
    "\n",
    "            result['paradigm'] = 'streaming'\n",
    "            result['dataset'] = dataset_name\n",
    "            result['n_features'] = info['n_features']\n",
    "            result['n_drifts'] = info['n_drifts']\n",
    "            result['drift_positions'] = true_drifts\n",
    "            result['intens'] = info['intens']\n",
    "            result['dims'] = info['dims']\n",
    "            result['ground_truth_type'] = dataset_config.get('ground_truth_type', 'unknown')\n",
    "            \n",
    "            # NEW: Add run tracking\n",
    "            result['run_id'] = run_idx\n",
    "            result['seed'] = seed\n",
    "\n",
    "            dataset_results.append(result)\n",
    "            all_results.append(result)\n",
    "\n",
    "        # Dataset summary (computed per run)\n",
    "        if dataset_results:\n",
    "            avg_f1 = np.mean([r['f1_score'] for r in dataset_results])\n",
    "            detection_rate = np.mean([r['detection_rate'] for r in dataset_results])\n",
    "\n",
    "            dataset_summaries.append({\n",
    "                'dataset': dataset_name,\n",
    "                'n_features': info['n_features'],\n",
    "                'n_drifts': info['n_drifts'],\n",
    "                'intens': info['intens'],\n",
    "                'avg_f1': avg_f1,\n",
    "                'detection_rate': detection_rate,\n",
    "                'run_id': run_idx,  # NEW: Track which run\n",
    "                'seed': seed\n",
    "            })\n",
    "        gc.collect()\n",
    "    \n",
    "    # Run summary\n",
    "    run_elapsed = time.time() - run_start_time\n",
    "    print(f\"\\n  [Run {run_idx} Summary] Completed in {run_elapsed:.1f}s | Total experiments: {len(all_results)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL VALIDATION AND SUMMARY\n",
    "# ============================================================================\n",
    "total_elapsed = time.time() - benchmark_start_time\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(f\"# ALL {N_RUNS} RUNS COMPLETED!\")\n",
    "print(f\"{'#'*80}\")\n",
    "print(f\"  Total experiments: {len(all_results)}\")\n",
    "print(f\"  Expected experiments: {expected_experiments}\")\n",
    "print(f\"  Total runtime: {total_elapsed/60:.1f} minutes\")\n",
    "\n",
    "# Validation check\n",
    "if len(all_results) != expected_experiments:\n",
    "    print(f\"\\n  ⚠️  WARNING: Experiment count mismatch!\")\n",
    "    print(f\"      Expected {expected_experiments}, got {len(all_results)}\")\n",
    "    print(f\"      This may indicate incomplete runs or configuration issues.\")\n",
    "else:\n",
    "    print(f\"\\n  ✅ Validation PASSED: All {expected_experiments} experiments completed successfully.\")\n",
    "\n",
    "# Show experiment distribution\n",
    "print(f\"\\n  Experiments per run: {len(all_results) // N_RUNS if N_RUNS > 0 else 0}\")\n",
    "print(f\"  Experiments per dataset: {len(all_results) // n_datasets if n_datasets > 0 else 0}\")\n",
    "print(f\"{'#'*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: STATISTICAL ANALYSIS & AGGREGATION\n",
    "# ============================================================================\n",
    "# Aggregate results from multiple runs with confidence intervals\n",
    "# and perform statistical significance testing\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, friedmanchisquare\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STATISTICAL ANALYSIS: {len(all_results)} total experiments from {N_RUNS} runs\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. AGGREGATE STATISTICS WITH CONFIDENCE INTERVALS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. AGGREGATE RESULTS (Mean ± Std with 95% Confidence Intervals)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_ci_95(data):\n",
    "    \"\"\"Calculate 95% confidence interval.\"\"\"\n",
    "    if len(data) < 2:\n",
    "        return np.nan, np.nan\n",
    "    mean = np.mean(data)\n",
    "    sem = stats.sem(data)  # Standard error of mean\n",
    "    ci = stats.t.interval(0.95, len(data)-1, loc=mean, scale=sem)\n",
    "    return ci[0], ci[1]\n",
    "\n",
    "# Group by method and dataset\n",
    "aggregated = df_results.groupby(['method', 'dataset']).agg({\n",
    "    'f1_score': ['mean', 'std', 'min', 'max', 'count'],\n",
    "    'beta_score': ['mean', 'std', 'min', 'max'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'mttd': ['mean', 'std'],\n",
    "    'detection_rate': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "# Calculate confidence intervals for key metrics\n",
    "ci_results = []\n",
    "for (method, dataset), group in df_results.groupby(['method', 'dataset']):\n",
    "    f1_ci_low, f1_ci_high = calculate_ci_95(group['f1_score'].values)\n",
    "    beta_ci_low, beta_ci_high = calculate_ci_95(group['beta_score'].values)\n",
    "    \n",
    "    ci_results.append({\n",
    "        'method': method,\n",
    "        'dataset': dataset,\n",
    "        'f1_mean': group['f1_score'].mean(),\n",
    "        'f1_std': group['f1_score'].std(),\n",
    "        'f1_ci_low': f1_ci_low,\n",
    "        'f1_ci_high': f1_ci_high,\n",
    "        'beta_mean': group['beta_score'].mean(),\n",
    "        'beta_std': group['beta_score'].std(),\n",
    "        'beta_ci_low': beta_ci_low,\n",
    "        'beta_ci_high': beta_ci_high,\n",
    "        'n_runs': len(group)\n",
    "    })\n",
    "\n",
    "df_ci = pd.DataFrame(ci_results)\n",
    "\n",
    "# Display aggregated results\n",
    "print(\"\\nAggregated Results (sorted by F1 score):\")\n",
    "print(aggregated.sort_values(('f1_score', 'mean'), ascending=False).head(20))\n",
    "\n",
    "print(\"\\n\\n95% Confidence Intervals (Top 10 by F1):\")\n",
    "df_ci_sorted = df_ci.sort_values('f1_mean', ascending=False).head(10)\n",
    "for _, row in df_ci_sorted.iterrows():\n",
    "    print(f\"{row['method']:25s} | {row['dataset']:15s} | \"\n",
    "          f\"F1: {row['f1_mean']:.3f} ± {row['f1_std']:.3f} \"\n",
    "          f\"[{row['f1_ci_low']:.3f}, {row['f1_ci_high']:.3f}] | \"\n",
    "          f\"β: {row['beta_mean']:.3f} ± {row['beta_std']:.3f} \"\n",
    "          f\"[{row['beta_ci_low']:.3f}, {row['beta_ci_high']:.3f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. STATISTICAL SIGNIFICANCE TESTING\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"2. STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define methods to compare (focus on ShapeDD variants)\n",
    "shapedd_methods = ['ShapeDD', 'ShapeDD_SNR_Adaptive', 'ShapeDD_OW_MMD']\n",
    "baseline_methods = ['MMD', 'D3', 'DAWIDD', 'KS']\n",
    "\n",
    "# Filter for methods that actually have results\n",
    "available_methods = df_results['method'].unique()\n",
    "shapedd_methods = [m for m in shapedd_methods if m in available_methods]\n",
    "baseline_methods = [m for m in baseline_methods if m in available_methods]\n",
    "\n",
    "print(f\"\\nShapeDD variants: {shapedd_methods}\")\n",
    "print(f\"Baseline methods: {baseline_methods}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2.1 Pairwise Wilcoxon Tests (ShapeDD variants vs Original)\n",
    "# ============================================================================\n",
    "if 'ShapeDD' in shapedd_methods and len(shapedd_methods) > 1:\n",
    "    print(\"\\n\\n--- 2.1 Pairwise Wilcoxon Signed-Rank Tests ---\")\n",
    "    print(\"(Comparing improved ShapeDD variants against original ShapeDD)\\n\")\n",
    "    \n",
    "    baseline_shapedd = df_results[df_results['method'] == 'ShapeDD']['f1_score'].values\n",
    "    \n",
    "    for method in shapedd_methods:\n",
    "        if method == 'ShapeDD':\n",
    "            continue\n",
    "        \n",
    "        method_scores = df_results[df_results['method'] == method]['f1_score'].values\n",
    "        \n",
    "        # Ensure paired comparison (same datasets)\n",
    "        if len(baseline_shapedd) == len(method_scores):\n",
    "            statistic, p_value = wilcoxon(baseline_shapedd, method_scores)\n",
    "            mean_diff = np.mean(method_scores) - np.mean(baseline_shapedd)\n",
    "            \n",
    "            # Cohen's d effect size\n",
    "            pooled_std = np.sqrt((np.std(baseline_shapedd)**2 + np.std(method_scores)**2) / 2)\n",
    "            cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            effect_label = \"large\" if abs(cohens_d) >= 0.8 else \"medium\" if abs(cohens_d) >= 0.5 else \"small\" if abs(cohens_d) >= 0.2 else \"negligible\"\n",
    "            \n",
    "            print(f\"{method:30s} vs ShapeDD:\")\n",
    "            print(f\"  Mean F1 difference: {mean_diff:+.3f} ({significance})\")\n",
    "            print(f\"  Wilcoxon p-value: {p_value:.5f}\")\n",
    "            print(f\"  Cohen's d: {cohens_d:.3f} ({effect_label} effect)\")\n",
    "            print(f\"  Interpretation: {'SIGNIFICANT improvement' if p_value < 0.05 and mean_diff > 0 else 'SIGNIFICANT decline' if p_value < 0.05 and mean_diff < 0 else 'No significant difference'}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2.2 Friedman Test (Multiple Methods Comparison)\n",
    "# ============================================================================\n",
    "all_methods = shapedd_methods + baseline_methods\n",
    "if len(all_methods) >= 3:\n",
    "    print(\"\\n--- 2.2 Friedman Test (Non-parametric ANOVA) ---\")\n",
    "    print(f\"(Comparing {len(all_methods)} methods simultaneously)\\n\")\n",
    "    \n",
    "    # Prepare data for Friedman test\n",
    "    scores_by_method = []\n",
    "    for method in all_methods:\n",
    "        method_scores = df_results[df_results['method'] == method]['f1_score'].values\n",
    "        scores_by_method.append(method_scores)\n",
    "    \n",
    "    # Check if all methods have same number of observations\n",
    "    lengths = [len(s) for s in scores_by_method]\n",
    "    if len(set(lengths)) == 1:\n",
    "        stat, p_value = friedmanchisquare(*scores_by_method)\n",
    "        \n",
    "        print(f\"Friedman statistic: χ² = {stat:.3f}\")\n",
    "        print(f\"p-value: {p_value:.5f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(f\"\\n✅ SIGNIFICANT difference detected among methods (p < 0.05)\")\n",
    "            print(f\"   Interpretation: At least one method performs significantly different from others.\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  No significant difference detected (p ≥ 0.05)\")\n",
    "    else:\n",
    "        print(f\"⚠️  Cannot perform Friedman test: methods have different sample sizes\")\n",
    "        print(f\"   Sample sizes: {dict(zip(all_methods, lengths))}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. SUMMARY TABLE FOR PUBLICATION\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"3. PUBLICATION-READY SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFormat: Method | Dataset | F1 ± std [95% CI] | β-score ± std\\n\")\n",
    "\n",
    "# Group by dataset and show top methods\n",
    "for dataset in df_results['dataset'].unique():\n",
    "    print(f\"\\n--- {dataset.upper()} ---\")\n",
    "    dataset_ci = df_ci[df_ci['dataset'] == dataset].sort_values('f1_mean', ascending=False)\n",
    "    \n",
    "    for _, row in dataset_ci.head(5).iterrows():\n",
    "        print(f\"{row['method']:25s} | \"\n",
    "              f\"F1: {row['f1_mean']:.3f} ± {row['f1_std']:.3f} [{row['f1_ci_low']:.3f}, {row['f1_ci_high']:.3f}] | \"\n",
    "              f\"β: {row['beta_mean']:.3f} ± {row['beta_std']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Statistical analysis complete!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: VISUALIZATION WITH CONFIDENCE INTERVALS\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS WITH STATISTICAL UNCERTAINTY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. F1 Score Comparison with 95% Confidence Intervals\n",
    "# ============================================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(f'F1 Score Comparison Across Methods (N={N_RUNS} runs, 95% CI)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "datasets = df_ci['dataset'].unique()\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    if idx >= 4:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Get data for this dataset\n",
    "    data = df_ci[df_ci['dataset'] == dataset].sort_values('f1_mean', ascending=True)\n",
    "    \n",
    "    # Calculate error bars (distance from mean to CI bounds)\n",
    "    yerr_lower = data['f1_mean'] - data['f1_ci_low']\n",
    "    yerr_upper = data['f1_ci_high'] - data['f1_mean']\n",
    "    yerr = [yerr_lower.values, yerr_upper.values]\n",
    "    \n",
    "    # Bar plot with error bars\n",
    "    bars = ax.barh(range(len(data)), data['f1_mean'], \n",
    "                   xerr=yerr, capsize=5, alpha=0.7,\n",
    "                   color=['#1f77b4' if 'ShapeDD' in m else '#ff7f0e' \n",
    "                          for m in data['method']])\n",
    "    \n",
    "    ax.set_yticks(range(len(data)))\n",
    "    ax.set_yticklabels(data['method'], fontsize=9)\n",
    "    ax.set_xlabel('F1 Score', fontsize=10)\n",
    "    ax.set_title(f'{dataset.upper()}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add mean values as text\n",
    "    for i, (mean, ci_low, ci_high) in enumerate(zip(data['f1_mean'], \n",
    "                                                      data['f1_ci_low'], \n",
    "                                                      data['f1_ci_high'])):\n",
    "        ax.text(mean + 0.02, i, f'{mean:.3f}\\n[{ci_low:.3f}, {ci_high:.3f}]', \n",
    "               va='center', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_comparison_with_ci.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: f1_comparison_with_ci.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. β-Score vs F1 Score Scatter with Confidence Ellipses\n",
    "# ============================================================================\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "colors = plt.cm.Set3(range(len(df_ci['method'].unique())))\n",
    "method_colors = dict(zip(df_ci['method'].unique(), colors))\n",
    "\n",
    "for method in df_ci['method'].unique():\n",
    "    method_data = df_ci[df_ci['method'] == method]\n",
    "    \n",
    "    # Plot points\n",
    "    ax.scatter(method_data['f1_mean'], method_data['beta_mean'], \n",
    "              s=100, alpha=0.7, label=method, color=method_colors[method])\n",
    "    \n",
    "    # Add error bars\n",
    "    for _, row in method_data.iterrows():\n",
    "        ax.errorbar(row['f1_mean'], row['beta_mean'],\n",
    "                   xerr=[[row['f1_mean'] - row['f1_ci_low']], \n",
    "                         [row['f1_ci_high'] - row['f1_mean']]],\n",
    "                   yerr=[[row['beta_mean'] - row['beta_ci_low']], \n",
    "                         [row['beta_ci_high'] - row['beta_mean']]],\n",
    "                   fmt='none', alpha=0.3, color=method_colors[method])\n",
    "\n",
    "ax.set_xlabel('F1 Score (β=1.0)', fontsize=12)\n",
    "ax.set_ylabel('β-Score (β=0.5, precision-focused)', fontsize=12)\n",
    "ax.set_title(f'F1 vs β-Score Comparison (N={N_RUNS} runs, 95% CI)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim(0, 1.0)\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Add diagonal line (F1 = β-score)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='F1 = β-score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_vs_beta_scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: f1_vs_beta_scatter.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Distribution of F1 Scores Across Runs (Box Plots)\n",
    "# ============================================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(f'F1 Score Distributions Across {N_RUNS} Runs', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    if idx >= 4:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Get data for this dataset\n",
    "    dataset_data = df_results[df_results['dataset'] == dataset]\n",
    "    \n",
    "    # Box plot\n",
    "    methods_in_dataset = dataset_data['method'].unique()\n",
    "    data_to_plot = [dataset_data[dataset_data['method'] == m]['f1_score'].values \n",
    "                    for m in methods_in_dataset]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=methods_in_dataset, \n",
    "                    patch_artist=True, vert=True)\n",
    "    \n",
    "    # Color ShapeDD variants differently\n",
    "    for patch, method in zip(bp['boxes'], methods_in_dataset):\n",
    "        if 'ShapeDD' in method:\n",
    "            patch.set_facecolor('#1f77b4')\n",
    "            patch.set_alpha(0.7)\n",
    "        else:\n",
    "            patch.set_facecolor('#ff7f0e')\n",
    "            patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_xticklabels(methods_in_dataset, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_ylabel('F1 Score', fontsize=10)\n",
    "    ax.set_title(f'{dataset.upper()}', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_distributions_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: f1_distributions_boxplot.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All visualizations complete!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: RESULTS SUMMARY AND ANALYSIS\n",
    "# ============================================================================\n",
    "# Consolidated results analysis with method rankings and performance metrics\n",
    "# IMPORTANT: Separates analysis by ground truth reliability\n",
    "\n",
    "if len(all_results) == 0:\n",
    "    print(\"No results to analyze. Run the benchmark first (Section 6).\")\n",
    "else:\n",
    "    # Convert results to DataFrame for analysis (with ground truth type)\n",
    "    results_df = pd.DataFrame([{\n",
    "        'Dataset': r['dataset'],\n",
    "        'Method': r['method'],\n",
    "        'Paradigm': r.get('paradigm', 'unknown'),\n",
    "        'N_Features': r.get('n_features', 0),\n",
    "        'N_Drifts': len(r.get('drift_positions', [])),\n",
    "        'Ground_Truth_Type': r.get('ground_truth_type', 'unknown'),\n",
    "        'Intensity': r.get('intensity', 0),\n",
    "        'TP': r.get('tp', 0),\n",
    "        'FP': r.get('fp', 0),\n",
    "        'FN': r.get('fn', 0),\n",
    "        'Precision': r.get('precision', 0.0),\n",
    "        'Recall': r.get('recall', 0.0),\n",
    "        'F1': r.get('f1_score', 0.0),\n",
    "        'MTTD': r.get('mttd', np.nan) if r.get('mttd') != float('inf') else np.nan,\n",
    "        'Detection_Rate': r.get('detection_rate', 0.0),\n",
    "        'N_Detections': r.get('n_detections', 0),\n",
    "        'Runtime_s': r.get('runtime_s', 0.0)\n",
    "    } for r in all_results])\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total experiments: {len(all_results)}\")\n",
    "    print(f\"Methods evaluated: {results_df['Method'].nunique()}\")\n",
    "    print(f\"Datasets used: {results_df['Dataset'].nunique()}\")\n",
    "    \n",
    "    # Show ground truth breakdown\n",
    "    gt_counts = results_df.groupby('Ground_Truth_Type')['Dataset'].nunique()\n",
    "    print(f\"\\nGround Truth Breakdown:\")\n",
    "    for gt_type, count in gt_counts.items():\n",
    "        print(f\"  - {gt_type}: {count} dataset(s)\")\n",
    "    \n",
    "    # Only use drift datasets for F1/Precision/Recall\n",
    "    drift_results_df = results_df[results_df['N_Drifts'] > 0].copy()\n",
    "    \n",
    "    if len(drift_results_df) > 0:\n",
    "        # Method rankings by F1\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"METHOD RANKINGS (by F1-Score)\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        method_f1 = drift_results_df.groupby('Method')['F1'].agg(['mean', 'std']).sort_values('mean', ascending=False)\n",
    "        \n",
    "        for rank, (method, row) in enumerate(method_f1.iterrows(), 1):\n",
    "            marker = \"★\" if rank == 1 else \" \"\n",
    "            print(f\"{marker} {rank}. {method:<30} F1 = {row['mean']:.3f} ± {row['std']:.3f}\")\n",
    "        \n",
    "        # Best method summary\n",
    "        best_method = method_f1.index[0]\n",
    "        best_stats = drift_results_df[drift_results_df['Method'] == best_method].agg({\n",
    "            'F1': 'mean', 'Precision': 'mean', 'Recall': 'mean', 'MTTD': 'mean'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nBest Method: {best_method}\")\n",
    "        print(f\"  F1-Score:  {best_stats['F1']:.3f}\")\n",
    "        print(f\"  Precision: {best_stats['Precision']:.3f}\")\n",
    "        print(f\"  Recall:    {best_stats['Recall']:.3f}\")\n",
    "        print(f\"  MTTD:      {best_stats['MTTD']:.1f} samples\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # GROUND TRUTH SEPARATION ANALYSIS\n",
    "        # ====================================================================\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"RESULTS BY GROUND TRUTH RELIABILITY\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"⚠️  NOTE: Only 'exact' ground truth results should be used for\")\n",
    "        print(\"    quantitative comparisons. 'Estimated' results are indicative only.\")\n",
    "        \n",
    "        # Exact ground truth (synthetic datasets - reliable metrics)\n",
    "        exact_results = drift_results_df[drift_results_df['Ground_Truth_Type'] == 'exact']\n",
    "        if len(exact_results) > 0:\n",
    "            print(f\"\\n[EXACT GROUND TRUTH] - {exact_results['Dataset'].nunique()} datasets\")\n",
    "            print(\"  (Synthetic datasets with known drift positions - RELIABLE)\")\n",
    "            exact_f1 = exact_results.groupby('Method')['F1'].agg(['mean', 'std']).sort_values('mean', ascending=False)\n",
    "            for rank, (method, row) in enumerate(exact_f1.iterrows(), 1):\n",
    "                marker = \"★\" if rank == 1 else \" \"\n",
    "                print(f\"  {marker} {rank}. {method:<28} F1 = {row['mean']:.3f} ± {row['std']:.3f}\")\n",
    "        \n",
    "        # Estimated ground truth (semi-real datasets - use with caution)\n",
    "        estimated_results = drift_results_df[drift_results_df['Ground_Truth_Type'] == 'estimated']\n",
    "        if len(estimated_results) > 0:\n",
    "            print(f\"\\n[ESTIMATED GROUND TRUTH] - {estimated_results['Dataset'].nunique()} datasets\")\n",
    "            print(\"  (Semi-real/incremental datasets - USE WITH CAUTION)\")\n",
    "            est_f1 = estimated_results.groupby('Method')['F1'].agg(['mean', 'std']).sort_values('mean', ascending=False)\n",
    "            for rank, (method, row) in enumerate(est_f1.iterrows(), 1):\n",
    "                print(f\"    {rank}. {method:<28} F1 = {row['mean']:.3f} ± {row['std']:.3f}\")\n",
    "        \n",
    "        # Stationary datasets (false positive analysis)\n",
    "        stationary_results = results_df[results_df['N_Drifts'] == 0]\n",
    "        if len(stationary_results) > 0:\n",
    "            print(f\"\\n[FALSE POSITIVE ANALYSIS] - Stationary datasets (no drift)\")\n",
    "            print(\"  (All detections on these datasets are FALSE POSITIVES)\")\n",
    "            fp_analysis = stationary_results.groupby('Method')['FP'].agg(['mean', 'sum']).sort_values('mean')\n",
    "            for method, row in fp_analysis.iterrows():\n",
    "                fp_rate = row['mean']\n",
    "                status = \"✅ LOW\" if fp_rate < 2 else \"⚠️  MODERATE\" if fp_rate < 5 else \"❌ HIGH\"\n",
    "                print(f\"    {method:<28} FP/run = {row['mean']:.1f} (total: {int(row['sum'])}) {status}\")\n",
    "        \n",
    "        # Runtime summary\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"RUNTIME ANALYSIS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        runtime_summary = results_df.groupby('Method')['Runtime_s'].agg(['mean', 'std', 'sum'])\n",
    "        runtime_summary = runtime_summary.sort_values('mean')\n",
    "        \n",
    "        for method, row in runtime_summary.iterrows():\n",
    "            throughput = STREAM_SIZE / row['mean'] if row['mean'] > 0 else 0\n",
    "            print(f\"  {method:<25} {row['mean']:>8.3f}s (±{row['std']:.3f}s) | {throughput:>8.0f} samples/s\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: PUBLICATION-QUALITY VISUALIZATIONS\n",
    "# ============================================================================\n",
    "# All thesis figures generated here with consistent styling\n",
    "# Figures saved to ./publication_figures/ in PNG and PDF formats\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"./publication_figures\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# UNIFIED PLOT STYLE (Publication Quality)\n",
    "# ============================================================================\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'DejaVu Serif'],\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 100,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.grid': False,\n",
    "    'axes.spines.top': True,\n",
    "    'axes.spines.right': True,\n",
    "})\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--', 'grid.alpha': 0.3})\n",
    "\n",
    "# Helper function to categorize datasets\n",
    "def categorize_dataset(name):\n",
    "    name_lower = name.lower()\n",
    "    if 'gradual' in name_lower or 'circles' in name_lower:\n",
    "        return 'B_Gradual'\n",
    "    elif 'rbf' in name_lower:\n",
    "        return 'C_Incremental'\n",
    "    elif 'electricity' in name_lower or 'covertype' in name_lower:\n",
    "        return 'D_Real-World'\n",
    "    elif 'none' in name_lower:\n",
    "        return 'E_Stationary'\n",
    "    else:\n",
    "        return 'A_Sudden'\n",
    "\n",
    "# Helper function to save figures\n",
    "def save_figure(fig, name):\n",
    "    for fmt in ['png', 'pdf']:\n",
    "        filepath = output_dir / f\"{name}.{fmt}\"\n",
    "        fig.savefig(filepath, dpi=300, bbox_inches='tight', format=fmt)\n",
    "    print(f\"  Saved: {name}.png/.pdf\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK PREREQUISITES\n",
    "# ============================================================================\n",
    "if 'all_results' not in dir() or len(all_results) == 0:\n",
    "    print(\"ERROR: No results found. Please run the benchmark (Cell 6) first.\")\n",
    "else:\n",
    "    # Create results_df if not exists (self-contained)\n",
    "    if 'results_df' not in dir():\n",
    "        results_df = pd.DataFrame([{\n",
    "            'Dataset': r['dataset'],\n",
    "            'Method': r['method'],\n",
    "            'N_Features': r.get('n_features', 0),\n",
    "            'N_Drifts': len(r.get('drift_positions', [])),\n",
    "            'Intensity': r.get('intensity', 0),\n",
    "            'TP': r.get('tp', 0),\n",
    "            'FP': r.get('fp', 0),\n",
    "            'FN': r.get('fn', 0),\n",
    "            'Precision': r.get('precision', 0.0),\n",
    "            'Recall': r.get('recall', 0.0),\n",
    "            'F1': r.get('f1_score', 0.0),\n",
    "            'MTTD': r.get('mttd', np.nan) if r.get('mttd') != float('inf') else np.nan,\n",
    "            'Detection_Rate': r.get('detection_rate', 0.0),\n",
    "            'N_Detections': r.get('n_detections', 0),\n",
    "            'Runtime_s': r.get('runtime_s', 0.0)\n",
    "        } for r in all_results])\n",
    "    \n",
    "    drift_results = results_df[results_df['N_Drifts'] > 0].copy()\n",
    "    \n",
    "    if len(drift_results) == 0:\n",
    "        print(\"ERROR: No drift datasets found in results.\")\n",
    "    else:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"GENERATING THESIS FIGURES\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FIGURE 1: F1-Score Heatmap\n",
    "        # ========================================================================\n",
    "        print(\"\\n[1/8] F1-Score Heatmap...\")\n",
    "        \n",
    "        f1_pivot = drift_results.pivot_table(values='F1', index='Method', columns='Dataset', aggfunc='mean')\n",
    "        f1_pivot['_avg'] = f1_pivot.mean(axis=1)\n",
    "        f1_pivot = f1_pivot.sort_values('_avg', ascending=False).drop('_avg', axis=1)\n",
    "        \n",
    "        # Sort datasets by category\n",
    "        dataset_cats = {col: categorize_dataset(col) for col in f1_pivot.columns}\n",
    "        sorted_cols = sorted(f1_pivot.columns, key=lambda x: (dataset_cats[x], x))\n",
    "        f1_pivot = f1_pivot[sorted_cols]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(max(12, len(sorted_cols) * 1.5), max(6, len(f1_pivot) * 0.7)))\n",
    "        sns.heatmap(f1_pivot, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1,\n",
    "                    cbar_kws={'label': 'F1-Score', 'shrink': 0.8},\n",
    "                    linewidths=0.5, linecolor='white', annot_kws={'fontsize': 11, 'weight': 'bold'}, ax=ax)\n",
    "        ax.set_title('F1-Score by Method and Dataset', fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Detection Method', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"figure_1_f1_heatmap\")\n",
    "        plt.show()\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FIGURE 2: Precision Heatmap\n",
    "        # ========================================================================\n",
    "        print(\"\\n[2/8] Precision Heatmap...\")\n",
    "        \n",
    "        prec_pivot = drift_results.pivot_table(values='Precision', index='Method', columns='Dataset', aggfunc='mean')\n",
    "        prec_pivot['_avg'] = prec_pivot.mean(axis=1)\n",
    "        prec_pivot = prec_pivot.sort_values('_avg', ascending=False).drop('_avg', axis=1)\n",
    "        prec_pivot = prec_pivot[sorted_cols]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(max(12, len(sorted_cols) * 1.5), max(6, len(prec_pivot) * 0.7)))\n",
    "        sns.heatmap(prec_pivot, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1,\n",
    "                    cbar_kws={'label': 'Precision', 'shrink': 0.8},\n",
    "                    linewidths=0.5, linecolor='white', annot_kws={'fontsize': 11, 'weight': 'bold'}, ax=ax)\n",
    "        ax.set_title('Precision by Method and Dataset', fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Detection Method', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"figure_2_precision_heatmap\")\n",
    "        plt.show()\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FIGURE 3: Recall Heatmap\n",
    "        # ========================================================================\n",
    "        print(\"\\n[3/8] Recall Heatmap...\")\n",
    "        \n",
    "        recall_pivot = drift_results.pivot_table(values='Recall', index='Method', columns='Dataset', aggfunc='mean')\n",
    "        recall_pivot['_avg'] = recall_pivot.mean(axis=1)\n",
    "        recall_pivot = recall_pivot.sort_values('_avg', ascending=False).drop('_avg', axis=1)\n",
    "        recall_pivot = recall_pivot[sorted_cols]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(max(12, len(sorted_cols) * 1.5), max(6, len(recall_pivot) * 0.7)))\n",
    "        sns.heatmap(recall_pivot, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1,\n",
    "                    cbar_kws={'label': 'Recall', 'shrink': 0.8},\n",
    "                    linewidths=0.5, linecolor='white', annot_kws={'fontsize': 11, 'weight': 'bold'}, ax=ax)\n",
    "        ax.set_title('Recall by Method and Dataset', fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Detection Method', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"figure_3_recall_heatmap\")\n",
    "        plt.show()\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FIGURE 4: MTTD Heatmap\n",
    "        # ========================================================================\n",
    "        print(\"\\n[4/8] MTTD Heatmap...\")\n",
    "        \n",
    "        mttd_pivot = drift_results.pivot_table(values='MTTD', index='Method', columns='Dataset', aggfunc='mean')\n",
    "        mttd_pivot['_avg'] = mttd_pivot.mean(axis=1)\n",
    "        mttd_pivot = mttd_pivot.sort_values('_avg', ascending=True).drop('_avg', axis=1)\n",
    "        mttd_pivot = mttd_pivot[sorted_cols]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(max(12, len(sorted_cols) * 1.5), max(6, len(mttd_pivot) * 0.7)))\n",
    "        sns.heatmap(mttd_pivot, annot=True, fmt='.0f', cmap='RdYlGn_r',\n",
    "                    cbar_kws={'label': 'MTTD (samples)', 'shrink': 0.8},\n",
    "                    linewidths=0.5, linecolor='white', annot_kws={'fontsize': 11, 'weight': 'bold'}, ax=ax)\n",
    "        ax.set_title('Mean Time To Detection (MTTD) by Method and Dataset', fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Detection Method', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"figure_4_mttd_heatmap\")\n",
    "        plt.show()\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FIGURE 5: Method Comparison Bar Chart\n",
    "        # ========================================================================\n",
    "        print(\"\\n[5/8] Method Comparison Bar Chart...\")\n",
    "        \n",
    "        method_summary = drift_results.groupby('Method').agg({\n",
    "            'F1': 'mean', 'Precision': 'mean', 'Recall': 'mean'\n",
    "        }).round(3).sort_values('F1', ascending=False)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "        \n",
    "        colors = ['steelblue', 'forestgreen', 'coral']\n",
    "        titles = ['F1-Score', 'Precision', 'Recall']\n",
    "        \n",
    "        for ax, col, color, title in zip(axes, ['F1', 'Precision', 'Recall'], colors, titles):\n",
    "            method_summary[col].plot(kind='barh', ax=ax, color=color, edgecolor='black', alpha=0.8)\n",
    "            ax.set_xlabel(title, fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'{title} by Method', fontsize=12, fontweight='bold')\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            for i, v in enumerate(method_summary[col]):\n",
    "                ax.text(v + 0.02, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "        \n",
    "        axes[1].set_ylabel('')\n",
    "        axes[2].set_ylabel('')\n",
    "        \n",
    "        plt.suptitle('Method Performance Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"figure_5_method_comparison\")\n",
    "        plt.show()\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FIGURE 6: Detection Timeline\n",
    "        # ========================================================================\n",
    "        print(\"\\n[6/8] Detection Timelines...\")\n",
    "        \n",
    "        datasets = drift_results['Dataset'].unique()\n",
    "        \n",
    "        for dataset_name in datasets:\n",
    "            dataset_results = [r for r in all_results if r['dataset'] == dataset_name]\n",
    "            if not dataset_results:\n",
    "                continue\n",
    "            \n",
    "            true_drifts = dataset_results[0].get('drift_positions', [])\n",
    "            n_drifts = len(true_drifts)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, max(4, len(dataset_results) * 0.5)))\n",
    "            \n",
    "            for i, drift_pos in enumerate(true_drifts):\n",
    "                ax.axvline(drift_pos, color='red', linestyle='--', linewidth=2,\n",
    "                          alpha=0.7, label='True Drift' if i == 0 else '')\n",
    "            \n",
    "            for idx, result in enumerate(dataset_results):\n",
    "                detections = result.get('detections', [])\n",
    "                method = result['method']\n",
    "                f1 = result.get('f1_score', 0)\n",
    "                \n",
    "                if detections:\n",
    "                    ax.scatter(detections, [idx]*len(detections), s=80, alpha=0.7,\n",
    "                              label=f\"{method} (F1={f1:.2f})\")\n",
    "            \n",
    "            ax.set_yticks(range(len(dataset_results)))\n",
    "            ax.set_yticklabels([r['method'] for r in dataset_results])\n",
    "            ax.set_xlabel('Sample Index', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel('Method', fontsize=11, fontweight='bold')\n",
    "            ax.set_title(f'Detection Timeline - {dataset_name} ({n_drifts} drifts)',\n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), fontsize=9)\n",
    "            ax.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            save_figure(fig, f\"figure_6_timeline_{dataset_name}\")\n",
    "            plt.show()\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FIGURE 7: Runtime Comparison\n",
    "        # ========================================================================\n",
    "        print(\"\\n[7/8] Runtime Comparison...\")\n",
    "        \n",
    "        runtime_summary = results_df.groupby('Method').agg({\n",
    "            'Runtime_s': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        runtime_summary.columns = ['Runtime_mean', 'Runtime_std']\n",
    "        runtime_summary = runtime_summary.sort_values('Runtime_mean')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        y_pos = range(len(runtime_summary))\n",
    "        bars = ax.barh(y_pos, runtime_summary['Runtime_mean'], \n",
    "                       xerr=runtime_summary['Runtime_std'],\n",
    "                       color='steelblue', edgecolor='black', alpha=0.8, capsize=5)\n",
    "        \n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(runtime_summary.index)\n",
    "        ax.set_xlabel('Runtime (seconds)', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Method', fontsize=11, fontweight='bold')\n",
    "        ax.set_title('Average Runtime by Method', fontsize=12, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        for i, (mean, std) in enumerate(zip(runtime_summary['Runtime_mean'], runtime_summary['Runtime_std'])):\n",
    "            ax.text(mean + std + 0.01, i, f'{mean:.3f}s', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"figure_7_runtime_comparison\")\n",
    "        plt.show()\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FIGURE 8: Speed-Accuracy Trade-off\n",
    "        # ========================================================================\n",
    "        print(\"\\n[8/8] Speed-Accuracy Trade-off...\")\n",
    "        \n",
    "        tradeoff = drift_results.groupby('Method').agg({\n",
    "            'F1': 'mean', 'Runtime_s': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        scatter = ax.scatter(tradeoff['Runtime_s'], tradeoff['F1'],\n",
    "                            s=200, c=range(len(tradeoff)), cmap='tab10',\n",
    "                            edgecolors='black', linewidths=1.5, alpha=0.8)\n",
    "        \n",
    "        for method, row in tradeoff.iterrows():\n",
    "            ax.annotate(method, (row['Runtime_s'], row['F1']),\n",
    "                       xytext=(8, 8), textcoords='offset points',\n",
    "                       fontsize=10, fontweight='bold')\n",
    "        \n",
    "        pareto_points = []\n",
    "        max_f1 = -1\n",
    "        for _, row in tradeoff.sort_values('Runtime_s').iterrows():\n",
    "            if row['F1'] > max_f1:\n",
    "                pareto_points.append(row)\n",
    "                max_f1 = row['F1']\n",
    "        \n",
    "        if len(pareto_points) > 1:\n",
    "            pareto_df = pd.DataFrame(pareto_points)\n",
    "            ax.plot(pareto_df['Runtime_s'], pareto_df['F1'],\n",
    "                   'r--', linewidth=2, alpha=0.7, label='Pareto Frontier')\n",
    "        \n",
    "        ax.set_xlabel('Runtime (seconds)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Speed-Accuracy Trade-off', fontsize=14, fontweight='bold')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend(loc='lower right')\n",
    "        \n",
    "        median_runtime = tradeoff['Runtime_s'].median()\n",
    "        median_f1 = tradeoff['F1'].median()\n",
    "        ax.axvline(median_runtime, color='gray', linestyle=':', alpha=0.5)\n",
    "        ax.axhline(median_f1, color='gray', linestyle=':', alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"figure_8_speed_accuracy_tradeoff\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"All figures saved to: {output_dir.absolute()}\")\n",
    "        print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 9: LATEX TABLE EXPORT FOR THESIS\n",
    "# ============================================================================\n",
    "# Publication-ready LaTeX tables for thesis inclusion\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_dir = Path(\"./publication_figures\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK PREREQUISITES\n",
    "# ============================================================================\n",
    "if 'all_results' not in dir() or len(all_results) == 0:\n",
    "    print(\"ERROR: No results found. Please run the benchmark (Cell 6) first.\")\n",
    "else:\n",
    "    # Create results_df if not exists (self-contained)\n",
    "    if 'results_df' not in dir():\n",
    "        results_df = pd.DataFrame([{\n",
    "            'Dataset': r['dataset'],\n",
    "            'Method': r['method'],\n",
    "            'N_Features': r.get('n_features', 0),\n",
    "            'N_Drifts': len(r.get('drift_positions', [])),\n",
    "            'Intensity': r.get('intensity', 0),\n",
    "            'TP': r.get('tp', 0),\n",
    "            'FP': r.get('fp', 0),\n",
    "            'FN': r.get('fn', 0),\n",
    "            'Precision': r.get('precision', 0.0),\n",
    "            'Recall': r.get('recall', 0.0),\n",
    "            'F1': r.get('f1_score', 0.0),\n",
    "            'MTTD': r.get('mttd', np.nan) if r.get('mttd') != float('inf') else np.nan,\n",
    "            'Detection_Rate': r.get('detection_rate', 0.0),\n",
    "            'N_Detections': r.get('n_detections', 0),\n",
    "            'Runtime_s': r.get('runtime_s', 0.0)\n",
    "        } for r in all_results])\n",
    "    \n",
    "    drift_results_df = results_df[results_df['N_Drifts'] > 0].copy()\n",
    "    \n",
    "    if len(drift_results_df) == 0:\n",
    "        print(\"ERROR: No drift datasets found in results.\")\n",
    "    else:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"LATEX TABLE EXPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # ========================================================================\n",
    "        # TABLE I: Comprehensive Performance Summary\n",
    "        # ========================================================================\n",
    "        print(\"\\n[Table I] Comprehensive Performance Summary\")\n",
    "        \n",
    "        method_stats = drift_results_df.groupby('Method').agg({\n",
    "            'F1': ['mean', 'std'],\n",
    "            'Precision': 'mean',\n",
    "            'Recall': 'mean',\n",
    "            'MTTD': 'mean',\n",
    "            'TP': 'sum',\n",
    "            'FP': 'sum',\n",
    "            'FN': 'sum'\n",
    "        }).round(4)\n",
    "        \n",
    "        pub_table = pd.DataFrame({\n",
    "            'Method': method_stats.index,\n",
    "            'F1': method_stats[('F1', 'mean')],\n",
    "            'F1_std': method_stats[('F1', 'std')],\n",
    "            'Precision': method_stats[('Precision', 'mean')],\n",
    "            'Recall': method_stats[('Recall', 'mean')],\n",
    "            'MTTD': method_stats[('MTTD', 'mean')].fillna(0).astype(int),\n",
    "            'TP': method_stats[('TP', 'sum')].astype(int),\n",
    "            'FP': method_stats[('FP', 'sum')].astype(int),\n",
    "            'FN': method_stats[('FN', 'sum')].astype(int)\n",
    "        })\n",
    "        \n",
    "        pub_table = pub_table.sort_values('F1', ascending=False).reset_index(drop=True)\n",
    "        pub_table['F1_formatted'] = pub_table.apply(\n",
    "            lambda row: f\"${row['F1']:.3f} \\\\pm {row['F1_std']:.3f}$\", axis=1\n",
    "        )\n",
    "        \n",
    "        latex_table = pub_table[['Method', 'F1_formatted', 'Precision', 'Recall', 'MTTD', 'TP', 'FP', 'FN']].copy()\n",
    "        latex_table.columns = ['Method', 'F1 ($\\\\mu \\\\pm \\\\sigma$)', 'Precision', 'Recall', 'MTTD', 'TP', 'FP', 'FN']\n",
    "        \n",
    "        for col in ['Precision', 'Recall']:\n",
    "            latex_table[col] = latex_table[col].apply(lambda x: f\"{x:.3f}\")\n",
    "        \n",
    "        latex_output = latex_table.to_latex(\n",
    "            index=False,\n",
    "            escape=False,\n",
    "            column_format='l' + 'c' * (len(latex_table.columns) - 1),\n",
    "            caption='Comprehensive drift detection performance. F1 is reported as mean $\\\\pm$ standard deviation across all datasets. MTTD = Mean Time To Detection (samples). TP/FP/FN = cumulative counts.',\n",
    "            label='tab:comprehensive_performance',\n",
    "            position='htbp'\n",
    "        )\n",
    "        \n",
    "        latex_file = output_dir / \"table_I_comprehensive_performance.tex\"\n",
    "        with open(latex_file, 'w') as f:\n",
    "            f.write(latex_output)\n",
    "        print(f\"  Saved: {latex_file}\")\n",
    "        \n",
    "        print(\"\\nTable I Preview:\")\n",
    "        print(latex_table.to_string(index=False))\n",
    "        \n",
    "        # ========================================================================\n",
    "        # TABLE II: Performance by Dataset\n",
    "        # ========================================================================\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"[Table II] Performance by Dataset\")\n",
    "        \n",
    "        f1_by_dataset = drift_results_df.pivot_table(\n",
    "            values='F1', index='Method', columns='Dataset', aggfunc='mean'\n",
    "        ).round(3)\n",
    "        \n",
    "        f1_by_dataset['Mean'] = f1_by_dataset.mean(axis=1).round(3)\n",
    "        f1_by_dataset = f1_by_dataset.sort_values('Mean', ascending=False)\n",
    "        \n",
    "        latex_dataset = f1_by_dataset.to_latex(\n",
    "            escape=False,\n",
    "            column_format='l' + 'c' * len(f1_by_dataset.columns),\n",
    "            caption='F1-Score by method and dataset. Best scores per dataset are highlighted.',\n",
    "            label='tab:f1_by_dataset',\n",
    "            position='htbp'\n",
    "        )\n",
    "        \n",
    "        latex_file2 = output_dir / \"table_II_f1_by_dataset.tex\"\n",
    "        with open(latex_file2, 'w') as f:\n",
    "            f.write(latex_dataset)\n",
    "        print(f\"  Saved: {latex_file2}\")\n",
    "        \n",
    "        print(\"\\nTable II Preview:\")\n",
    "        print(f1_by_dataset.to_string())\n",
    "        \n",
    "        # ========================================================================\n",
    "        # TABLE III: Runtime Statistics\n",
    "        # ========================================================================\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"[Table III] Runtime Statistics\")\n",
    "        \n",
    "        runtime_stats = results_df.groupby('Method').agg({\n",
    "            'Runtime_s': ['mean', 'std', 'min', 'max']\n",
    "        }).round(4)\n",
    "        runtime_stats.columns = ['Mean (s)', 'Std (s)', 'Min (s)', 'Max (s)']\n",
    "        \n",
    "        # Handle NaN/inf values in throughput calculation\n",
    "        throughput = STREAM_SIZE / runtime_stats['Mean (s)']\n",
    "        throughput = throughput.replace([np.inf, -np.inf], np.nan)\n",
    "        runtime_stats['Throughput (samples/s)'] = throughput.fillna(0).round(0).astype(int)\n",
    "        \n",
    "        runtime_stats = runtime_stats.sort_values('Mean (s)')\n",
    "        \n",
    "        latex_runtime = runtime_stats.to_latex(\n",
    "            escape=False,\n",
    "            column_format='l' + 'c' * len(runtime_stats.columns),\n",
    "            caption='Runtime statistics by detection method. Throughput = samples processed per second.',\n",
    "            label='tab:runtime_stats',\n",
    "            position='htbp'\n",
    "        )\n",
    "        \n",
    "        latex_file3 = output_dir / \"table_III_runtime_stats.tex\"\n",
    "        with open(latex_file3, 'w') as f:\n",
    "            f.write(latex_runtime)\n",
    "        print(f\"  Saved: {latex_file3}\")\n",
    "        \n",
    "        print(\"\\nTable III Preview:\")\n",
    "        print(runtime_stats.to_string())\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"All LaTeX tables saved to: {output_dir.absolute()}\")\n",
    "        print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
