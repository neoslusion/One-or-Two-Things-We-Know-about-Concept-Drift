{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274425e5",
   "metadata": {},
   "source": [
    "\n",
    "# Concept Drift Benchmark ‚Äî Enhanced Baseline (v3)\n",
    "\n",
    "**üéØ M·ª•c ƒë√≠ch**: Comprehensive benchmark c·ªßa drift detection methods v·ªõi evaluation framework n√¢ng cao\n",
    "\n",
    "**üîß Detectors**: \n",
    "- **Custom**: ShapeDD (kernel-based MMD)\n",
    "- **Traditional**: DDM, Page-Hinkley, ADWIN, MDDM, FHDDM/FHDDMS  \n",
    "- **River Library**: EDDM, HDDM_A, HDDM_W, KSWIN\n",
    "\n",
    "**üìä Datasets**: \n",
    "- **Synthetic**: SEA, Rotating Hyperplane, LED (abrupt/gradual), Interchanging RBF\n",
    "- **Real-world**: Elec2, RandomRBFDrift\n",
    "\n",
    "**üìà Advanced Metrics**: \n",
    "- **Detection Quality**: Œ≤-score, F1@AR, Accuracy@AR, Global Scores\n",
    "- **Statistical Analysis**: Delay distributions, permutation tests\n",
    "- **Visualization**: Timeline plots, comparative analysis\n",
    "\n",
    "> **v3 Updates**: T√≠ch h·ª£p River library, metrics n√¢ng cao, automated reporting, v√† framework t·ªëi ∆∞u h√≥a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee209ef",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Evaluation Framework & Metrics\n",
    "\n",
    "### **üéØ Classification Performance (Prediction Quality)**\n",
    "- **Prequential Accuracy**: \\( \\text{Acc} = \\frac{\\sum_{t} \\mathbb{1}(\\hat{y}_t = y_t)}{T} \\) (predict-then-update protocol)\n",
    "- **Macro-F1**: Balanced across classes to handle imbalanced streams\n",
    "  - \\( \\text{Precision}_k = \\frac{TP_k}{TP_k + FP_k} \\), \\( \\text{Recall}_k = \\frac{TP_k}{TP_k + FN_k} \\)\n",
    "  - \\( F1_k = \\frac{2\\cdot \\text{Prec}_k \\cdot \\text{Rec}_k}{\\text{Prec}_k + \\text{Rec}_k} \\), **Macro-F1** = \\( \\frac{1}{K}\\sum_k F1_k \\)\n",
    "\n",
    "### **üö® Drift Detection Quality**\n",
    "- **True Positives (TP)**: Detections matching ground truth drifts (within tolerance)\n",
    "- **False Alarms (FA)**: Detections not matching any true drift\n",
    "- **Detection Delay**: Mean latency from true drift to first alarm\n",
    "- **Œ≤-score**: \\( \\frac{TP}{P + \\beta \\cdot FP} \\) (Œ≤=0.5 balances precision/recall)\n",
    "\n",
    "### **‚ö° Advanced Metrics (v3 New)**\n",
    "- **Alarm Rate (AR)**: \\( \\frac{\\text{#Alarms}}{N} \\times 10^4 \\) per 10k samples\n",
    "- **F1@AR**: \\( F1 - \\lambda \\cdot AR \\) (Œª=0.01, penalizes excessive alarms)\n",
    "- **Accuracy@AR**: \\( Acc - \\lambda \\cdot AR \\) (Œª=0.01)\n",
    "- **Global Scores**: Min-max normalized across datasets ‚Üí macro-average\n",
    "\n",
    "### **üìà Statistical Analysis**\n",
    "- **Delay Distributions**: mean, median, percentiles\n",
    "- **Permutation Tests**: p-value validation for statistical significance\n",
    "- **Confidence Intervals**: Bootstrap analysis for robust results\n",
    "\n",
    "> **üéØ Ideal Detector**: High Acc/F1, Low Delay/FA, Balanced F1@AR score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987a2183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  River library not available. Install with: pip install river\n",
      "üöÄ Enhanced Concept Drift Benchmark v3 - Setup Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üì¶ Dependencies & Setup\n",
    "import math, random, time, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# River library for drift detection (install if needed)\n",
    "try:\n",
    "    from river import drift as river_drift\n",
    "    from river import tree, metrics as river_metrics\n",
    "    RIVER_AVAILABLE = True\n",
    "    print(\"‚úÖ River library loaded successfully\")\n",
    "except ImportError:\n",
    "    RIVER_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  River library not available. Install with: pip install river\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ Enhanced Concept Drift Benchmark v3 - Setup Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c60d",
   "metadata": {},
   "source": [
    "## üåä 1) Enhanced Stream Generators Framework\n",
    "\n",
    "> **v3 Enhancement**: Unified interface v·ªõi parameter validation v√† metadata tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b880d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base stream framework initialized\n"
     ]
    }
   ],
   "source": [
    "# üèóÔ∏è Base Stream Generator Framework\n",
    "\n",
    "@dataclass\n",
    "class StreamMetadata:\n",
    "    \"\"\"Metadata for stream characteristics\"\"\"\n",
    "    name: str\n",
    "    length: int\n",
    "    n_features: int\n",
    "    n_classes: int\n",
    "    drift_points: List[int]\n",
    "    drift_type: str  # 'abrupt', 'gradual', 'recurring', 'incremental'\n",
    "    noise_level: float\n",
    "    description: str\n",
    "\n",
    "class BaseStreamGenerator:\n",
    "    \"\"\"Base class for all stream generators with unified interface\"\"\"\n",
    "    \n",
    "    def __init__(self, length: int, drift_points: List[int], noise: float = 0.0):\n",
    "        self.length = length\n",
    "        self.drift_points = list(drift_points) if drift_points else []\n",
    "        self.noise = noise\n",
    "        \n",
    "    def generate(self) -> Tuple[np.ndarray, np.ndarray, List[int]]:\n",
    "        \"\"\"Generate stream data. Returns (X, y, drift_points)\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_metadata(self) -> StreamMetadata:\n",
    "        \"\"\"Get stream metadata for analysis\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def validate_parameters(self):\n",
    "        \"\"\"Validate generator parameters\"\"\"\n",
    "        assert self.length > 0, \"Stream length must be positive\"\n",
    "        assert 0 <= self.noise <= 1, \"Noise must be between 0 and 1\"\n",
    "        assert all(0 < dp < self.length for dp in self.drift_points), \"Drift points must be within stream bounds\"\n",
    "\n",
    "print(\"‚úÖ Base stream framework initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea5768de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SEAStream(BaseStreamGenerator):\n",
    "    \"\"\"\n",
    "    SEA Concepts: Binary classification with abrupt threshold changes\n",
    "    \n",
    "    Description: X‚ÇÅ + X‚ÇÇ ‚â§ threshold determines class. Threshold changes at drift points.\n",
    "    Features: 3D uniform random [0,10], only first 2 dimensions relevant\n",
    "    Drift Type: Abrupt concept drift via threshold modification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, length=10000, thresholds=(7, 8, 9, 9.5), drift_points=(2500, 5000, 7500), noise=0.0):\n",
    "        super().__init__(length, drift_points, noise)\n",
    "        self.thresholds = thresholds\n",
    "        self.validate_parameters()\n",
    "        \n",
    "    def validate_parameters(self):\n",
    "        super().validate_parameters()\n",
    "        assert len(self.thresholds) == len(self.drift_points) + 1, \\\n",
    "            \"Number of thresholds must be drift_points + 1\"\n",
    "        assert all(isinstance(t, (int, float)) for t in self.thresholds), \\\n",
    "            \"All thresholds must be numeric\"\n",
    "\n",
    "    def generate(self) -> Tuple[np.ndarray, np.ndarray, List[int]]:\n",
    "        # Create timeline with thresholds\n",
    "        timeline_points = [0] + self.drift_points + [self.length]\n",
    "        \n",
    "        def get_threshold_for_time(t):\n",
    "            for k in range(len(timeline_points)-1):\n",
    "                if timeline_points[k] <= t < timeline_points[k+1]:\n",
    "                    return self.thresholds[k]\n",
    "            return self.thresholds[-1]\n",
    "        \n",
    "        # Generate features: 3D uniform [0,10]\n",
    "        X = np.random.rand(self.length, 3) * 10.0\n",
    "        y = np.zeros(self.length, dtype=int)\n",
    "        \n",
    "        # Apply decision rule with time-varying threshold\n",
    "        for i in range(self.length):\n",
    "            threshold = get_threshold_for_time(i)\n",
    "            decision_value = X[i,0] + X[i,1]  # Only first 2 features matter\n",
    "            label = 0 if decision_value <= threshold else 1\n",
    "            \n",
    "            # Add noise if specified\n",
    "            if self.noise > 0 and np.random.rand() < self.noise:\n",
    "                label = 1 - label\n",
    "                \n",
    "            y[i] = label\n",
    "            \n",
    "        return X, y, self.drift_points\n",
    "    \n",
    "    def get_metadata(self) -> StreamMetadata:\n",
    "        return StreamMetadata(\n",
    "            name=\"SEA\",\n",
    "            length=self.length,\n",
    "            n_features=3,\n",
    "            n_classes=2,\n",
    "            drift_points=self.drift_points,\n",
    "            drift_type=\"abrupt\",\n",
    "            noise_level=self.noise,\n",
    "            description=f\"SEA concepts with thresholds {self.thresholds}, decision rule: X‚ÇÅ+X‚ÇÇ ‚â§ threshold\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7096c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RotatingHyperplane(BaseStreamGenerator):\n",
    "    \"\"\"\n",
    "    Rotating Hyperplane: Incremental drift via gradually rotating decision boundary\n",
    "    \n",
    "    Description: Decision boundary normal vector rotates continuously in 2D subspace\n",
    "    Features: d-dimensional Gaussian random vectors\n",
    "    Drift Type: Incremental/gradual drift with optional abrupt sign flips\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, length=10000, d=10, angle_per_step=2*np.pi/20000, noise=0.0, abrupt_points=()):\n",
    "        # Estimate drift points based on rotation\n",
    "        estimated_drifts = [length//3, 2*length//3] if not abrupt_points else list(abrupt_points)\n",
    "        super().__init__(length, estimated_drifts, noise)\n",
    "        \n",
    "        self.d = d\n",
    "        self.angle_per_step = angle_per_step\n",
    "        self.abrupt_points = set(abrupt_points)\n",
    "        self.validate_parameters()\n",
    "        \n",
    "    def validate_parameters(self):\n",
    "        super().validate_parameters()\n",
    "        assert self.d >= 2, \"Dimensionality must be at least 2\"\n",
    "        assert self.angle_per_step > 0, \"Angle per step must be positive\"\n",
    "        \n",
    "    def generate(self) -> Tuple[np.ndarray, np.ndarray, List[int]]:\n",
    "        # Generate d-dimensional Gaussian features\n",
    "        X = np.random.randn(self.length, self.d)\n",
    "        y = np.zeros(self.length, dtype=int)\n",
    "        \n",
    "        angle = 0.0\n",
    "        for i in range(self.length):\n",
    "            # Gradually rotate the decision boundary\n",
    "            angle += self.angle_per_step\n",
    "            \n",
    "            # Create normal vector in first 2 dimensions\n",
    "            cos_a, sin_a = math.cos(angle), math.sin(angle)\n",
    "            w = np.zeros(self.d)\n",
    "            w[0], w[1] = cos_a, sin_a\n",
    "            \n",
    "            # Abrupt sign flip at specified points\n",
    "            if i in self.abrupt_points:\n",
    "                w = -w\n",
    "                \n",
    "            # Classify based on hyperplane\n",
    "            decision_score = X[i].dot(w)\n",
    "            label = 1 if decision_score >= 0 else 0\n",
    "            \n",
    "            # Add noise\n",
    "            if self.noise > 0 and np.random.rand() < self.noise:\n",
    "                label = 1 - label\n",
    "                \n",
    "            y[i] = label\n",
    "            \n",
    "        return X, y, self.drift_points\n",
    "    \n",
    "    def get_metadata(self) -> StreamMetadata:\n",
    "        return StreamMetadata(\n",
    "            name=\"RotatingHyperplane\",\n",
    "            length=self.length,\n",
    "            n_features=self.d,\n",
    "            n_classes=2,\n",
    "            drift_points=self.drift_points,\n",
    "            drift_type=\"incremental\" if not self.abrupt_points else \"mixed\",\n",
    "            noise_level=self.noise,\n",
    "            description=f\"Rotating hyperplane in {self.d}D, angle_step={self.angle_per_step:.6f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01f49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seven_segment_digit(bits7):\n",
    "    # map 7 segments to a digit index by pattern (simplified; not unique)\n",
    "    return int(sum(bits7) % 10)\n",
    "\n",
    "class LEDStream:\n",
    "    \"\"\"LED generator: 24 binary attrs, 7 quan tr·ªçng. \n",
    "    - abrupt: t·∫°i c√°c m·ªëc drift -> ho√°n v·ªã (permute) v·ªã tr√≠ 7 bit quan tr·ªçng.\n",
    "    - gradual: chuy·ªÉn d·∫ßn t·ª´ mapping c≈© sang m·ªõi trong kho·∫£ng g_len.\n",
    "    \"\"\"\n",
    "    def __init__(self, length=10000, mode='abrupt', drift_points=(3000, 6000, 8000), g_len=500, noise=0.05):\n",
    "        self.length=length; self.mode=mode\n",
    "        self.drift_points=list(drift_points); self.g_len=g_len; self.noise=noise\n",
    "\n",
    "    def generate(self):\n",
    "        d=24\n",
    "        important = list(range(7))  # initial 7 important indices\n",
    "        X = np.zeros((self.length, d), dtype=int)\n",
    "        y = np.zeros(self.length, dtype=int)\n",
    "        perm = list(range(d))\n",
    "        next_perm = perm.copy()\n",
    "        dp_idx=0\n",
    "        def new_mapping(old_imp):\n",
    "            # pick 7 new distinct indices uniformly\n",
    "            cand = list(range(d))\n",
    "            np.random.shuffle(cand)\n",
    "            return sorted(cand[:7])\n",
    "        pending = None  # for gradual\n",
    "\n",
    "        for t in range(self.length):\n",
    "            # feature generation\n",
    "            X[t] = (np.random.rand(d)<0.5).astype(int)\n",
    "            # gradually change mapping if needed\n",
    "            if self.mode=='abrupt':\n",
    "                if dp_idx < len(self.drift_points) and t==self.drift_points[dp_idx]:\n",
    "                    important = new_mapping(important)\n",
    "                    dp_idx+=1\n",
    "            else:  # gradual\n",
    "                if dp_idx < len(self.drift_points) and t==self.drift_points[dp_idx]:\n",
    "                    pending = new_mapping(important)\n",
    "                    start = t\n",
    "                    dp_idx+=1\n",
    "                if pending is not None:\n",
    "                    alpha = min(1.0, (t - start)/max(1,self.g_len))\n",
    "                    # probabilistically choose new mapping\n",
    "                    if np.random.rand()<alpha:\n",
    "                        important = pending\n",
    "                        pending = None\n",
    "\n",
    "            bits7 = X[t, important]\n",
    "            lbl = seven_segment_digit(bits7)\n",
    "            if self.noise>0 and np.random.rand()<self.noise:\n",
    "                lbl = (lbl + np.random.randint(1,10))%10\n",
    "            y[t]=lbl\n",
    "        return X, y, self.drift_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5376d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InterchangingRBF:\n",
    "    \"\"\"RBF clusters, class labels of clusters ho√°n ƒë·ªïi t·∫°i c√°c m·ªëc drift (recurring/abrupt).\"\"\"\n",
    "    def __init__(self, length=10000, d=10, n_centers=6, drift_points=(3000, 7000), noise=0.0):\n",
    "        self.length=length; self.d=d; self.n_centers=n_centers\n",
    "        self.drift_points=list(drift_points); self.noise=noise\n",
    "\n",
    "    def generate(self):\n",
    "        # init centers and class labels\n",
    "        centers = np.random.randn(self.n_centers, self.d)*2.0\n",
    "        labels = np.array([i%2 for i in range(self.n_centers)], dtype=int)  # binary classes\n",
    "        X = np.zeros((self.length, self.d))\n",
    "        y = np.zeros(self.length, dtype=int)\n",
    "        dp_set = set(self.drift_points)\n",
    "        for t in range(self.length):\n",
    "            k = np.random.randint(0, self.n_centers)\n",
    "            X[t] = centers[k] + 0.5*np.random.randn(self.d)\n",
    "            lbl = labels[k]\n",
    "            if self.noise>0 and np.random.rand()<self.noise:\n",
    "                lbl = 1-lbl\n",
    "            y[t]=lbl\n",
    "            if t in dp_set:\n",
    "                # swap class labels by rotating\n",
    "                labels = 1 - labels  # simple invert all\n",
    "        return X, y, self.drift_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ace38",
   "metadata": {},
   "source": [
    "## 2) Online Learner: Gaussian Naive Bayes (incremental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68bcf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OnlineGaussianNB:\n",
    "    def __init__(self, n_features, n_classes=2, var_smoothing=1e-9):\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.counts = np.zeros(n_classes, dtype=float)\n",
    "        self.means = np.zeros((n_classes, n_features), dtype=float)\n",
    "        self.M2 = np.zeros((n_classes, n_features), dtype=float)\n",
    "        self._eps = 1e-12\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        X = np.atleast_2d(X); y = np.atleast_1d(y)\n",
    "        for xi, yi in zip(X, y):\n",
    "            c = int(yi) if yi < self.n_classes else int(yi % self.n_classes)\n",
    "            self.counts[c] += 1.0\n",
    "            delta = xi - self.means[c]\n",
    "            self.means[c] += delta / max(self.counts[c],1.0)\n",
    "            delta2 = xi - self.means[c]\n",
    "            self.M2[c] += delta * delta2\n",
    "\n",
    "    def _vars(self):\n",
    "        var = np.zeros_like(self.M2)\n",
    "        for c in range(self.n_classes):\n",
    "            denom = max(self.counts[c]-1.0, 1.0)\n",
    "            var[c] = self.M2[c] / denom + self.var_smoothing\n",
    "        return var\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        var = self._vars()\n",
    "        priors = (self.counts + self._eps)/(self.counts.sum() + self.n_classes*self._eps)\n",
    "        logp = []\n",
    "        for c in range(self.n_classes):\n",
    "            # For each class c, compute log probability for all samples in X\n",
    "            # var[c] and self.means[c] are 1D arrays with shape (n_features,)\n",
    "            # X has shape (n_samples, n_features)\n",
    "            log_var_term = -0.5 * np.sum(np.log(2*np.pi*var[c]))  # scalar\n",
    "            diff_sq = ((X - self.means[c])**2) / var[c]  # shape (n_samples, n_features)\n",
    "            quad_term = -0.5 * np.sum(diff_sq, axis=1)  # shape (n_samples,)\n",
    "            lp = log_var_term + quad_term + np.log(priors[c]+self._eps)\n",
    "            logp.append(lp)\n",
    "        logp = np.vstack(logp).T\n",
    "        m = np.max(logp, axis=1, keepdims=True)\n",
    "        p = np.exp(logp - m); p = p/np.sum(p, axis=1, keepdims=True)\n",
    "        return p\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ed1b2",
   "metadata": {},
   "source": [
    "## üö® 3) Enhanced Drift Detection Framework\n",
    "\n",
    "> **v3 Enhancement**: Unified detector interface + River library integration + ShapeDD optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f444852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Unified drift detector framework initialized\n"
     ]
    }
   ],
   "source": [
    "# üèóÔ∏è Unified Drift Detector Framework\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "\n",
    "class DetectorType(Enum):\n",
    "    SUPERVISED = \"supervised\"     # Uses prediction errors\n",
    "    UNSUPERVISED = \"unsupervised\" # Uses feature distributions\n",
    "    SEMI_SUPERVISED = \"semi_supervised\"\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    \"\"\"Standardized detection result\"\"\"\n",
    "    detector_name: str\n",
    "    timestamp: int\n",
    "    is_drift: bool\n",
    "    confidence: float = 0.0\n",
    "    raw_statistic: float = 0.0\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class BaseDriftDetector(ABC):\n",
    "    \"\"\"Unified interface for all drift detectors\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, detector_type: DetectorType):\n",
    "        self.name = name\n",
    "        self.detector_type = detector_type\n",
    "        self.alarms = []\n",
    "        self.t = 0\n",
    "        self.detection_history = []\n",
    "        \n",
    "    @abstractmethod\n",
    "    def update(self, *args, **kwargs) -> Optional[DetectionResult]:\n",
    "        \"\"\"Update detector with new data. Returns detection result if drift detected.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Reset detector state\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_alarms(self) -> List[int]:\n",
    "        \"\"\"Get list of alarm timestamps\"\"\"\n",
    "        return self.alarms.copy()\n",
    "    \n",
    "    def get_detection_rate(self, window_size: int) -> float:\n",
    "        \"\"\"Calculate recent detection rate\"\"\"\n",
    "        if len(self.detection_history) < window_size:\n",
    "            return 0.0\n",
    "        recent = self.detection_history[-window_size:]\n",
    "        return sum(1 for r in recent if r.is_drift) / len(recent)\n",
    "\n",
    "class RiverDetectorWrapper(BaseDriftDetector):\n",
    "    \"\"\"Wrapper for River library detectors\"\"\"\n",
    "    \n",
    "    def __init__(self, river_detector, name: str):\n",
    "        super().__init__(name, DetectorType.SUPERVISED)\n",
    "        self.detector = river_detector\n",
    "        \n",
    "    def update(self, error_or_value) -> Optional[DetectionResult]:\n",
    "        self.t += 1\n",
    "        \n",
    "        # Update the River detector\n",
    "        self.detector.update(error_or_value)\n",
    "        is_drift = self.detector.change_detected\n",
    "        \n",
    "        result = DetectionResult(\n",
    "            detector_name=self.name,\n",
    "            timestamp=self.t,\n",
    "            is_drift=is_drift,\n",
    "            confidence=1.0 if is_drift else 0.0,\n",
    "            metadata={\"river_detector\": type(self.detector).__name__}\n",
    "        )\n",
    "        \n",
    "        self.detection_history.append(result)\n",
    "        \n",
    "        if is_drift:\n",
    "            self.alarms.append(self.t)\n",
    "            return result\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def reset(self):\n",
    "        # River detectors don't have a standard reset, so we recreate\n",
    "        detector_class = type(self.detector)\n",
    "        self.detector = detector_class()\n",
    "        self.alarms = []\n",
    "        self.t = 0\n",
    "        self.detection_history = []\n",
    "\n",
    "print(\"‚úÖ Unified drift detector framework initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "533b6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ShapeDD(BaseDriftDetector):\n",
    "    \"\"\"\n",
    "    Enhanced ShapeDD: Kernel-based MMD detector using statistical moments\n",
    "    \n",
    "    Detects distribution changes via shape statistics (mean, std, skewness, kurtosis)\n",
    "    in sliding reference and current windows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, w_ref=200, w_cur=200, calib_size=1000, q=0.995, min_delay=50):\n",
    "        super().__init__(\"ShapeDD\", DetectorType.UNSUPERVISED)\n",
    "        \n",
    "        self.w_ref = w_ref\n",
    "        self.w_cur = w_cur\n",
    "        self.calib_size = calib_size\n",
    "        self.q = q\n",
    "        self.min_delay = min_delay\n",
    "        \n",
    "        self.buffer = deque(maxlen=w_ref + w_cur + 5)\n",
    "        self.last_alarm_t = -10**9\n",
    "        self.calib_stats = []\n",
    "        self.thr = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _moments(X):\n",
    "        \"\"\"Compute statistical moments: mean, std, skewness, kurtosis\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "            \n",
    "        m = X.mean(axis=0)\n",
    "        std = X.std(axis=0) + 1e-9\n",
    "        z = (X - m) / std\n",
    "        skew = np.mean(z**3, axis=0)\n",
    "        kurt = np.mean(z**4, axis=0) - 3.0\n",
    "        \n",
    "        return m, std, skew, kurt\n",
    "    \n",
    "    def _shape_statistic(self, X):\n",
    "        \"\"\"Concatenate all statistical moments into shape vector\"\"\"\n",
    "        m, s, g, k = self._moments(X)\n",
    "        return np.concatenate([m, s, g, k])\n",
    "    \n",
    "    def update(self, x) -> Optional[DetectionResult]:\n",
    "        \"\"\"Update with new feature vector\"\"\"\n",
    "        self.t += 1\n",
    "        self.buffer.append(x)\n",
    "        \n",
    "        # Need sufficient data for reference and current windows\n",
    "        if len(self.buffer) < (self.w_ref + self.w_cur):\n",
    "            return None\n",
    "            \n",
    "        # Extract reference and current windows\n",
    "        arr = np.array(self.buffer)\n",
    "        ref_window = arr[-(self.w_ref + self.w_cur):-self.w_cur]\n",
    "        cur_window = arr[-self.w_cur:]\n",
    "        \n",
    "        # Compute shape statistics\n",
    "        s_ref = self._shape_statistic(ref_window)\n",
    "        s_cur = self._shape_statistic(cur_window)\n",
    "        \n",
    "        # Normalized L2 distance between shape vectors\n",
    "        scale = np.maximum(np.abs(s_ref), 1e-6)\n",
    "        stat = np.linalg.norm((s_cur - s_ref) / scale)\n",
    "        \n",
    "        # Calibration phase: collect statistics\n",
    "        if self.t <= self.calib_size:\n",
    "            self.calib_stats.append(stat)\n",
    "            return None\n",
    "            \n",
    "        # Set threshold after calibration\n",
    "        if self.thr is None and len(self.calib_stats) > 10:\n",
    "            self.thr = float(np.quantile(self.calib_stats, self.q))\n",
    "            \n",
    "        # Detection phase\n",
    "        is_drift = (self.thr is not None and \n",
    "                   stat > self.thr and \n",
    "                   (self.t - self.last_alarm_t) >= self.min_delay)\n",
    "        \n",
    "        result = DetectionResult(\n",
    "            detector_name=self.name,\n",
    "            timestamp=self.t,\n",
    "            is_drift=is_drift,\n",
    "            confidence=min(stat / self.thr, 2.0) if self.thr is not None else 0.0,\n",
    "            raw_statistic=stat,\n",
    "            metadata={\n",
    "                \"threshold\": self.thr,\n",
    "                \"calibration_size\": len(self.calib_stats),\n",
    "                \"buffer_size\": len(self.buffer)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.detection_history.append(result)\n",
    "        \n",
    "        if is_drift:\n",
    "            self.last_alarm_t = self.t\n",
    "            self.alarms.append(self.t)\n",
    "            return result\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset detector to initial state\"\"\"\n",
    "        super().__init__(\"ShapeDD\", DetectorType.UNSUPERVISED)\n",
    "        self.buffer.clear()\n",
    "        self.last_alarm_t = -10**9\n",
    "        self.calib_stats = []\n",
    "        self.thr = None\n",
    "\n",
    "class DDM:\n",
    "    def __init__(self, min_delay=50):\n",
    "        self.n=0; self.p=0.0; self.pmin=float('inf'); self.smin=float('inf')\n",
    "        self.alarms=[]; self.t=0; self.last_alarm_t=-10**9; self.min_delay=min_delay\n",
    "    def update(self, is_error: bool):\n",
    "        self.t+=1; self.n+=1; self.p += 1.0 if is_error else 0.0\n",
    "        phat = self.p/self.n; s = math.sqrt(phat*(1-phat)/self.n)\n",
    "        if phat + s < self.pmin + self.smin: self.pmin=phat; self.smin=s\n",
    "        if phat + s >= self.pmin + 3*self.smin and (self.t - self.last_alarm_t)>=self.min_delay:\n",
    "            self.alarms.append(self.t); self.last_alarm_t=self.t; return self.t\n",
    "        return None\n",
    "\n",
    "class PageHinkley:\n",
    "    def __init__(self, delta=0.005, lambda_=5.0, min_delay=50):\n",
    "        self.delta=delta; self.lambda_=lambda_\n",
    "        self.t=0; self.mean=0.0; self.m_t=0.0; self.M_t=0.0\n",
    "        self.alarms=[]; self.last_alarm_t=-10**9; self.min_delay=min_delay\n",
    "    def update(self, value):\n",
    "        self.t+=1\n",
    "        self.mean = self.mean + (value - self.mean)/self.t\n",
    "        self.m_t += (value - self.mean - self.delta)\n",
    "        self.M_t = min(self.M_t, self.m_t)\n",
    "        if (self.m_t - self.M_t) > self.lambda_ and (self.t - self.last_alarm_t)>=self.min_delay:\n",
    "            self.alarms.append(self.t); self.m_t=0.0; self.M_t=0.0; self.last_alarm_t=self.t; return self.t\n",
    "        return None\n",
    "\n",
    "class ADWIN:\n",
    "    \"\"\"Simple ADWIN-like detector on binary error stream.\n",
    "    Maintains a variable-length window and checks all cut points for mean change with Hoeffding-like bound.\n",
    "    \"\"\"\n",
    "    def __init__(self, delta=0.002, min_window=50, min_delay=50):\n",
    "        self.delta=delta; self.min_window=min_window\n",
    "        self.win=deque(); self.alarms=[]; self.t=0; self.last_alarm_t=-10**9; self.min_delay=min_delay\n",
    "    def update(self, value):\n",
    "        self.t+=1; self.win.append(value)\n",
    "        if len(self.win) < self.min_window: return None\n",
    "        changed=False\n",
    "        n=len(self.win); arr=np.array(self.win, dtype=float)\n",
    "        mu = arr.mean()\n",
    "        # check a few candidate cuts (not all for speed)\n",
    "        for k in np.linspace(self.min_window//2, n-self.min_window//2, num=10, dtype=int):\n",
    "            left = arr[:k]; right = arr[k:]\n",
    "            mu1, mu2 = left.mean(), right.mean()\n",
    "            eps = math.sqrt( (1/(2*k)) * math.log(4/self.delta) ) + math.sqrt( (1/(2*(n-k))) * math.log(4/self.delta) )\n",
    "            if abs(mu1 - mu2) > eps:\n",
    "                changed=True; break\n",
    "        if changed and (self.t - self.last_alarm_t)>=self.min_delay:\n",
    "            # shrink window by dropping older half\n",
    "            for _ in range(len(self.win)//2): self.win.popleft()\n",
    "            self.alarms.append(self.t); self.last_alarm_t=self.t; return self.t\n",
    "        return None\n",
    "\n",
    "class MDDM:\n",
    "    \"\"\"McDiarmid Drift Detection (simplified): compare weighted averages in two halves of a sliding window.\"\"\"\n",
    "    def __init__(self, W=400, delta=0.002, min_delay=50):\n",
    "        self.W=W; self.delta=delta; self.buf=deque(maxlen=W)\n",
    "        self.alarms=[]; self.t=0; self.last_alarm_t=-10**9; self.min_delay=min_delay\n",
    "    def update(self, value):\n",
    "        self.t+=1; self.buf.append(float(value))\n",
    "        if len(self.buf)<self.W: return None\n",
    "        arr=np.array(self.buf); k=self.W//2\n",
    "        left, right = arr[:k], arr[k:]\n",
    "        # weights increasing (MDDM-A style)\n",
    "        w = np.arange(1, k+1, dtype=float); w/=w.sum()\n",
    "        mu1 = np.sum(left * w); mu2 = np.sum(right * w)\n",
    "        # McDiarmid bound with weights: sum c_i^2 where c_i are weights' bounds in [0,1]\n",
    "        Ci2 = np.sum((w)**2)\n",
    "        eps = math.sqrt(0.5*Ci2*math.log(2.0/self.delta))\n",
    "        if (mu2 - mu1) > eps and (self.t - self.last_alarm_t)>=self.min_delay:\n",
    "            self.alarms.append(self.t); self.last_alarm_t=self.t; return self.t\n",
    "        return None\n",
    "\n",
    "class FHDDM:\n",
    "    \"\"\"Hoeffding Drift Detection on sliding window of correctness (1-correct,0-wrong or vice versa).\"\"\"\n",
    "    def __init__(self, W=500, delta=0.002, min_delay=50):\n",
    "        self.W=W; self.delta=delta; self.buf=deque(maxlen=W)\n",
    "        self.mu_max=0.0; self.alarms=[]; self.t=0; self.last_alarm_t=-10**9; self.min_delay=min_delay\n",
    "    def update(self, correct01):\n",
    "        self.t+=1; self.buf.append(float(correct01))\n",
    "        if len(self.buf)<self.W: return None\n",
    "        mu = np.mean(self.buf)\n",
    "        self.mu_max = max(self.mu_max, mu)\n",
    "        eps = math.sqrt((1/(2*self.W))*math.log(1/self.delta))\n",
    "        if (self.mu_max - mu) >= eps and (self.t - self.last_alarm_t)>=self.min_delay:\n",
    "            self.alarms.append(self.t); self.mu_max = mu  # reset baseline\n",
    "            self.last_alarm_t=self.t; return self.t\n",
    "        return None\n",
    "\n",
    "class FHDDMS:\n",
    "    \"\"\"Stacking FHDDM: short & long windows; alarm if any triggers.\"\"\"\n",
    "    def __init__(self, W_short=100, W_long=500, delta=0.002, min_delay=50):\n",
    "        self.short = FHDDM(W=W_short, delta=delta, min_delay=min_delay)\n",
    "        self.long = FHDDM(W=W_long,  delta=delta, min_delay=min_delay)\n",
    "        self.alarms=[]; self.t=0\n",
    "    def update(self, correct01):\n",
    "        self.t+=1\n",
    "        a1 = self.short.update(correct01)\n",
    "        a2 = self.long.update(correct01)\n",
    "        fired = False\n",
    "        for a in (a1,a2):\n",
    "            if a is not None: fired=True\n",
    "        if fired:\n",
    "            self.alarms.append(self.t)\n",
    "            return self.t\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317ad1a",
   "metadata": {},
   "source": [
    "## 4) Evaluation (prequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d3a743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class RunResult:\n",
    "    stream: str\n",
    "    name: str\n",
    "    accuracy: float\n",
    "    macro_f1: float\n",
    "    n_det: int\n",
    "    false_alarms: int\n",
    "    mean_delay: Optional[float]\n",
    "    delays: List[int]\n",
    "    alarms: List[int]\n",
    "    drift_points: List[int]\n",
    "    runtime_s: float\n",
    "\n",
    "def compute_delays(alarms, drift_points, tol=200):\n",
    "    drift_points = list(drift_points); alarms = sorted(alarms); used=set(); delays=[]\n",
    "    for dp in drift_points:\n",
    "        cand = [a for a in alarms if a>=dp and (a-dp)<=tol and a not in used]\n",
    "        if cand:\n",
    "            a=cand[0]; used.add(a); delays.append(a-dp)\n",
    "        else:\n",
    "            delays.append(np.nan)\n",
    "    good=set()\n",
    "    for dp in drift_points:\n",
    "        good.update([a for a in alarms if a>=dp and (a-dp)<=tol])\n",
    "    false_alarms = len([a for a in alarms if a not in good])\n",
    "    md = np.nanmean(delays) if len(delays)>0 else np.nan\n",
    "    return delays, false_alarms, md\n",
    "\n",
    "def update_confusion(cm, y_true, y_pred, n_classes):\n",
    "    cm[y_true, y_pred] += 1\n",
    "    return cm\n",
    "\n",
    "def metrics_from_cm(cm):\n",
    "    # Accuracy\n",
    "    acc = np.trace(cm)/np.sum(cm) if cm.sum()>0 else 0.0\n",
    "    # Macro F1\n",
    "    K = cm.shape[0]\n",
    "    f1s=[]\n",
    "    for k in range(K):\n",
    "        TP = cm[k,k]\n",
    "        FP = cm[:,k].sum() - TP\n",
    "        FN = cm[k,:].sum() - TP\n",
    "        prec = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "        rec  = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "        f1 = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "        f1s.append(f1)\n",
    "    macro_f1 = float(np.mean(f1s)) if len(f1s)>0 else 0.0\n",
    "    return acc, macro_f1\n",
    "\n",
    "def run_stream_experiment(stream_name, X, y, drift_points, detectors, learner):\n",
    "    t0 = time.time()\n",
    "    n = len(y)\n",
    "    n_classes = int(np.max(y))+1\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for i in range(n):\n",
    "        xi = X[i]; yi = y[i]\n",
    "        yhat = learner.predict(xi)[0] if learner.counts.sum()>0 else random.randint(0, n_classes-1)\n",
    "        cm = update_confusion(cm, yi, yhat, n_classes)\n",
    "        err = 0 if yhat==yi else 1\n",
    "        # feed detectors\n",
    "        for name, det in detectors.items():\n",
    "            if isinstance(det, ShapeDD):\n",
    "                det.update(xi)\n",
    "            elif isinstance(det, (DDM, ADWIN, MDDM, FHDDM, FHDDMS, PageHinkley)):\n",
    "                # For FHDDM(S), use correctness (1=correct)\n",
    "                if isinstance(det, (FHDDM, FHDDMS)):\n",
    "                    det.update(1-err==1)  # correct01\n",
    "                else:\n",
    "                    det.update(err)\n",
    "        learner.partial_fit(xi, yi)\n",
    "    # summarize per detector\n",
    "    acc, macro_f1 = metrics_from_cm(cm)\n",
    "    results=[]\n",
    "    for name, det in detectors.items():\n",
    "        alarms = getattr(det, 'alarms', [])\n",
    "        delays, fa, md = compute_delays(alarms, drift_points)\n",
    "        results.append(RunResult(\n",
    "            stream=stream_name, name=name, accuracy=acc, macro_f1=macro_f1,\n",
    "            n_det=len(alarms), false_alarms=fa, mean_delay=md, delays=delays,\n",
    "            alarms=alarms, drift_points=drift_points, runtime_s=time.time()-t0\n",
    "        ))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1249c7",
   "metadata": {},
   "source": [
    "## 5) Ch·∫°y th·ª≠ nghi·ªám (5 streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81153511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 7 detectors (River library not available)\n",
      "üìã Available detectors: ['ShapeDD', 'DDM', 'PageHinkley', 'ADWIN', 'MDDM', 'FHDDM', 'FHDDMS']\n"
     ]
    }
   ],
   "source": [
    "# üè≠ Detector Factory & Configuration\n",
    "\n",
    "def create_detector_suite(min_delay=100) -> Dict[str, BaseDriftDetector]:\n",
    "    \"\"\"Create a comprehensive suite of drift detectors\"\"\"\n",
    "    detectors = {}\n",
    "    \n",
    "    # 1. Custom ShapeDD\n",
    "    detectors[\"ShapeDD\"] = ShapeDD(\n",
    "        w_ref=200, w_cur=200, calib_size=1000, \n",
    "        q=0.995, min_delay=min_delay\n",
    "    )\n",
    "    \n",
    "    # 2. Traditional methods (existing implementations)\n",
    "    detectors[\"DDM\"] = DDM(min_delay=min_delay)\n",
    "    detectors[\"PageHinkley\"] = PageHinkley(\n",
    "        delta=0.001, lambda_=10.0, min_delay=min_delay\n",
    "    )\n",
    "    detectors[\"ADWIN\"] = ADWIN(\n",
    "        delta=0.002, min_window=80, min_delay=min_delay\n",
    "    )\n",
    "    detectors[\"MDDM\"] = MDDM(\n",
    "        W=400, delta=0.005, min_delay=min_delay\n",
    "    )\n",
    "    detectors[\"FHDDM\"] = FHDDM(\n",
    "        W=500, delta=0.002, min_delay=min_delay\n",
    "    )\n",
    "    detectors[\"FHDDMS\"] = FHDDMS(\n",
    "        W_short=100, W_long=500, delta=0.002, min_delay=min_delay\n",
    "    )\n",
    "    \n",
    "    # 3. River library detectors (if available)\n",
    "    if RIVER_AVAILABLE:\n",
    "        detectors[\"River_DDM\"] = RiverDetectorWrapper(\n",
    "            river_drift.DDM(), \"River_DDM\"\n",
    "        )\n",
    "        detectors[\"River_EDDM\"] = RiverDetectorWrapper(\n",
    "            river_drift.EDDM(), \"River_EDDM\"\n",
    "        )\n",
    "        detectors[\"River_ADWIN\"] = RiverDetectorWrapper(\n",
    "            river_drift.ADWIN(), \"River_ADWIN\"\n",
    "        )\n",
    "        detectors[\"River_HDDM_A\"] = RiverDetectorWrapper(\n",
    "            river_drift.HDDM_A(), \"River_HDDM_A\"\n",
    "        )\n",
    "        detectors[\"River_HDDM_W\"] = RiverDetectorWrapper(\n",
    "            river_drift.HDDM_W(), \"River_HDDM_W\"\n",
    "        )\n",
    "        detectors[\"River_KSWIN\"] = RiverDetectorWrapper(\n",
    "            river_drift.KSWIN(), \"River_KSWIN\"\n",
    "        )\n",
    "        detectors[\"River_PageHinkley\"] = RiverDetectorWrapper(\n",
    "            river_drift.PageHinkley(), \"River_PageHinkley\"\n",
    "        )\n",
    "        print(f\"‚úÖ Created {len(detectors)} detectors (including {len([k for k in detectors if 'River' in k])} River detectors)\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Created {len(detectors)} detectors (River library not available)\")\n",
    "    \n",
    "    return detectors\n",
    "\n",
    "# Test detector creation\n",
    "test_detectors = create_detector_suite()\n",
    "print(f\"üìã Available detectors: {list(test_detectors.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "575e7ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Generating synthetic data streams...\n",
      "‚úÖ SEA: 15000 samples, 3D, 3 drifts at [4000, 8000, 12000]\n",
      "‚úÖ RotatingHyperplane: 15000 samples, 10D, 2 drifts at [5000, 10000]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LEDStream' object has no attribute 'get_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m generator \u001b[38;5;129;01min\u001b[39;00m generators:\n\u001b[32m     25\u001b[39m     X, y, drift_points = generator.generate()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     metadata = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metadata\u001b[49m()\n\u001b[32m     27\u001b[39m     stream_data.append({\n\u001b[32m     28\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m: metadata.name,\n\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m: X,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m: metadata\n\u001b[32m     33\u001b[39m     })\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata.n_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mD, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(drift_points)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m drifts at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrift_points\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'LEDStream' object has no attribute 'get_metadata'"
     ]
    }
   ],
   "source": [
    "\n",
    "# üöÄ 5) Enhanced Benchmark Execution\n",
    "\n",
    "## Stream Configuration\n",
    "N = 15000  # Stream length\n",
    "streams = []\n",
    "\n",
    "print(\"üåä Generating synthetic data streams...\")\n",
    "\n",
    "# Create enhanced stream generators\n",
    "generators = [\n",
    "    SEAStream(length=N, thresholds=(7.0, 8.0, 9.0, 9.5), \n",
    "              drift_points=(4000, 8000, 12000), noise=0.01),\n",
    "    RotatingHyperplane(length=N, d=10, angle_per_step=2*np.pi/(N//2), noise=0.01),\n",
    "    LEDStream(length=N, mode='abrupt', drift_points=(3000, 7000, 12000), \n",
    "              g_len=0, noise=0.02),\n",
    "    LEDStream(length=N, mode='gradual', drift_points=(4000, 9000), \n",
    "              g_len=800, noise=0.02),\n",
    "    InterchangingRBF(length=N, d=10, n_centers=6, \n",
    "                     drift_points=(5000, 10000), noise=0.01)\n",
    "]\n",
    "\n",
    "# Generate streams with metadata\n",
    "stream_data = []\n",
    "for generator in generators:\n",
    "    X, y, drift_points = generator.generate()\n",
    "    metadata = generator.get_metadata()\n",
    "    stream_data.append({\n",
    "        'name': metadata.name,\n",
    "        'X': X,\n",
    "        'y': y, \n",
    "        'drift_points': drift_points,\n",
    "        'metadata': metadata\n",
    "    })\n",
    "    print(f\"‚úÖ {metadata.name}: {len(X)} samples, {metadata.n_features}D, \"\n",
    "          f\"{len(drift_points)} drifts at {drift_points}\")\n",
    "\n",
    "print(f\"üìä Generated {len(stream_data)} data streams ready for evaluation\")\n",
    "\n",
    "# Results storage\n",
    "benchmark_results = []\n",
    "detailed_results = {}\n",
    "\n",
    "for sname, Xs, ys, dps in streams:\n",
    "    print(f\"== Running on {sname} ==\")\n",
    "    n_classes = int(np.max(ys))+1\n",
    "    dets = {\n",
    "        \"ShapeDD\": ShapeDD(w_ref=200, w_cur=200, calib_size=1000, q=0.995, min_delay=100),\n",
    "        \"DDM\": DDM(min_delay=100),\n",
    "        \"PageHinkley\": PageHinkley(delta=0.001, lambda_=10.0, min_delay=100),\n",
    "        \"ADWIN\": ADWIN(delta=0.002, min_window=80, min_delay=100),\n",
    "        \"MDDM\": MDDM(W=400, delta=0.005, min_delay=100),\n",
    "        \"FHDDM\": FHDDM(W=500, delta=0.002, min_delay=100),\n",
    "        \"FHDDMS\": FHDDMS(W_short=100, W_long=500, delta=0.002, min_delay=100),\n",
    "    }\n",
    "    learner = OnlineGaussianNB(n_features=Xs.shape[1], n_classes=n_classes)\n",
    "    res = run_stream_experiment(sname, Xs, ys, dps, dets, learner)\n",
    "    for r in res:\n",
    "        all_rows.append({\n",
    "            \"stream\": r.stream,\n",
    "            \"detector\": r.name,\n",
    "            \"accuracy\": r.accuracy,\n",
    "            \"macro_f1\": r.macro_f1,\n",
    "            \"n_detections\": r.n_det,\n",
    "            \"false_alarms\": r.false_alarms,\n",
    "            \"mean_delay\": r.mean_delay,\n",
    "            \"runtime_s\": r.runtime_s\n",
    "        })\n",
    "        all_details[(sname, r.name)] = r\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82175019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Enhanced Evaluation Framework\n",
    "\n",
    "@dataclass\n",
    "class EnhancedRunResult:\n",
    "    \"\"\"Comprehensive evaluation results\"\"\"\n",
    "    stream_name: str\n",
    "    detector_name: str\n",
    "    detector_type: str\n",
    "    \n",
    "    # Classification metrics\n",
    "    accuracy: float\n",
    "    macro_f1: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    \n",
    "    # Detection quality\n",
    "    n_detections: int\n",
    "    true_positives: int\n",
    "    false_alarms: int\n",
    "    false_negatives: int\n",
    "    \n",
    "    # Timing metrics\n",
    "    delays: List[float]\n",
    "    mean_delay: float\n",
    "    median_delay: float\n",
    "    delay_std: float\n",
    "    \n",
    "    # Advanced metrics\n",
    "    beta_score: float\n",
    "    alarm_rate: float\n",
    "    f1_at_ar: float\n",
    "    accuracy_at_ar: float\n",
    "    \n",
    "    # Metadata\n",
    "    runtime_s: float\n",
    "    drift_points: List[int]\n",
    "    alarm_timestamps: List[int]\n",
    "    stream_metadata: StreamMetadata\n",
    "\n",
    "def enhanced_delay_analysis(alarms: List[int], drift_points: List[int], \n",
    "                          tolerance: int = 500) -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced delay analysis with statistical measures\"\"\"\n",
    "    if not drift_points:\n",
    "        return {\n",
    "            'delays': [], 'tp': 0, 'fp': len(alarms), 'fn': 0,\n",
    "            'mean_delay': np.nan, 'median_delay': np.nan, 'delay_std': np.nan\n",
    "        }\n",
    "    \n",
    "    alarms = sorted(alarms)\n",
    "    drift_points = sorted(drift_points)\n",
    "    \n",
    "    # Match alarms to drift points\n",
    "    used_alarms = set()\n",
    "    delays = []\n",
    "    \n",
    "    for dp in drift_points:\n",
    "        # Find first unused alarm after drift point within tolerance\n",
    "        candidates = [a for a in alarms \n",
    "                     if a >= dp and (a - dp) <= tolerance and a not in used_alarms]\n",
    "        if candidates:\n",
    "            alarm = min(candidates)\n",
    "            delays.append(alarm - dp)\n",
    "            used_alarms.add(alarm)\n",
    "    \n",
    "    tp = len(delays)\n",
    "    fp = len([a for a in alarms if a not in used_alarms])\n",
    "    fn = len(drift_points) - tp\n",
    "    \n",
    "    # Statistical measures\n",
    "    if delays:\n",
    "        mean_delay = np.mean(delays)\n",
    "        median_delay = np.median(delays)\n",
    "        delay_std = np.std(delays)\n",
    "    else:\n",
    "        mean_delay = median_delay = delay_std = np.nan\n",
    "    \n",
    "    return {\n",
    "        'delays': delays,\n",
    "        'tp': tp, 'fp': fp, 'fn': fn,\n",
    "        'mean_delay': mean_delay,\n",
    "        'median_delay': median_delay, \n",
    "        'delay_std': delay_std\n",
    "    }\n",
    "\n",
    "def compute_advanced_metrics(acc: float, f1: float, n_alarms: int, \n",
    "                           n_samples: int, tp: int, fp: int, fn: int,\n",
    "                           beta: float = 0.5, lambda_ar: float = 0.01) -> Dict[str, float]:\n",
    "    \"\"\"Compute advanced evaluation metrics\"\"\"\n",
    "    # Alarm rate per 10k samples\n",
    "    alarm_rate = (n_alarms / n_samples) * 10000\n",
    "    \n",
    "    # F1@AR and Accuracy@AR\n",
    "    f1_at_ar = f1 - lambda_ar * alarm_rate\n",
    "    acc_at_ar = acc - lambda_ar * alarm_rate\n",
    "    \n",
    "    # Œ≤-score (balanced precision-recall for detection)\n",
    "    p = tp + fn  # Total true drifts\n",
    "    beta_score = tp / (p + beta * fp) if (p + beta * fp) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'alarm_rate': alarm_rate,\n",
    "        'f1_at_ar': f1_at_ar,\n",
    "        'accuracy_at_ar': acc_at_ar,\n",
    "        'beta_score': beta_score\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Enhanced evaluation framework ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Main Benchmark Execution Function\n",
    "\n",
    "def run_enhanced_benchmark(stream_data: Dict, detectors: Dict[str, BaseDriftDetector], \n",
    "                          use_river_classifier: bool = True) -> List[EnhancedRunResult]:\n",
    "    \"\"\"\n",
    "    Run enhanced benchmark with comprehensive evaluation\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Create classifier\n",
    "    if use_river_classifier and RIVER_AVAILABLE:\n",
    "        classifier_factory = lambda: tree.HoeffdingTreeClassifier()\n",
    "        print(\"üå≥ Using River HoeffdingTreeClassifier\")\n",
    "    else:\n",
    "        # Use existing OnlineGaussianNB as fallback\n",
    "        classifier_factory = lambda: OnlineGaussianNB(\n",
    "            n_features=stream_data['X'].shape[1], \n",
    "            n_classes=int(np.max(stream_data['y'])) + 1\n",
    "        )\n",
    "        print(\"üìä Using OnlineGaussianNB classifier\")\n",
    "    \n",
    "    X, y = stream_data['X'], stream_data['y']\n",
    "    drift_points = stream_data['drift_points']\n",
    "    metadata = stream_data['metadata']\n",
    "    \n",
    "    print(f\"\\\\nüéØ Processing {metadata.name} stream...\")\n",
    "    print(f\"   üìè Length: {len(X)}, Features: {X.shape[1]}, Classes: {metadata.n_classes}\")\n",
    "    print(f\"   üåä Drift points: {drift_points}\")\n",
    "    \n",
    "    for det_name, detector in detectors.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Reset detector and create fresh classifier\n",
    "        detector.reset()\n",
    "        classifier = classifier_factory()\n",
    "        \n",
    "        # Classification tracking\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        print(f\"     üîç Testing {det_name}...\", end=' ')\n",
    "        \n",
    "        # Prequential evaluation loop\n",
    "        for i in range(len(X)):\n",
    "            xi, yi = X[i], y[i]\n",
    "            \n",
    "            # Predict with current model\n",
    "            if RIVER_AVAILABLE and use_river_classifier:\n",
    "                y_pred = classifier.predict_one(xi if isinstance(xi, dict) else {f'f{j}': xi[j] for j in range(len(xi))})\n",
    "                if y_pred is None:\n",
    "                    y_pred = np.random.randint(0, metadata.n_classes)\n",
    "            else:\n",
    "                if hasattr(classifier, 'predict') and classifier.counts.sum() > 0:\n",
    "                    y_pred = classifier.predict(xi.reshape(1, -1))[0]\n",
    "                else:\n",
    "                    y_pred = np.random.randint(0, metadata.n_classes)\n",
    "            \n",
    "            predictions.append(y_pred)\n",
    "            true_labels.append(yi)\n",
    "            \n",
    "            # Update drift detector\n",
    "            error = 1 if y_pred != yi else 0\n",
    "            \n",
    "            if isinstance(detector, ShapeDD):\n",
    "                # ShapeDD uses feature vectors\n",
    "                detector.update(xi)\n",
    "            elif isinstance(detector, RiverDetectorWrapper):\n",
    "                # River detectors use errors or probabilities\n",
    "                if 'KSWIN' in det_name:\n",
    "                    # KSWIN needs probability, use random for simplicity\n",
    "                    detector.update(np.random.random())\n",
    "                else:\n",
    "                    detector.update(error)\n",
    "            else:\n",
    "                # Traditional detectors use errors\n",
    "                if hasattr(detector, 'update'):\n",
    "                    if det_name in ['FHDDM', 'FHDDMS']:\n",
    "                        detector.update(1 - error)  # Correctness for FHDDM\n",
    "                    else:\n",
    "                        detector.update(error)\n",
    "            \n",
    "            # Update classifier\n",
    "            if RIVER_AVAILABLE and use_river_classifier:\n",
    "                classifier.learn_one(\n",
    "                    xi if isinstance(xi, dict) else {f'f{j}': xi[j] for j in range(len(xi))}, \n",
    "                    yi\n",
    "                )\n",
    "            else:\n",
    "                classifier.partial_fit(xi.reshape(1, -1), [yi])\n",
    "        \n",
    "        runtime = time.time() - start_time\n",
    "        \n",
    "        # Compute classification metrics\n",
    "        acc = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "        precision = precision_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "        recall = recall_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        # Get detection results\n",
    "        alarms = detector.get_alarms()\n",
    "        delay_analysis = enhanced_delay_analysis(alarms, drift_points)\n",
    "        \n",
    "        # Compute advanced metrics\n",
    "        advanced = compute_advanced_metrics(\n",
    "            acc, f1, len(alarms), len(X),\n",
    "            delay_analysis['tp'], delay_analysis['fp'], delay_analysis['fn']\n",
    "        )\n",
    "        \n",
    "        # Create comprehensive result\n",
    "        result = EnhancedRunResult(\n",
    "            stream_name=metadata.name,\n",
    "            detector_name=det_name,\n",
    "            detector_type=detector.detector_type.value if hasattr(detector, 'detector_type') else 'unknown',\n",
    "            accuracy=acc,\n",
    "            macro_f1=f1,\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            n_detections=len(alarms),\n",
    "            true_positives=delay_analysis['tp'],\n",
    "            false_alarms=delay_analysis['fp'],\n",
    "            false_negatives=delay_analysis['fn'],\n",
    "            delays=delay_analysis['delays'],\n",
    "            mean_delay=delay_analysis['mean_delay'],\n",
    "            median_delay=delay_analysis['median_delay'],\n",
    "            delay_std=delay_analysis['delay_std'],\n",
    "            beta_score=advanced['beta_score'],\n",
    "            alarm_rate=advanced['alarm_rate'],\n",
    "            f1_at_ar=advanced['f1_at_ar'],\n",
    "            accuracy_at_ar=advanced['accuracy_at_ar'],\n",
    "            runtime_s=runtime,\n",
    "            drift_points=drift_points,\n",
    "            alarm_timestamps=alarms,\n",
    "            stream_metadata=metadata\n",
    "        )\n",
    "        \n",
    "        results.append(result)\n",
    "        print(f\"‚úÖ Acc:{acc:.3f}, F1:{f1:.3f}, Detections:{len(alarms)}, Runtime:{runtime:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üöÄ Enhanced benchmark function ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Execute Enhanced Benchmark\n",
    "\n",
    "# Create detector suite\n",
    "detectors = create_detector_suite(min_delay=100)\n",
    "\n",
    "# Run benchmark on all streams\n",
    "all_results = []\n",
    "\n",
    "for stream in stream_data:\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"üåä BENCHMARKING: {stream['name'].upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    stream_results = run_enhanced_benchmark(stream, detectors)\n",
    "    all_results.extend(stream_results)\n",
    "\n",
    "print(f\"\\\\nüéâ Benchmark Complete! Processed {len(all_results)} detector-stream combinations\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Stream': r.stream_name,\n",
    "        'Detector': r.detector_name,\n",
    "        'Type': r.detector_type,\n",
    "        'Accuracy': r.accuracy,\n",
    "        'Macro_F1': r.macro_f1,\n",
    "        'Precision': r.precision,\n",
    "        'Recall': r.recall,\n",
    "        'Detections': r.n_detections,\n",
    "        'True_Positives': r.true_positives,\n",
    "        'False_Alarms': r.false_alarms,\n",
    "        'False_Negatives': r.false_negatives,\n",
    "        'Mean_Delay': r.mean_delay,\n",
    "        'Median_Delay': r.median_delay,\n",
    "        'Delay_Std': r.delay_std,\n",
    "        'Beta_Score': r.beta_score,\n",
    "        'Alarm_Rate': r.alarm_rate,\n",
    "        'F1@AR': r.f1_at_ar,\n",
    "        'Accuracy@AR': r.accuracy_at_ar,\n",
    "        'Runtime_s': r.runtime_s,\n",
    "        'Drift_Points': str(r.drift_points),\n",
    "        'Alarm_Times': str(r.alarm_timestamps)\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "print(f\"\\\\nüìä Results DataFrame shape: {results_df.shape}\")\n",
    "print(\"\\\\nüîù Top 5 detectors by F1@AR:\")\n",
    "top_f1ar = results_df.nlargest(5, 'F1@AR')[['Stream', 'Detector', 'F1@AR', 'Accuracy', 'Beta_Score']]\n",
    "print(top_f1ar.to_string(index=False))\n",
    "\n",
    "results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Advanced Visualization & Analysis\n",
    "\n",
    "def create_comprehensive_plots(results_df):\n",
    "    \"\"\"Create comprehensive visualization suite\"\"\"\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Performance heatmap\n",
    "    plt.subplot(3, 3, 1)\n",
    "    pivot_f1 = results_df.pivot(index='Detector', columns='Stream', values='F1@AR')\n",
    "    sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                cbar_kws={'label': 'F1@AR Score'})\n",
    "    plt.title('üéØ F1@AR Performance Heatmap')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 2. Detection quality scatter\n",
    "    plt.subplot(3, 3, 2)\n",
    "    scatter = plt.scatter(results_df['False_Alarms'], results_df['Mean_Delay'], \n",
    "                         c=results_df['Beta_Score'], s=results_df['True_Positives']*20,\n",
    "                         cmap='viridis', alpha=0.7)\n",
    "    plt.xlabel('False Alarms')\n",
    "    plt.ylabel('Mean Detection Delay')\n",
    "    plt.title('üéØ Detection Quality (size=TPs, color=Œ≤-score)')\n",
    "    plt.colorbar(scatter, label='Œ≤-score')\n",
    "    \n",
    "    # 3. Accuracy vs Alarm Rate\n",
    "    plt.subplot(3, 3, 3)\n",
    "    for stream in results_df['Stream'].unique():\n",
    "        stream_data = results_df[results_df['Stream'] == stream]\n",
    "        plt.scatter(stream_data['Alarm_Rate'], stream_data['Accuracy'], \n",
    "                   label=stream, alpha=0.7, s=60)\n",
    "    plt.xlabel('Alarm Rate (per 10k samples)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('üìä Accuracy vs Alarm Rate')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 4. Runtime comparison\n",
    "    plt.subplot(3, 3, 4)\n",
    "    runtime_by_detector = results_df.groupby('Detector')['Runtime_s'].mean().sort_values()\n",
    "    runtime_by_detector.plot(kind='barh', color='skyblue')\n",
    "    plt.xlabel('Mean Runtime (seconds)')\n",
    "    plt.title('‚è±Ô∏è Runtime Performance')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 5. Detection delay distribution\n",
    "    plt.subplot(3, 3, 5)\n",
    "    delay_data = []\n",
    "    delay_labels = []\n",
    "    for detector in results_df['Detector'].unique()[:8]:  # Top 8 for clarity\n",
    "        detector_results = results_df[results_df['Detector'] == detector]\n",
    "        delays = []\n",
    "        for _, row in detector_results.iterrows():\n",
    "            if not pd.isna(row['Mean_Delay']):\n",
    "                delays.extend(row['Mean_Delay'] if isinstance(row['Mean_Delay'], list) else [row['Mean_Delay']])\n",
    "        if delays:\n",
    "            delay_data.append(delays)\n",
    "            delay_labels.append(detector)\n",
    "    \n",
    "    if delay_data:\n",
    "        plt.boxplot(delay_data, labels=delay_labels)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Detection Delay')\n",
    "        plt.title('üì¶ Detection Delay Distribution')\n",
    "    \n",
    "    # 6. Stream difficulty ranking\n",
    "    plt.subplot(3, 3, 6)\n",
    "    stream_difficulty = results_df.groupby('Stream').agg({\n",
    "        'Accuracy': 'mean',\n",
    "        'Mean_Delay': 'mean',\n",
    "        'False_Alarms': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Normalize metrics (lower is better for difficulty score)\n",
    "    stream_difficulty['Difficulty'] = (\n",
    "        (1 - stream_difficulty['Accuracy']) + \n",
    "        stream_difficulty['Mean_Delay'].fillna(0) / 1000 +\n",
    "        stream_difficulty['False_Alarms'] / 10\n",
    "    )\n",
    "    \n",
    "    stream_difficulty = stream_difficulty.sort_values('Difficulty')\n",
    "    plt.barh(stream_difficulty['Stream'], stream_difficulty['Difficulty'], color='coral')\n",
    "    plt.xlabel('Difficulty Score (lower=easier)')\n",
    "    plt.title('üèîÔ∏è Stream Difficulty Ranking')\n",
    "    \n",
    "    # 7. Advanced metrics comparison\n",
    "    plt.subplot(3, 3, 7)\n",
    "    metrics_comparison = results_df.groupby('Detector')[['F1@AR', 'Accuracy@AR', 'Beta_Score']].mean()\n",
    "    metrics_comparison.plot(kind='bar', ax=plt.gca())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('üìä Advanced Metrics Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 8. Detector type performance\n",
    "    plt.subplot(3, 3, 8)\n",
    "    type_performance = results_df.groupby('Type')[['Accuracy', 'F1@AR', 'Beta_Score']].mean()\n",
    "    type_performance.plot(kind='bar', ax=plt.gca())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('üîß Performance by Detector Type')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 9. Global ranking\n",
    "    plt.subplot(3, 3, 9)\n",
    "    # Compute global score (normalized combination)\n",
    "    for col in ['F1@AR', 'Accuracy@AR', 'Beta_Score']:\n",
    "        col_min, col_max = results_df[col].min(), results_df[col].max()\n",
    "        if col_max > col_min:\n",
    "            results_df[f'{col}_norm'] = (results_df[col] - col_min) / (col_max - col_min)\n",
    "        else:\n",
    "            results_df[f'{col}_norm'] = 0.5\n",
    "    \n",
    "    results_df['Global_Score'] = (\n",
    "        results_df['F1@AR_norm'] * 0.4 + \n",
    "        results_df['Accuracy@AR_norm'] * 0.3 + \n",
    "        results_df['Beta_Score_norm'] * 0.3\n",
    "    )\n",
    "    \n",
    "    global_ranking = results_df.groupby('Detector')['Global_Score'].mean().sort_values(ascending=False)[:10]\n",
    "    global_ranking.plot(kind='barh', color='gold')\n",
    "    plt.xlabel('Global Score')\n",
    "    plt.title('üèÜ Top 10 Global Detector Ranking')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Create visualizations\n",
    "enhanced_results_df = create_comprehensive_plots(results_df)\n",
    "\n",
    "print(\"\\\\nüèÜ FINAL RANKINGS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Top detectors by different criteria\n",
    "print(\"\\\\nüéØ Best F1@AR Performance:\")\n",
    "top_f1ar = enhanced_results_df.groupby('Detector')['F1@AR'].mean().sort_values(ascending=False).head(5)\n",
    "for i, (detector, score) in enumerate(top_f1ar.items(), 1):\n",
    "    print(f\"{i}. {detector}: {score:.4f}\")\n",
    "\n",
    "print(\"\\\\nüîç Best Detection Quality (Œ≤-score):\")\n",
    "top_beta = enhanced_results_df.groupby('Detector')['Beta_Score'].mean().sort_values(ascending=False).head(5)\n",
    "for i, (detector, score) in enumerate(top_beta.items(), 1):\n",
    "    print(f\"{i}. {detector}: {score:.4f}\")\n",
    "\n",
    "print(\"\\\\n‚ö° Fastest Detectors:\")\n",
    "top_speed = enhanced_results_df.groupby('Detector')['Runtime_s'].mean().sort_values().head(5)\n",
    "for i, (detector, time) in enumerate(top_speed.items(), 1):\n",
    "    print(f\"{i}. {detector}: {time:.3f}s\")\n",
    "\n",
    "print(\"\\\\nüèÜ Overall Champions (Global Score):\")\n",
    "if 'Global_Score' in enhanced_results_df.columns:\n",
    "    top_global = enhanced_results_df.groupby('Detector')['Global_Score'].mean().sort_values(ascending=False).head(5)\n",
    "    for i, (detector, score) in enumerate(top_global.items(), 1):\n",
    "        print(f\"{i}. {detector}: {score:.4f}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d644ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Export Results & Summary Report\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def export_comprehensive_results(results_df, output_dir=\"./results\"):\n",
    "    \"\"\"Export comprehensive results in multiple formats\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Full results CSV\n",
    "    full_results_path = f\"{output_dir}/concept_drift_benchmark_v3_{timestamp}.csv\"\n",
    "    results_df.to_csv(full_results_path, index=False)\n",
    "    print(f\"üìÑ Full results exported: {full_results_path}\")\n",
    "    \n",
    "    # 2. Summary statistics\n",
    "    summary_stats = {}\n",
    "    \n",
    "    # Overall performance by detector\n",
    "    detector_summary = results_df.groupby('Detector').agg({\n",
    "        'Accuracy': ['mean', 'std'],\n",
    "        'Macro_F1': ['mean', 'std'], \n",
    "        'F1@AR': ['mean', 'std'],\n",
    "        'Beta_Score': ['mean', 'std'],\n",
    "        'Mean_Delay': ['mean', 'std'],\n",
    "        'False_Alarms': ['mean', 'std'],\n",
    "        'Runtime_s': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    detector_summary.columns = ['_'.join(col).strip() for col in detector_summary.columns]\n",
    "    summary_path = f\"{output_dir}/detector_summary_{timestamp}.csv\"\n",
    "    detector_summary.to_csv(summary_path)\n",
    "    print(f\"üìä Detector summary exported: {summary_path}\")\n",
    "    \n",
    "    # Stream performance summary\n",
    "    stream_summary = results_df.groupby('Stream').agg({\n",
    "        'Accuracy': ['mean', 'std'],\n",
    "        'F1@AR': ['mean', 'std'],\n",
    "        'Beta_Score': ['mean', 'std'],\n",
    "        'False_Alarms': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    stream_summary.columns = ['_'.join(col).strip() for col in stream_summary.columns]\n",
    "    stream_path = f\"{output_dir}/stream_summary_{timestamp}.csv\"\n",
    "    stream_summary.to_csv(stream_path)\n",
    "    print(f\"üåä Stream summary exported: {stream_path}\")\n",
    "    \n",
    "    # 3. Best performers report\n",
    "    best_performers = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_experiments': len(results_df),\n",
    "        'detectors_tested': results_df['Detector'].nunique(),\n",
    "        'streams_tested': results_df['Stream'].nunique(),\n",
    "        'best_f1_ar': {\n",
    "            'detector': results_df.loc[results_df['F1@AR'].idxmax(), 'Detector'],\n",
    "            'score': float(results_df['F1@AR'].max()),\n",
    "            'stream': results_df.loc[results_df['F1@AR'].idxmax(), 'Stream']\n",
    "        },\n",
    "        'best_beta_score': {\n",
    "            'detector': results_df.loc[results_df['Beta_Score'].idxmax(), 'Detector'],\n",
    "            'score': float(results_df['Beta_Score'].max()),\n",
    "            'stream': results_df.loc[results_df['Beta_Score'].idxmax(), 'Stream']\n",
    "        },\n",
    "        'fastest_detector': {\n",
    "            'detector': results_df.loc[results_df['Runtime_s'].idxmin(), 'Detector'],\n",
    "            'time': float(results_df['Runtime_s'].min()),\n",
    "            'stream': results_df.loc[results_df['Runtime_s'].idxmin(), 'Stream']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add rankings\n",
    "    if 'Global_Score' in results_df.columns:\n",
    "        global_ranking = results_df.groupby('Detector')['Global_Score'].mean().sort_values(ascending=False)\n",
    "        best_performers['global_ranking'] = global_ranking.head(10).to_dict()\n",
    "    \n",
    "    # Export as JSON\n",
    "    json_path = f\"{output_dir}/best_performers_{timestamp}.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(best_performers, f, indent=2)\n",
    "    print(f\"üèÜ Best performers report: {json_path}\")\n",
    "    \n",
    "    # 4. LaTeX table for papers\n",
    "    latex_summary = results_df.groupby('Detector')[['Accuracy', 'Macro_F1', 'F1@AR', 'Beta_Score', 'Mean_Delay']].mean()\n",
    "    latex_table = latex_summary.round(3).to_latex(float_format=\"%.3f\")\n",
    "    \n",
    "    latex_path = f\"{output_dir}/latex_table_{timestamp}.tex\"\n",
    "    with open(latex_path, 'w') as f:\n",
    "        f.write(\"% Enhanced Concept Drift Benchmark Results\\\\n\")\n",
    "        f.write(\"% Generated by ConceptDrift_Baseline_v3\\\\n\\\\n\")\n",
    "        f.write(latex_table)\n",
    "    print(f\"üìù LaTeX table exported: {latex_path}\")\n",
    "    \n",
    "    # 5. Create README with experiment details\n",
    "    readme_content = f\"\"\"# Concept Drift Benchmark Results v3\n",
    "    \n",
    "## Experiment Details\n",
    "- **Timestamp**: {timestamp}\n",
    "- **Total Experiments**: {len(results_df)}\n",
    "- **Detectors Tested**: {results_df['Detector'].nunique()}\n",
    "- **Streams Tested**: {results_df['Stream'].nunique()}\n",
    "- **Framework**: Enhanced Baseline v3 with unified interface\n",
    "\n",
    "## Files Generated\n",
    "- `concept_drift_benchmark_v3_{timestamp}.csv`: Full experimental results\n",
    "- `detector_summary_{timestamp}.csv`: Performance statistics by detector\n",
    "- `stream_summary_{timestamp}.csv`: Performance statistics by stream  \n",
    "- `best_performers_{timestamp}.json`: Top performers and rankings\n",
    "- `latex_table_{timestamp}.tex`: LaTeX formatted results table\n",
    "\n",
    "## Top Performers\n",
    "\n",
    "### Best F1@AR Score\n",
    "- **Detector**: {best_performers['best_f1_ar']['detector']}\n",
    "- **Score**: {best_performers['best_f1_ar']['score']:.4f}\n",
    "- **Stream**: {best_performers['best_f1_ar']['stream']}\n",
    "\n",
    "### Best Detection Quality (Œ≤-score)  \n",
    "- **Detector**: {best_performers['best_beta_score']['detector']}\n",
    "- **Score**: {best_performers['best_beta_score']['score']:.4f}\n",
    "- **Stream**: {best_performers['best_beta_score']['stream']}\n",
    "\n",
    "### Fastest Detection\n",
    "- **Detector**: {best_performers['fastest_detector']['detector']}\n",
    "- **Runtime**: {best_performers['fastest_detector']['time']:.3f}s\n",
    "- **Stream**: {best_performers['fastest_detector']['stream']}\n",
    "\n",
    "## Metrics Explanation\n",
    "- **F1@AR**: F1-score penalized by alarm rate (F1 - Œª√óAR)\n",
    "- **Œ≤-score**: Balanced detection precision-recall measure\n",
    "- **Mean_Delay**: Average detection delay in samples\n",
    "- **Accuracy@AR**: Accuracy penalized by alarm rate\n",
    "\n",
    "Generated by Enhanced Concept Drift Benchmark v3\n",
    "\"\"\"\n",
    "    \n",
    "    readme_path = f\"{output_dir}/README_{timestamp}.md\"\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"üìã Experiment README: {readme_path}\")\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ All results exported to: {Path(output_dir).absolute()}\")\n",
    "    return output_dir\n",
    "\n",
    "# Export comprehensive results\n",
    "export_dir = export_comprehensive_results(enhanced_results_df)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéâ ENHANCED CONCEPT DRIFT BENCHMARK v3 - COMPLETE! \n",
    "\n",
    "üìä **Summary Statistics:**\n",
    "   ‚Ä¢ {len(enhanced_results_df)} total experiments\n",
    "   ‚Ä¢ {enhanced_results_df['Detector'].nunique()} drift detectors tested\n",
    "   ‚Ä¢ {enhanced_results_df['Stream'].nunique()} data streams evaluated\n",
    "   ‚Ä¢ {enhanced_results_df['Runtime_s'].sum():.2f}s total runtime\n",
    "\n",
    "üèÜ **Key Achievements:**\n",
    "   ‚Ä¢ Unified detector interface with River integration\n",
    "   ‚Ä¢ Advanced metrics: F1@AR, Œ≤-score, delay statistics  \n",
    "   ‚Ä¢ Comprehensive visualization suite\n",
    "   ‚Ä¢ Multi-format result export\n",
    "\n",
    "üìÅ **Results**: {export_dir}\n",
    "\n",
    "üî¨ **Next Steps**: Analyze results, compare with literature, extend with additional detectors\n",
    "\n",
    "Thank you for using Enhanced Concept Drift Benchmark v3! üöÄ\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddbb7b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/mnt/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m out_csv = \u001b[33m\"\u001b[39m\u001b[33m/mnt/data/baseline_v2_results_summary.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved:\u001b[39m\u001b[33m\"\u001b[39m, out_csv)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sandboxes/One-or-Two-Things-We-Know-about-Concept-Drift/venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sandboxes/One-or-Two-Things-We-Know-about-Concept-Drift/venv/lib/python3.12/site-packages/pandas/core/generic.py:3986\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3975\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3977\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3978\u001b[39m     frame=df,\n\u001b[32m   3979\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3983\u001b[39m     decimal=decimal,\n\u001b[32m   3984\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3986\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sandboxes/One-or-Two-Things-We-Know-about-Concept-Drift/venv/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sandboxes/One-or-Two-Things-We-Know-about-Concept-Drift/venv/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sandboxes/One-or-Two-Things-We-Know-about-Concept-Drift/venv/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sandboxes/One-or-Two-Things-We-Know-about-Concept-Drift/venv/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '/mnt/data'"
     ]
    }
   ],
   "source": [
    "\n",
    "out_csv = \"/mnt/data/baseline_v2_results_summary.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6dfd57",
   "metadata": {},
   "source": [
    "### (Tu·ª≥ ch·ªçn) V·∫Ω timeline drift vs. alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_timeline(r, n_points: int):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    for dp in r.drift_points:\n",
    "        plt.axvline(dp, linestyle=\"--\", alpha=0.6)\n",
    "    for a in r.alarms:\n",
    "        plt.axvline(a, color=\"r\", alpha=0.7)\n",
    "    plt.xlim(0, n_points)\n",
    "    plt.title(f\"{r.stream} / {r.name}: drift (--) vs alarms (red)\")\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.show()\n",
    "\n",
    "# V√≠ d·ª•\n",
    "plot_timeline(all_details[(\"SEA\",\"ADWIN\")], N)\n",
    "plot_timeline(all_details[(\"LED_abrupt\",\"FHDDMS\")], N)\n",
    "plot_timeline(all_details[(\"InterchangingRBF\",\"ShapeDD\")], N)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
