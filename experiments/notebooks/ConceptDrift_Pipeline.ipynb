{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee209ef",
   "metadata": {},
   "source": [
    "# Concept Drift Detection Methods Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import psutil\n",
    "import gc\n",
    "from collections import deque\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# matplotlib\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# River imports\n",
    "from river import compose, linear_model, preprocessing, metrics\n",
    "from river.drift import ADWIN\n",
    "from river.drift.binary import DDM, EDDM, FHDDM, HDDM_A, HDDM_W\n",
    "\n",
    "# Setup paths for local modules\n",
    "sys.path.insert(0, os.path.abspath('../backup'))\n",
    "\n",
    "# Import original implementations\n",
    "from gen_data import gen_random\n",
    "from dawidd import dawidd\n",
    "from shape_dd import shape\n",
    "from d3 import d3\n",
    "\n",
    "# Random seed configuration\n",
    "RANDOM_SEED = 999\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(RANDOM_SEED)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prequential_accuracy(predictions, true_labels, window_size=100):\n",
    "    \"\"\"Calculate prequential (test-then-train) accuracy over time.\"\"\"\n",
    "    accuracies = []\n",
    "    for i in range(len(predictions)):\n",
    "        start_idx = max(0, i - window_size + 1)\n",
    "        window_preds = predictions[start_idx:i+1]\n",
    "        window_true = true_labels[start_idx:i+1]\n",
    "        \n",
    "        if len(window_preds) > 0:\n",
    "            accuracy = np.mean(np.array(window_preds) == np.array(window_true))\n",
    "            accuracies.append(accuracy)\n",
    "        else:\n",
    "            accuracies.append(0.0)\n",
    "    \n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mttd_metrics(detections, true_drifts, acceptable_delta=100, allow_early_detection=True):\n",
    "    \"\"\"\n",
    "    Calculate Mean Time To Detection and TP/FP/FN metrics with improved one-to-one matching.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    detections : list\n",
    "        List of detection timestamps\n",
    "    true_drifts : list\n",
    "        List of true drift timestamps  \n",
    "    acceptable_delta : int, default=100\n",
    "        Maximum acceptable delay for a detection to be considered valid\n",
    "    allow_early_detection : bool, default=True\n",
    "        Whether to allow detections slightly before true drifts (within acceptable_delta/2)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Performance metrics including MTTD, precision, recall, TP/FP/FN\n",
    "    \"\"\"\n",
    "    if true_drifts is None:\n",
    "        true_drifts = []\n",
    "\n",
    "    sorted_drifts = sorted(int(d) for d in true_drifts)\n",
    "    sorted_detections = sorted(int(d) for d in detections)\n",
    "\n",
    "    if len(sorted_drifts) == 0:\n",
    "        return {\n",
    "            'ground_truth_available': False,\n",
    "            'mttd': np.nan,\n",
    "            'tp': np.nan,\n",
    "            'fp': np.nan,\n",
    "            'fn': np.nan,\n",
    "            'precision': np.nan,\n",
    "            'recall': np.nan,\n",
    "            'detection_delays': [],\n",
    "            'acceptable_delta': acceptable_delta\n",
    "        }\n",
    "\n",
    "    # Enhanced matching with Hungarian algorithm-like approach\n",
    "    # For each drift, find all valid detections within acceptable window\n",
    "    valid_matches = {}  # drift_idx -> [(det_idx, delay), ...]\n",
    "    \n",
    "    for drift_idx, drift in enumerate(sorted_drifts):\n",
    "        valid_matches[drift_idx] = []\n",
    "        \n",
    "        for det_idx, detection in enumerate(sorted_detections):\n",
    "            delay = detection - drift\n",
    "            \n",
    "            # Allow early detection within half the acceptable window\n",
    "            min_delay = -acceptable_delta // 2 if allow_early_detection else 0\n",
    "            max_delay = acceptable_delta\n",
    "            \n",
    "            if min_delay <= delay <= max_delay:\n",
    "                valid_matches[drift_idx].append((det_idx, delay))\n",
    "    \n",
    "    # Greedy assignment: prioritize shortest delays and unused detections\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    used_detection_idx = set()\n",
    "    detection_delays = []\n",
    "    matched_pairs = []  # For debugging/analysis\n",
    "    \n",
    "    # Sort drifts by number of available matches (fewer options first)\n",
    "    drift_priorities = sorted(valid_matches.keys(), \n",
    "                             key=lambda d: len(valid_matches[d]))\n",
    "    \n",
    "    for drift_idx in drift_priorities:\n",
    "        drift = sorted_drifts[drift_idx]\n",
    "        available_matches = [\n",
    "            (det_idx, delay) for det_idx, delay in valid_matches[drift_idx]\n",
    "            if det_idx not in used_detection_idx\n",
    "        ]\n",
    "        \n",
    "        if available_matches:\n",
    "            # Choose the detection with minimum absolute delay\n",
    "            best_det_idx, best_delay = min(available_matches, key=lambda x: abs(x[1]))\n",
    "            \n",
    "            tp += 1\n",
    "            detection_delays.append(float(best_delay))\n",
    "            used_detection_idx.add(best_det_idx)\n",
    "            matched_pairs.append((drift, sorted_detections[best_det_idx], best_delay))\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    # Count false positives (unmatched detections)\n",
    "    fp = sum(1 for idx in range(len(sorted_detections)) if idx not in used_detection_idx)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mttd = float(np.mean(detection_delays)) if detection_delays else float('inf')\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    \n",
    "    # Additional diagnostic information\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'ground_truth_available': True,\n",
    "        'mttd': mttd,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'detection_delays': detection_delays,\n",
    "        'acceptable_delta': acceptable_delta,\n",
    "        'allow_early_detection': allow_early_detection,\n",
    "        'matched_pairs': matched_pairs,  # For analysis/debugging\n",
    "        'total_true_drifts': len(sorted_drifts),\n",
    "        'total_detections': len(sorted_detections)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_resources():\n",
    "    \"\"\"Monitor memory usage.\"\"\"\n",
    "    process = psutil.Process()\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return memory_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f890b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_window_based_method(\n",
    "    X, y, true_drifts, method_name, chunk_size=150, overlap=100,\n",
    "    cooldown=None, d3_threshold=0.5, dawidd_alpha=0.05, shape_alpha=0.05\n",
    "):\n",
    "    \n",
    "    if cooldown is None:\n",
    "        cooldown = chunk_size // 2\n",
    "\n",
    "    start_mem = monitor_resources()\n",
    "    t0_all = time.perf_counter()\n",
    "\n",
    "    method_detections = []\n",
    "    runtime_per_batch_ms = []\n",
    "    drift_estimates = []\n",
    "    true_drift_scores = []\n",
    "    true_drift_reference = sorted(int(d) for d in true_drifts) if true_drifts is not None else []\n",
    "\n",
    "    shift = chunk_size - overlap\n",
    "    batches = []\n",
    "    for i in range(int(X.shape[0] / shift) - int(chunk_size / shift) + 1):\n",
    "        batch_indices = np.arange(i * shift, min(chunk_size + i * shift, X.shape[0]))\n",
    "        batches.append(batch_indices)\n",
    "\n",
    "    batch_count = len(batches)\n",
    "    last_det = -10**9\n",
    "\n",
    "    # Precompute ShapeDD p-values once per stream\n",
    "    shape_total_time_ms = 0.0\n",
    "    if method_name == 'ShapeDD':\n",
    "        t_shape0 = time.perf_counter()\n",
    "        shp_full = shape(X, 50, chunk_size, 2500)\n",
    "        shp_pvals = shp_full[:, 2]\n",
    "        t_shape1 = time.perf_counter()\n",
    "        shape_total_time_ms = (t_shape1 - t_shape0) * 1000.0\n",
    "\n",
    "    for b in batches:\n",
    "        xb = X[b]\n",
    "        yb = y[b]\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        # Drift score replicates the continuous analysis when drift states are supplied\n",
    "        if len(yb) > 1:\n",
    "            drift_score = (yb[None, :] != yb[:, None]).sum() / (len(yb) * (len(yb) - 1))\n",
    "        else:\n",
    "            drift_score = 0.0\n",
    "        true_drift_scores.append(float(drift_score))\n",
    "\n",
    "        try:\n",
    "            if method_name == 'D3':\n",
    "                d3_estimates = {\n",
    "                    'linear': d3(xb),\n",
    "                    'extra_trees': d3(xb, ExtraTreesClassifier(max_depth=5)),\n",
    "                    'random_forest': d3(xb, RandomForestClassifier(max_depth=5)),\n",
    "                    'knn': d3(xb, KNeighborsClassifier())\n",
    "                }\n",
    "                est = float(d3_estimates['linear'])\n",
    "                trigger = est > d3_threshold\n",
    "                det_pos = b[-1]\n",
    "\n",
    "            elif method_name == 'DAWIDD':\n",
    "                _, est = dawidd(xb, 'rbf')\n",
    "                est = float(est)\n",
    "                trigger = est < dawidd_alpha\n",
    "                det_pos = b[-1]\n",
    "\n",
    "            elif method_name == 'ShapeDD':\n",
    "                batch_pvals = shp_pvals[b]\n",
    "                est = float(batch_pvals.min())\n",
    "                trigger = est < shape_alpha\n",
    "                det_pos = b[int(np.argmin(batch_pvals))]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Unknown window method: {method_name}')\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f'Error in {method_name}: {exc}')\n",
    "            est, trigger, det_pos = 0.0, False, b[-1]\n",
    "\n",
    "        drift_estimates.append(est)\n",
    "\n",
    "        if trigger and (det_pos - last_det >= cooldown):\n",
    "            method_detections.append(int(det_pos))\n",
    "            last_det = det_pos\n",
    "\n",
    "        batch_ms = (time.perf_counter() - t1) * 1000.0\n",
    "        if method_name == 'ShapeDD':\n",
    "            batch_ms += shape_total_time_ms / max(1, batch_count)\n",
    "        runtime_per_batch_ms.append(batch_ms)\n",
    "\n",
    "    total_time_s = time.perf_counter() - t0_all\n",
    "    end_mem = monitor_resources()\n",
    "    memory_mb = max(0.0, end_mem - start_mem)\n",
    "\n",
    "    estimate_correlation = 0.0\n",
    "    if len(drift_estimates) > 1 and len(true_drift_scores) > 1:\n",
    "        try:\n",
    "            est_arr = np.asarray(drift_estimates, dtype=float)\n",
    "            tds_arr = np.asarray(true_drift_scores, dtype=float)\n",
    "            if len(est_arr) == len(tds_arr) and np.std(est_arr) > 0 and np.std(tds_arr) > 0:\n",
    "                estimate_correlation = np.corrcoef(est_arr, tds_arr)[0, 1]\n",
    "                if method_name in ['DAWIDD', 'ShapeDD']:\n",
    "                    estimate_correlation = -estimate_correlation\n",
    "        except Exception as exc:\n",
    "            print(f'Warning: Could not calculate correlation for {method_name}: {exc}')\n",
    "            estimate_correlation = 0.0\n",
    "\n",
    "    mttd_metrics = calculate_mttd_metrics(method_detections, true_drifts)\n",
    "\n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'paradigm': 'window',\n",
    "        'detections': method_detections,\n",
    "        'drift_estimates': drift_estimates,\n",
    "        'true_drift_scores': true_drift_scores,\n",
    "        'true_drifts_reference': true_drift_reference,\n",
    "        'estimate_correlation': float(estimate_correlation),\n",
    "        'runtime_per_batch_ms': float(np.mean(runtime_per_batch_ms)) if runtime_per_batch_ms else 0.0,\n",
    "        'runtime_per_instance_ms': (total_time_s * 1000.0) / max(1, len(X)),\n",
    "        'total_runtime_s': total_time_s,\n",
    "        'memory_mb': memory_mb,\n",
    "        **mttd_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_streaming_method(\n",
    "    X, y, true_drifts=None, method_name=None, feature_names=None,\n",
    "    stream_dicts=None, initial_train_period=200, warm_start_window=200,\n",
    "    detection_cooldown=5, accuracy_window_size=50\n",
    "):\n",
    "    if method_name is None:\n",
    "        raise ValueError('A drift detector name must be provided.')\n",
    "\n",
    "    start_memory = monitor_resources()\n",
    "    start_time = time.time()\n",
    "\n",
    "    def make_detector(name):\n",
    "        if name == 'ADWIN':\n",
    "            return ADWIN(delta=0.002)\n",
    "        if name == 'DDM':\n",
    "            return DDM()\n",
    "        if name == 'EDDM':\n",
    "            return EDDM(alpha=0.95, beta=0.9)\n",
    "        if name == 'HDDM_A':\n",
    "            return HDDM_A()\n",
    "        if name == 'HDDM_W':\n",
    "            return HDDM_W()\n",
    "        if name == 'FHDDM':\n",
    "            return FHDDM(short_window_size=20)\n",
    "        raise ValueError(f\"Unknown streaming method: {name}\")\n",
    "\n",
    "    def make_model():\n",
    "        return compose.Pipeline(\n",
    "            preprocessing.OneHotEncoder(),\n",
    "            linear_model.LogisticRegression()\n",
    "        )\n",
    "\n",
    "    # Determine optimal signal type for each detector\n",
    "    def get_signal_type(detector_name):\n",
    "        \"\"\"Return the optimal signal type for each detector.\"\"\"\n",
    "        continuous_preferred = {'ADWIN'}\n",
    "        continuous_capable = {'HDDM_A'}  # Can use both but may benefit from continuous\n",
    "        binary_optimal = {'DDM', 'EDDM', 'HDDM_W', 'FHDDM'}\n",
    "        \n",
    "        if detector_name in continuous_preferred:\n",
    "            return 'continuous'\n",
    "        elif detector_name in continuous_capable:\n",
    "            return 'continuous'  # Use continuous for potentially better performance\n",
    "        else:\n",
    "            return 'binary'\n",
    "\n",
    "    drift_detector = make_detector(method_name)\n",
    "    model = make_model()\n",
    "    signal_type = get_signal_type(method_name)\n",
    "\n",
    "    y_array = np.asarray(y)\n",
    "    y_list = y_array.tolist()\n",
    "\n",
    "    if stream_dicts is not None:\n",
    "        stream_sequence = list(stream_dicts)\n",
    "    else:\n",
    "        X_array = np.asarray(X)\n",
    "        if feature_names is None:\n",
    "            feature_names = [f'f_{j}' for j in range(X_array.shape[1])]\n",
    "        stream_sequence = []\n",
    "        for row in X_array:\n",
    "            feature_map = {feature_names[idx]: float(row[idx]) for idx in range(len(feature_names))}\n",
    "            stream_sequence.append(feature_map)\n",
    "\n",
    "    if len(stream_sequence) != len(y_list):\n",
    "        raise ValueError('Feature stream and label stream must have the same length.')\n",
    "\n",
    "    detections = []\n",
    "    runtime_per_instance = []\n",
    "    predictions = []\n",
    "    accuracies = []\n",
    "    recent_buffer = deque(maxlen=warm_start_window)\n",
    "    accuracy_metric = metrics.Accuracy()\n",
    "    true_drift_reference = sorted(int(d) for d in true_drifts) if true_drifts is not None else []\n",
    "    last_detection_idx = -10**9\n",
    "    \n",
    "    # Enhanced signal tracking for different detector types\n",
    "    accuracy_buffer = deque(maxlen=accuracy_window_size)  # For windowed accuracy calculation\n",
    "    error_rate_buffer = deque(maxlen=accuracy_window_size)  # For error rate calculation\n",
    "\n",
    "    for i, (x_raw, y_true_val) in enumerate(zip(stream_sequence, y_list)):\n",
    "        instance_start = time.time()\n",
    "        x_dict = dict(x_raw)\n",
    "        if isinstance(y_true_val, np.generic):\n",
    "            y_true = y_true_val.item()\n",
    "        else:\n",
    "            y_true = y_true_val\n",
    "\n",
    "        y_pred = model.predict_one(x_dict)\n",
    "        predictions.append(int(y_pred) if isinstance(y_pred, (np.integer, np.bool_, bool)) else (y_pred if y_pred is not None else 0))\n",
    "\n",
    "        in_warmup = (i < initial_train_period) or (y_pred is None)\n",
    "        if not in_warmup:\n",
    "            is_correct = int(y_pred == y_true)\n",
    "            accuracies.append(is_correct)\n",
    "            accuracy_metric.update(y_true, y_pred)\n",
    "            \n",
    "            # Update signal buffers\n",
    "            accuracy_buffer.append(is_correct)\n",
    "            error_rate_buffer.append(1 - is_correct)\n",
    "            \n",
    "            # Prepare optimized signal based on detector type\n",
    "            if signal_type == 'continuous':\n",
    "                if method_name == 'ADWIN':\n",
    "                    # Use windowed accuracy for ADWIN (more sensitive to gradual changes)\n",
    "                    if len(accuracy_buffer) >= min(10, accuracy_window_size // 2):\n",
    "                        windowed_accuracy = float(np.mean(accuracy_buffer))\n",
    "                        signal = windowed_accuracy\n",
    "                    else:\n",
    "                        # Fallback to current accuracy for initial samples\n",
    "                        signal = float(is_correct)\n",
    "                elif method_name == 'HDDM_A':\n",
    "                    # Use windowed error rate for HDDM_A (can benefit from continuous signals)\n",
    "                    if len(error_rate_buffer) >= min(10, accuracy_window_size // 2):\n",
    "                        windowed_error_rate = float(np.mean(error_rate_buffer))\n",
    "                        signal = windowed_error_rate\n",
    "                    else:\n",
    "                        signal = float(1 - is_correct)\n",
    "                else:\n",
    "                    # Default continuous signal (error rate)\n",
    "                    signal = float(1 - is_correct)\n",
    "            else:\n",
    "                # Binary signal for methods that prefer it\n",
    "                signal = bool(1 - is_correct)  # Error signal (True = error, False = correct)\n",
    "\n",
    "            # Update drift detector with optimized signal\n",
    "            drift_detector.update(signal)\n",
    "\n",
    "            if drift_detector.drift_detected and (i - last_detection_idx) >= detection_cooldown:\n",
    "                detections.append(i)\n",
    "                last_detection_idx = i\n",
    "                \n",
    "                # Model restart with warm start\n",
    "                tail_samples = list(recent_buffer)[-min(50, len(recent_buffer)):]\n",
    "                drift_detector = make_detector(method_name)\n",
    "                model = make_model()\n",
    "                recent_buffer.clear()\n",
    "                recent_buffer.extend(tail_samples)\n",
    "                \n",
    "                # Clear signal buffers for fresh start\n",
    "                accuracy_buffer.clear()\n",
    "                error_rate_buffer.clear()\n",
    "                \n",
    "                # Retrain on recent samples\n",
    "                for x_hist, y_hist in tail_samples:\n",
    "                    model.learn_one(dict(x_hist), y_hist)\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "\n",
    "        model.learn_one(x_dict, y_true)\n",
    "        recent_buffer.append((dict(x_dict), y_true))\n",
    "\n",
    "        instance_time = (time.time() - instance_start) * 1000.0\n",
    "        runtime_per_instance.append(instance_time)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    end_memory = monitor_resources()\n",
    "    memory_usage = max(0.0, end_memory - start_memory)\n",
    "\n",
    "    mttd_metrics = calculate_mttd_metrics(detections, true_drifts)\n",
    "\n",
    "    post_burnin_start = min(initial_train_period, len(accuracies))\n",
    "    post_burnin_accuracies = accuracies[post_burnin_start:]\n",
    "    moving_accuracy = float(np.mean(post_burnin_accuracies)) if post_burnin_accuracies else 0.0\n",
    "    accuracy_std = float(np.std(post_burnin_accuracies)) if post_burnin_accuracies else 0.0\n",
    "    prequential_accuracy = accuracy_metric.get() if getattr(accuracy_metric, 'n_samples', 0) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'paradigm': 'streaming',\n",
    "        'detections': detections,\n",
    "        'total_detections': len(detections),\n",
    "        'runtime_per_instance_ms': float(np.mean(runtime_per_instance)) if runtime_per_instance else 0.0,\n",
    "        'total_runtime_s': total_time,\n",
    "        'memory_mb': memory_usage,\n",
    "        'prequential_accuracy': prequential_accuracy,\n",
    "        'moving_accuracy': moving_accuracy,\n",
    "        'accuracy_std': accuracy_std,\n",
    "        'predictions': predictions,\n",
    "        'accuracies': accuracies,\n",
    "        'true_drifts_reference': true_drift_reference,\n",
    "        'signal_type_used': signal_type,  # For debugging/analysis\n",
    "        'accuracy_window_size': accuracy_window_size,  # For reproducibility\n",
    "        **mttd_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_scenarios = [\n",
    "    {\n",
    "        'name': 'Common_Drift',\n",
    "        'params': {'dist': 'unif', 'intens': 1, 'number': 1, 'alt': True, 'length': 1000, 'dims': 3}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Methods to evaluate\n",
    "window_methods = ['D3', 'DAWIDD', 'ShapeDD']\n",
    "streaming_methods = ['ADWIN', 'DDM', 'EDDM', 'HDDM_A', 'HDDM_W', 'FHDDM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d17428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(X, e, angle_step_deg=25.0, bias=0.0, noise_rate=0.01, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = np.asarray(X)\n",
    "    e = np.asarray(e)\n",
    "    n, d = X.shape\n",
    "    if d < 2:\n",
    "        score = X.sum(axis=1) - bias\n",
    "        y = (score > 0).astype(int)\n",
    "    else:\n",
    "        phi = np.deg2rad(angle_step_deg) * e\n",
    "        w0 = np.cos(phi)\n",
    "        w1 = np.sin(phi)\n",
    "        score = w0 * X[:, 0] + w1 * X[:, 1] - bias\n",
    "        y = (score > 0).astype(int)\n",
    "\n",
    "    if noise_rate and noise_rate > 0:\n",
    "        flips = rng.random(n) < noise_rate\n",
    "        y[flips] ^= 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d75ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the scenario from unified_scenarios\n",
    "scenario = unified_scenarios[0]  # Use the first (and only) scenario\n",
    "scenario_name = scenario['name']\n",
    "params = scenario['params']\n",
    "\n",
    "label_config = scenario.get('label_config', {\n",
    "    'angle_step_deg': 25.0,\n",
    "    'bias': 1.255,\n",
    "    'noise_rate': 0.05,\n",
    "    'seed': RANDOM_SEED,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, e = gen_random(**params)\n",
    "y = make_labels(X, e, **label_config)\n",
    "# Calculate true drift positions from the drift indicator\n",
    "true_drifts = np.where(np.diff(e) != 0)[0] + 1\n",
    "stream_feature_names = [f'x_{idx}' for idx in range(X.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efad26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "if X.shape[1] == 1:\n",
    "    plt.scatter(range(len(X)), X[:, 0], c=e, cmap='tab10', s=10)\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Feature Value')\n",
    "elif X.shape[1] == 2:\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=e, cmap='tab10', s=10)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "else:\n",
    "    # For higher dimensions, plot first two features colored by y\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=e, cmap='tab10', s=10)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "plt.title('Generated Data (colored by label y)')\n",
    "plt.colorbar(label='Label y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_unified_experiment(scenario_config):\n",
    "    print(f\"  Data: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "    print(f\"  True drifts at: {list(true_drifts)}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Evaluate window-based methods\n",
    "    for method_name in window_methods:\n",
    "        print(f\"  Running {method_name}\")\n",
    "        result = evaluate_window_based_method(X, e, true_drifts, method_name)\n",
    "        result['scenario'] = scenario_name\n",
    "        results.append(result)\n",
    "    \n",
    "    # Evaluate streaming methods\n",
    "    for method_name in streaming_methods:\n",
    "        print(f\"  Running {method_name}\")\n",
    "        result = evaluate_streaming_method(\n",
    "            X,\n",
    "            y,\n",
    "            true_drifts,\n",
    "            method_name,\n",
    "            feature_names=stream_feature_names\n",
    "        )\n",
    "        result['scenario'] = scenario_name\n",
    "        results.append(result)\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unified_results = []\n",
    "\n",
    "scenario_results = run_unified_experiment(scenario)\n",
    "all_unified_results.extend(scenario_results)\n",
    "    \n",
    "print(f\"Completed scenario: {scenario['name']}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "unified_df = pd.DataFrame(all_unified_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true drifts from the first scenario (since we know the data generation parameters)\n",
    "scenario_params = unified_scenarios[0]['params']  # Get the parameters used\n",
    "\n",
    "print(f\"Extracted {len(true_drifts)} true drift points: {list(true_drifts)}\")\n",
    "\n",
    "# Calculate F1-score\n",
    "unified_df['f1_score'] = 2 * unified_df['precision'] * unified_df['recall'] / (unified_df['precision'] + unified_df['recall'])\n",
    "unified_df['f1_score'] = unified_df['f1_score'].fillna(0)\n",
    "\n",
    "# Academic summary table\n",
    "summary_table = unified_df.groupby(['paradigm', 'method']).agg({\n",
    "    'precision': 'mean',\n",
    "    'recall': 'mean', \n",
    "    'f1_score': 'mean',\n",
    "    'mttd': lambda x: np.mean(x[x != float('inf')]) if any(x != float('inf')) else float('inf'),\n",
    "    'runtime_per_instance_ms': 'mean',\n",
    "    'tp': 'sum',\n",
    "    'fp': 'sum',\n",
    "    'fn': 'sum'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nPerformance Metrics by Method\")\n",
    "print(f\"{'Method':<12} {'Paradigm':<10} {'Precision':<10} {'Recall':<8} {'F1':<8} {'MTTD':<8} {'Runtime(ms)':<12}\")\n",
    "\n",
    "for (paradigm, method), row in summary_table.iterrows():\n",
    "    mttd_str = f\"{row['mttd']:.1f}\" if row['mttd'] != float('inf') else \"∞\"\n",
    "    print(f\"{method:<12} {paradigm:<10} {row['precision']:<10.3f} {row['recall']:<8.3f} \"\n",
    "            f\"{row['f1_score']:<8.3f} {mttd_str:<8} {row['runtime_per_instance_ms']:<12.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors and markers\n",
    "colors = {'window': '#2E86AB', 'streaming': '#A23B72'}  # Professional blue/burgundy\n",
    "markers = {'window': 'o', 'streaming': 's'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1615a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Performance Comparison by Paradigm\n",
    "plt.figure(figsize=(10, 6))\n",
    "metric_keys = ['precision', 'recall', 'f1_score']\n",
    "metric_labels = ['Precision', 'Recall', 'F₁-Score']\n",
    "\n",
    "x = np.arange(len(metric_labels))\n",
    "width = 0.35\n",
    "\n",
    "window_means = [unified_df[unified_df['paradigm'] == 'window'][m].mean() for m in metric_keys]\n",
    "streaming_means = [unified_df[unified_df['paradigm'] == 'streaming'][m].mean() for m in metric_keys]\n",
    "\n",
    "bars1 = plt.bar(x - width/2, window_means, width, label='Window-based', \n",
    "                color=colors['window'], alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "bars2 = plt.bar(x + width/2, streaming_means, width, label='Streaming', \n",
    "                color=colors['streaming'], alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (w_val, s_val) in enumerate(zip(window_means, streaming_means)):\n",
    "    plt.text(i - width/2, w_val + 0.01, f'{w_val:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "    plt.text(i + width/2, s_val + 0.01, f'{s_val:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Comparison by Paradigm')\n",
    "plt.xticks(x, metric_labels)\n",
    "plt.legend(frameon=True, fancybox=False, shadow=False)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Detection Timeline Analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot true drifts as vertical lines\n",
    "for drift in true_drifts:\n",
    "    plt.axvline(x=drift, color='red', linestyle='--', alpha=0.6, linewidth=2, zorder=1)\n",
    "\n",
    "# Create method mapping for y-positions with clear labels\n",
    "method_names = []\n",
    "y_positions = []\n",
    "y_pos = 0\n",
    "\n",
    "# Process window methods first, then streaming\n",
    "for paradigm in ['window', 'streaming']:\n",
    "    paradigm_data = unified_df[unified_df['paradigm'] == paradigm]\n",
    "    for _, row in paradigm_data.iterrows():\n",
    "        method_name = f\"{row['method']}\"\n",
    "        method_names.append(method_name)\n",
    "        y_positions.append(y_pos)\n",
    "        \n",
    "        detections = row['detections']\n",
    "        if len(detections) > 0:\n",
    "            plt.scatter(detections, [y_pos] * len(detections), \n",
    "                        c=colors[paradigm], marker=markers[paradigm], \n",
    "                        s=80, alpha=0.8, zorder=2, \n",
    "                        label=f\"{paradigm.title()}\" if method_name == paradigm_data.iloc[0]['method'] else \"\")\n",
    "        \n",
    "        # Add detection count annotation\n",
    "        if len(detections) > 0:\n",
    "            plt.text(scenario_params['length'] * 1.01, y_pos, f\"({len(detections)})\", \n",
    "                    va='center', fontsize=10, alpha=0.7)\n",
    "        \n",
    "        y_pos += 1\n",
    "\n",
    "plt.xlabel('Time (instances)')\n",
    "plt.ylabel('Detection Methods')\n",
    "plt.title('Detection Timeline (numbers show detection count)')\n",
    "plt.xlim(0, scenario_params['length'] * 1.15)\n",
    "plt.yticks(range(len(method_names)), method_names, fontsize=10)\n",
    "\n",
    "# Create custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], color='red', linestyle='--', label='True Drift'),\n",
    "                    Line2D([0], [0], marker='o', color='w', markerfacecolor=colors['window'], \n",
    "                        markersize=10, label='Window-based'),\n",
    "                    Line2D([0], [0], marker='s', color='w', markerfacecolor=colors['streaming'], \n",
    "                        markersize=10, label='Streaming')]\n",
    "plt.legend(handles=legend_elements, frameon=True, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Precision-Recall Trade-off Analysis\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for paradigm in ['window', 'streaming']:\n",
    "    data = unified_df[unified_df['paradigm'] == paradigm]\n",
    "    scatter = plt.scatter(data['recall'], data['precision'], \n",
    "                        c=colors[paradigm], marker=markers[paradigm], s=120, \n",
    "                        alpha=0.8, label=paradigm.title(), edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Add method labels\n",
    "    for _, row in data.iterrows():\n",
    "        plt.annotate(row['method'], (row['recall'], row['precision']), \n",
    "                    xytext=(8, 8), textcoords='offset points', fontsize=10, alpha=0.8,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "\n",
    "# Add diagonal line for reference\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1, label='Perfect Trade-off')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Trade-off Analysis', fontsize=14, fontweight='bold')\n",
    "plt.legend(frameon=True, fontsize=11)\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Performance vs Computational Cost Analysis\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for paradigm in ['window', 'streaming']:\n",
    "    data = unified_df[unified_df['paradigm'] == paradigm]\n",
    "    scatter = plt.scatter(data['runtime_per_instance_ms'], data['f1_score'], \n",
    "                        c=colors[paradigm], marker=markers[paradigm], s=120, \n",
    "                        alpha=0.8, label=paradigm.title(), edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Add method labels\n",
    "    for _, row in data.iterrows():\n",
    "        plt.annotate(row['method'], (row['runtime_per_instance_ms'], row['f1_score']), \n",
    "                    xytext=(8, 8), textcoords='offset points', fontsize=10, alpha=0.8,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.xlabel('Runtime per Instance (ms)', fontsize=12)\n",
    "plt.ylabel('F₁-Score', fontsize=12)\n",
    "plt.title('Performance vs Computational Cost Analysis', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')\n",
    "plt.legend(frameon=True, fontsize=11)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5: Detection Count vs True Drifts Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "methods = unified_df['method'].tolist()\n",
    "detection_counts = [len(row['detections']) for _, row in unified_df.iterrows()]\n",
    "true_drift_count = len(true_drifts)\n",
    "\n",
    "bars = plt.bar(range(len(methods)), detection_counts, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "plt.axhline(y=true_drift_count, color='red', linestyle='--', linewidth=3,\n",
    "            label=f'True Drifts ({true_drift_count})')\n",
    "\n",
    "# Color bars by paradigm\n",
    "for i, (_, row) in enumerate(unified_df.iterrows()):\n",
    "    color = colors[row['paradigm']]\n",
    "    bars[i].set_color(color)\n",
    "    # Add count labels on bars\n",
    "    plt.text(i, detection_counts[i] + 0.5, str(detection_counts[i]), \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Methods', fontsize=12)\n",
    "plt.ylabel('Detection Count', fontsize=12)\n",
    "plt.title('Detection Count vs True Drifts Analysis', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(len(methods)), methods, rotation=45, ha='right')\n",
    "\n",
    "# Create custom legend for paradigms\n",
    "legend_elements = [plt.Rectangle((0,0),1,1, facecolor=colors['window'], label='Window-based'),\n",
    "                   plt.Rectangle((0,0),1,1, facecolor=colors['streaming'], label='Streaming'),\n",
    "                   plt.Line2D([0], [0], color='red', linestyle='--', label=f'True Drifts ({true_drift_count})')]\n",
    "plt.legend(handles=legend_elements, frameon=True, fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 6: Performance Metrics Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "metrics_data = unified_df[['precision', 'recall', 'f1_score']].values\n",
    "methods = unified_df['method'].tolist()\n",
    "\n",
    "im = plt.imshow(metrics_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "plt.xticks(range(3), ['Precision', 'Recall', 'F₁-Score'], fontsize=12)\n",
    "plt.yticks(range(len(methods)), methods, fontsize=10)\n",
    "plt.title('Performance Metrics Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, shrink=0.8)\n",
    "cbar.set_label('Score', fontsize=12)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(methods)):\n",
    "    for j in range(3):\n",
    "        text = plt.text(j, i, f'{metrics_data[i, j]:.2f}',\n",
    "                        ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=10)\n",
    "\n",
    "# Add paradigm indicators on the right\n",
    "for i, (_, row) in enumerate(unified_df.iterrows()):\n",
    "    paradigm = row['paradigm']\n",
    "    plt.text(3.2, i, paradigm, ha='left', va='center', fontsize=9,\n",
    "             bbox=dict(boxstyle='round,pad=0.2', facecolor=colors[paradigm], alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 7: Mean Time To Detection Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "mttd_values = []\n",
    "method_labels = []\n",
    "colors_list = []\n",
    "paradigm_list = []\n",
    "\n",
    "for _, row in unified_df.iterrows():\n",
    "    if row['mttd'] != float('inf'):\n",
    "        mttd_values.append(row['mttd'])\n",
    "        method_labels.append(row['method'])\n",
    "        colors_list.append(colors[row['paradigm']])\n",
    "        paradigm_list.append(row['paradigm'])\n",
    "\n",
    "if mttd_values:\n",
    "    bars = plt.bar(range(len(mttd_values)), mttd_values, color=colors_list, alpha=0.8, \n",
    "                   edgecolor='black', linewidth=0.5)\n",
    "    plt.xlabel('Methods (with successful detections)', fontsize=12)\n",
    "    plt.ylabel('Mean Time To Detection (instances)', fontsize=12)\n",
    "    plt.title('Mean Time To Detection Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(method_labels)), method_labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, val in enumerate(mttd_values):\n",
    "        plt.text(i, val + max(mttd_values) * 0.01, f'{val:.1f}', \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Create custom legend for paradigms\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=colors['window'], label='Window-based'),\n",
    "                       plt.Rectangle((0,0),1,1, facecolor=colors['streaming'], label='Streaming')]\n",
    "    plt.legend(handles=legend_elements, frameon=True, fontsize=11)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No successful detections\\nwith finite MTTD', \n",
    "            ha='center', va='center', transform=plt.gca().transAxes, fontsize=14)\n",
    "\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 8: Error Analysis (TP/FP/FN)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "methods = unified_df['method'].tolist()\n",
    "tp_values = unified_df['tp'].tolist()\n",
    "fp_values = unified_df['fp'].tolist()\n",
    "fn_values = unified_df['fn'].tolist()\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = plt.bar(x - width, tp_values, width, label='True Positives', \n",
    "                color='green', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "bars2 = plt.bar(x, fp_values, width, label='False Positives', \n",
    "                color='orange', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "bars3 = plt.bar(x + width, fn_values, width, label='False Negatives', \n",
    "                color='red', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (tp, fp, fn) in enumerate(zip(tp_values, fp_values, fn_values)):\n",
    "    if tp > 0:\n",
    "        plt.text(i - width, tp + 0.1, str(tp), ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    if fp > 0:\n",
    "        plt.text(i, fp + 0.1, str(fp), ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    if fn > 0:\n",
    "        plt.text(i + width, fn + 0.1, str(fn), ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Methods', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Error Analysis: True Positives, False Positives, False Negatives', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, methods, rotation=45, ha='right')\n",
    "plt.legend(frameon=True, fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add paradigm indicators\n",
    "for i, (_, row) in enumerate(unified_df.iterrows()):\n",
    "    paradigm = row['paradigm']\n",
    "    plt.text(i, max(max(tp_values), max(fp_values), max(fn_values)) * 1.1, \n",
    "             paradigm[0].upper(), ha='center', va='center', fontsize=8,\n",
    "             bbox=dict(boxstyle='circle,pad=0.1', facecolor=colors[paradigm], alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40834a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset matplotlib parameters to default\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1510f8f",
   "metadata": {},
   "source": [
    "# Empirical Evaluation on Real-World Datasets\n",
    "\n",
    "This section evaluates the comparative performance of concept drift detection methods on real-world benchmark datasets from the River library. Unlike controlled synthetic experiments, real-world datasets present inherent challenges including unknown drift timing, varying signal-to-noise ratios, and complex distribution shifts. The evaluation methodology employs prequential learning (test-then-train) to assess detector responsiveness in realistic streaming scenarios.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The evaluation framework integrates standard benchmark datasets (Elec2, AirlinePassengers) directly from the River streaming machine learning library. For datasets without annotated drift locations, detection performance is assessed through computational efficiency metrics, detection frequency patterns, and prequential accuracy maintenance. The window-based methods operate on batch segments while streaming methods process instances sequentially, enabling direct paradigm comparison under identical data conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Dataset Loading Utilities for Real-World Benchmarks\n",
    "from river import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.Elec2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.AirlinePassengers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elec2 = pd.read_csv('../datasets/electricity.csv')\n",
    "df_elec2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline = pd.read_csv('../datasets/airline-passengers.csv')\n",
    "df_airline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_airline_copy = df_airline.copy()\n",
    "\n",
    "# Convert month to datetime for better plotting\n",
    "df_airline_copy['month'] = pd.to_datetime(df_airline['month'])\n",
    "df_airline_copy['passengers'] = df_airline['passengers']\n",
    "\n",
    "# Plot the AirlinePassengers dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(df_airline['month'], df_airline['passengers'], 'b-', linewidth=2, marker='o', markersize=4)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.title('Airline Passengers Time Series (1949-1960)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"Passenger count range: {df_airline['passengers'].min()} - {df_airline['passengers'].max()}\")\n",
    "print(f\"Mean passengers: {df_airline['passengers'].mean():.1f}\")\n",
    "print(f\"Standard deviation: {df_airline['passengers'].std():.1f}\")\n",
    "print(f\"Time period: {df_airline['month'].min()} to {df_airline['month'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare Elec2 data for plotting\n",
    "df_elec2_copy = df_elec2.copy()\n",
    "\n",
    "# Create a figure with subplots for better visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Price over time\n",
    "ax1.plot(range(len(df_elec2)), df_elec2_copy['nswprice'], 'b-', linewidth=1, alpha=0.7, label='NSW Price')\n",
    "ax1.plot(range(len(df_elec2)), df_elec2_copy['vicprice'], 'r-', linewidth=1, alpha=0.7, label='VIC Price')\n",
    "ax1.set_xlabel('Time (instances)')\n",
    "ax1.set_ylabel('Price')\n",
    "ax1.set_title('Electricity Prices Over Time (NSW vs VIC)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Price increase indicator (binary target)\n",
    "\n",
    "# Convert the binary class to integer for plotting\n",
    "df_elec2_copy['class'] = df_elec2_copy['class'].map({'UP': 1, 'DOWN': 0})\n",
    "\n",
    "price_increase = df_elec2_copy['class'].astype(int)\n",
    "ax2.plot(range(len(df_elec2_copy)), price_increase, 'g-', linewidth=1, alpha=0.8)\n",
    "ax2.fill_between(range(len(df_elec2_copy)), price_increase, alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Time (instances)')\n",
    "ax2.set_ylabel('Price Increase (Binary)')\n",
    "ax2.set_title('Price Increase Indicator Over Time')\n",
    "ax2.set_ylim(-0.1, 1.1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"Dataset shape: {df_elec2_copy.shape}\")\n",
    "print(f\"NSW price range: {df_elec2_copy['nswprice'].min():.3f} - {df_elec2['nswprice'].max():.3f}\")\n",
    "print(f\"VIC price range: {df_elec2_copy['vicprice'].min():.3f} - {df_elec2['vicprice'].max():.3f}\")\n",
    "print(f\"Price increase rate: {df_elec2_copy['class'].mean():.3f}\")\n",
    "print(f\"Class distribution: {df_elec2_copy['class'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e404958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elec2_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a smaller subset of Elec2 data for better visualization\n",
    "sample_size = 1000  # Use first 1000 instances for clearer visualization\n",
    "df_elec2_sample = df_elec2_copy.head(sample_size).copy()\n",
    "\n",
    "# Create a figure with subplots for better visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot 1: Price over time (sample)\n",
    "ax1.plot(range(len(df_elec2_sample)), df_elec2_sample['nswprice'], 'b-', linewidth=1, alpha=0.8, label='NSW Price')\n",
    "ax1.plot(range(len(df_elec2_sample)), df_elec2_sample['vicprice'], 'r-', linewidth=1, alpha=0.8, label='VIC Price')\n",
    "ax1.set_xlabel('Time (instances)')\n",
    "ax1.set_ylabel('Price')\n",
    "ax1.set_title(f'Electricity Prices Over Time - First {sample_size} Instances (NSW vs VIC)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Price increase indicator (binary target) - sample\n",
    "price_increase_sample = df_elec2_sample['class']\n",
    "ax2.plot(range(len(df_elec2_sample)), price_increase_sample, 'g-', linewidth=1, alpha=0.9)\n",
    "ax2.fill_between(range(len(df_elec2_sample)), price_increase_sample, alpha=0.4, color='green')\n",
    "ax2.set_xlabel('Time (instances)')\n",
    "ax2.set_ylabel('Price Increase (Binary)')\n",
    "ax2.set_title(f'Price Increase Indicator Over Time - First {sample_size} Instances')\n",
    "ax2.set_ylim(-0.1, 1.1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample statistics\n",
    "print(f\"Sample dataset shape: {df_elec2_sample.shape}\")\n",
    "print(f\"Sample NSW price range: {df_elec2_sample['nswprice'].min():.3f} - {df_elec2_sample['nswprice'].max():.3f}\")\n",
    "print(f\"Sample VIC price range: {df_elec2_sample['vicprice'].min():.3f} - {df_elec2_sample['vicprice'].max():.3f}\")\n",
    "print(f\"Full dataset has {len(df_elec2):,} instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-World Evaluation Functions (without ground truth)\n",
    "\n",
    "def evaluate_window_based_method_without_groundtruth(X, method_name):\n",
    "    \"\"\"Evaluate window-based methods on real-world data without ground truth.\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    start_memory = monitor_resources()\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    try:\n",
    "        if method_name == 'D3':\n",
    "            estimate = d3(X)\n",
    "            # Use adaptive threshold based on data characteristics\n",
    "            threshold = 0.5\n",
    "            if estimate > threshold:\n",
    "                detections.append(len(X) // 2)  # Mark center as detection point\n",
    "                \n",
    "        elif method_name == 'DAWIDD':\n",
    "            _, estimate = dawidd(X, 'rbf')\n",
    "            threshold = 0.05\n",
    "            if estimate < threshold:\n",
    "                detections.append(len(X) // 2)\n",
    "                \n",
    "        elif method_name == 'ShapeDD':\n",
    "            shape_result = shape(X, 50, len(X), 2500)\n",
    "            if len(shape_result) > 0:\n",
    "                min_p_value = np.min(shape_result[:, 2])\n",
    "                threshold = 0.05\n",
    "                if min_p_value < threshold:\n",
    "                    # Find all detection points below threshold\n",
    "                    detection_indices = np.where(shape_result[:, 2] < threshold)[0]\n",
    "                    detections.extend(detection_indices.tolist())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error in {method_name}: {e}\")\n",
    "        # Return default empty result on error\n",
    "        pass\n",
    "    \n",
    "    total_time = time.perf_counter() - start_time\n",
    "    end_memory = monitor_resources()\n",
    "    \n",
    "    # Calculate metrics without ground truth\n",
    "    result = {\n",
    "        'method': method_name,\n",
    "        'paradigm': 'window',\n",
    "        'detections': detections,\n",
    "        'total_detections': len(detections),\n",
    "        'runtime_per_instance_ms': (total_time * 1000.0) / len(X),\n",
    "        'total_runtime_s': total_time,\n",
    "        'memory_mb': max(0.0, end_memory - start_memory),\n",
    "        'data_size': len(X)\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_streaming_method_without_groundtruth(X, method_name, stream_dicts):\n",
    "    \"\"\"Evaluate streaming methods on real-world data without ground truth.\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    start_memory = monitor_resources()\n",
    "    \n",
    "    # Initialize detector based on method\n",
    "    if method_name == 'ADWIN':\n",
    "        detector = ADWIN(delta=0.002)\n",
    "    elif method_name == 'DDM':\n",
    "        detector = DDM()\n",
    "    elif method_name == 'EDDM':\n",
    "        detector = EDDM(alpha=0.95, beta=0.9)\n",
    "    elif method_name == 'HDDM_A':\n",
    "        detector = HDDM_A()\n",
    "    elif method_name == 'HDDM_W':\n",
    "        detector = HDDM_W()\n",
    "    elif method_name == 'FHDDM':\n",
    "        detector = FHDDM(short_window_size=20)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown streaming method: {method_name}\")\n",
    "    \n",
    "    # Initialize classifier for accuracy-based signal\n",
    "    model = compose.Pipeline(\n",
    "        preprocessing.StandardScaler(),\n",
    "        linear_model.LogisticRegression()\n",
    "    )\n",
    "    \n",
    "    detections = []\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    # Process stream one sample at a time\n",
    "    for i, (x_dict, y_true) in enumerate(stream_dicts):\n",
    "        try:\n",
    "            # Make prediction if model is trained\n",
    "            if i >= 50:  # Wait for some training data\n",
    "                y_pred = model.predict_one(x_dict)\n",
    "                is_correct = (y_pred == y_true) if y_pred is not None else False\n",
    "                accuracy_scores.append(float(is_correct))\n",
    "                \n",
    "                # Prepare signal for detector\n",
    "                if method_name == 'ADWIN':\n",
    "                    signal = float(is_correct)  # Accuracy signal for ADWIN\n",
    "                elif method_name in ['HDDM_A']:\n",
    "                    signal = float(not is_correct)  # Error rate for HDDM_A\n",
    "                else:\n",
    "                    signal = bool(not is_correct)  # Binary error signal for others\n",
    "                \n",
    "                # Update detector\n",
    "                detector.update(signal)\n",
    "                \n",
    "                # Check for drift\n",
    "                if detector.drift_detected:\n",
    "                    detections.append(i)\n",
    "            \n",
    "            # Train model incrementally\n",
    "            model.learn_one(x_dict, y_true)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if i == 50:  # Only print error once to avoid spam\n",
    "                print(f\"    Warning in {method_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    total_time = time.perf_counter() - start_time\n",
    "    end_memory = monitor_resources()\n",
    "    \n",
    "    # Calculate final accuracy\n",
    "    final_accuracy = np.mean(accuracy_scores) if accuracy_scores else 0.0\n",
    "    \n",
    "    result = {\n",
    "        'method': method_name,\n",
    "        'paradigm': 'streaming',\n",
    "        'detections': detections,\n",
    "        'total_detections': len(detections),\n",
    "        'runtime_per_instance_ms': (total_time * 1000.0) / len(X),\n",
    "        'total_runtime_s': total_time,\n",
    "        'memory_mb': max(0.0, end_memory - start_memory),\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'data_size': len(X)\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_realworld_experiment(dataset_data, dataset_name):    \n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Evaluate window-based methods\n",
    "    for method_name in window_methods:\n",
    "        print(f\"  Running {method_name}\")\n",
    "        result = evaluate_window_based_method_without_groundtruth(\n",
    "            dataset_data,\n",
    "            method_name=method_name\n",
    "        )\n",
    "        result['dataset'] = dataset_name\n",
    "        results.append(result)\n",
    "    \n",
    "    # TODO: Currently skipping streaming methods on real-world data due to lack of ground truth\n",
    "    # # Evaluate streaming methods\n",
    "    # for method_name in streaming_methods:\n",
    "    #     print(f\"  Running {method_name}\")\n",
    "    #     result = evaluate_streaming_method_without_groundtruth(\n",
    "    #         dataset_data,\n",
    "    #         method_name=method_name,\n",
    "    #         stream_dicts=dataset_data['stream_records']\n",
    "    #     )\n",
    "    #     result['dataset'] = dataset_name\n",
    "    #     results.append(result)\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12250a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elec2_copy\n",
    "\n",
    "df_elec2_benchmark = df_elec2_sample.drop(columns=['class']).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb526b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elec2_benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd44196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_benchmark = df_airline_copy['month'].to_numpy().reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_realworld_results = []\n",
    "\n",
    "# for dataset_name, dataset_data in loaded_datasets.items():\n",
    "#     dataset_results = run_realworld_experiment(dataset_data, dataset_name)\n",
    "#     all_realworld_results.extend(dataset_results)\n",
    "#     print(f\"Completed {dataset_name}: {len(dataset_results)} method evaluations\")\n",
    "\n",
    "# Methods to evaluate\n",
    "window_methods = ['D3', 'DAWIDD', 'ShapeDD']\n",
    "streaming_methods = ['ADWIN', 'DDM', 'EDDM', 'HDDM_A', 'HDDM_W', 'FHDDM']\n",
    "\n",
    "dataset_name = 'AirLine'\n",
    "\n",
    "dataset_results = run_realworld_experiment(df_airline_benchmark, dataset_name)\n",
    "all_realworld_results.extend(dataset_results)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "realworld_df = pd.DataFrame(all_realworld_results)\n",
    "\n",
    "print(f\"\\nReal-World Evaluation Summary:\")\n",
    "print(f\"   Total evaluations: {len(all_realworld_results)}\")\n",
    "print(f\"   Datasets: {realworld_df['dataset'].unique().tolist()}\")\n",
    "print(f\"   Methods: {realworld_df['method'].unique().tolist()}\")\n",
    "\n",
    "# Add detection count column for consistency with synthetic results\n",
    "realworld_df['detection_count'] = realworld_df.apply(\n",
    "    lambda row: row.get('total_detections', len(row.get('detections', []))), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Real-World Results\n",
    "print(\"\\nReal-World Performance Summary\")\n",
    "print(f\"{'Method':<12} {'Dataset':<15} {'Paradigm':<10} {'Detections':<10} {'Runtime(ms)':<12} {'Accuracy':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for _, row in realworld_df.iterrows():\n",
    "    accuracy_str = f\"{row.get('final_accuracy', 0):.3f}\" if 'final_accuracy' in row else \"N/A\"\n",
    "    print(f\"{row['method']:<12} {row['dataset']:<15} {row['paradigm']:<10} \"\n",
    "          f\"{row['detection_count']:<10} {row['runtime_per_instance_ms']:<12.4f} {accuracy_str:<10}\")\n",
    "\n",
    "# Quick statistics\n",
    "print(f\"\\nQuick Statistics:\")\n",
    "print(f\"   Average detections per method: {realworld_df['detection_count'].mean():.1f}\")\n",
    "print(f\"   Average runtime per instance: {realworld_df['runtime_per_instance_ms'].mean():.4f} ms\")\n",
    "print(f\"   Methods with detections: {(realworld_df['detection_count'] > 0).sum()}/{len(realworld_df)}\")\n",
    "\n",
    "# Show detection counts by paradigm\n",
    "window_detections = realworld_df[realworld_df['paradigm'] == 'window']['detection_count'].sum()\n",
    "streaming_detections = realworld_df[realworld_df['paradigm'] == 'streaming']['detection_count'].sum()\n",
    "print(f\"   Total detections - Window: {window_detections}, Streaming: {streaming_detections}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-World Detection Count Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "methods = [f\"{row['method']}\\n({row['dataset']})\" for _, row in realworld_df.iterrows()]\n",
    "detection_counts = realworld_df['detection_count'].tolist()\n",
    "\n",
    "bars = plt.bar(range(len(methods)), detection_counts, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Color bars by paradigm (same colors as synthetic)\n",
    "for i, (_, row) in enumerate(realworld_df.iterrows()):\n",
    "    color = colors[row['paradigm']]\n",
    "    bars[i].set_color(color)\n",
    "    # Add count labels on bars\n",
    "    if detection_counts[i] > 0:\n",
    "        plt.text(i, detection_counts[i] + max(detection_counts) * 0.01, str(detection_counts[i]), \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Methods and Datasets', fontsize=12)\n",
    "plt.ylabel('Detection Count', fontsize=12)\n",
    "plt.title('Real-World Detection Count by Method', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(len(methods)), methods, rotation=45, ha='right')\n",
    "\n",
    "# Same legend as synthetic\n",
    "legend_elements = [plt.Rectangle((0,0),1,1, facecolor=colors['window'], label='Window-based'),\n",
    "                   plt.Rectangle((0,0),1,1, facecolor=colors['streaming'], label='Streaming')]\n",
    "plt.legend(handles=legend_elements, frameon=True, fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot detection points along with data visualization for real-world data\n",
    "\n",
    "fig, (ax3, ax4) = plt.subplots(2, 1, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Real-world Airline Data with Detections\n",
    "time_points = range(len(df_airline_benchmark))\n",
    "passengers = df_airline['passengers'].values\n",
    "ax3.plot(time_points, passengers, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "\n",
    "# Add detection points from real-world analysis\n",
    "for _, row in realworld_df.iterrows():\n",
    "    detections = row['detections']\n",
    "    if len(detections) > 0:\n",
    "        detection_passengers = [passengers[d] for d in detections if d < len(passengers)]\n",
    "        ax3.scatter(detections, detection_passengers, c='red', marker='x', s=150, \n",
    "                   linewidth=3, label=f\"{row['method']} Detection\")\n",
    "        # Add method label\n",
    "        for i, d in enumerate(detections):\n",
    "            if d < len(passengers):\n",
    "                ax3.annotate(row['method'], (d, passengers[d]), \n",
    "                           xytext=(5, 10), textcoords='offset points', fontsize=9,\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "ax3.set_xlabel('Time (months)')\n",
    "ax3.set_ylabel('Number of Passengers')\n",
    "ax3.set_title('Airline Passengers: Detection Points')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 2: Elec2 Sample Data with Potential Drift Regions\n",
    "time_elec = range(len(df_elec2_sample))\n",
    "ax4.plot(time_elec, df_elec2_sample['nswprice'], 'b-', linewidth=1, alpha=0.8, label='NSW Price')\n",
    "ax4.plot(time_elec, df_elec2_sample['vicprice'], 'r-', linewidth=1, alpha=0.8, label='VIC Price')\n",
    "\n",
    "# Highlight regions where prices diverge significantly (potential drift indicators)\n",
    "price_diff = abs(df_elec2_sample['nswprice'] - df_elec2_sample['vicprice'])\n",
    "high_divergence = price_diff > price_diff.quantile(0.95)\n",
    "divergence_points = np.where(high_divergence)[0]\n",
    "\n",
    "if len(divergence_points) > 0:\n",
    "    ax4.scatter(divergence_points, df_elec2_sample['nswprice'].iloc[divergence_points], \n",
    "               c='orange', marker='o', s=30, alpha=0.7, label='High Price Divergence')\n",
    "\n",
    "ax4.set_xlabel('Time (instances)')\n",
    "ax4.set_ylabel('Electricity Price')\n",
    "ax4.set_title('Elec2 Sample: Price Patterns and Potential Drift Regions')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nReal-world Data (Airline Passengers):\")\n",
    "for _, row in realworld_df.iterrows():\n",
    "    detections = row['detections']\n",
    "    if len(detections) > 0:\n",
    "        detection_str = f\"[{', '.join(map(str, detections))}]\"\n",
    "        print(f\"  {row['method']:<10}: {detection_str}\")\n",
    "    else:\n",
    "        print(f\"  {row['method']:<10}: No detections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff167161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elec2_benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elec2_benchmark.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1eb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_results = run_realworld_experiment(df_elec2_benchmark, dataset_name)\n",
    "all_realworld_results.extend(dataset_results)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "realworld_df = pd.DataFrame(all_realworld_results)\n",
    "\n",
    "print(f\"\\nReal-World Evaluation Summary:\")\n",
    "print(f\"   Total evaluations: {len(all_realworld_results)}\")\n",
    "print(f\"   Datasets: {realworld_df['dataset'].unique().tolist()}\")\n",
    "print(f\"   Methods: {realworld_df['method'].unique().tolist()}\")\n",
    "\n",
    "# Add detection count column for consistency with synthetic results\n",
    "realworld_df['detection_count'] = realworld_df.apply(\n",
    "    lambda row: row.get('total_detections', len(row.get('detections', []))), axis=1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
