{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import psutil\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Statistical and ML utilities\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Online learning framework (River)\n",
    "from river import preprocessing, linear_model, optim, metrics, compose\n",
    "from river.datasets import synth\n",
    "from river.drift import ADWIN\n",
    "from river.drift.binary import DDM, EDDM, FHDDM, HDDM_A, HDDM_W\n",
    "\n",
    "# Drift detection and adaptation modules\n",
    "sys.path.insert(0, os.path.abspath('../backup'))\n",
    "from shape_dd import shape, shape_adaptive\n",
    "from d3 import d3\n",
    "from dawidd import dawidd\n",
    "from gen_data import gen_random\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../drift-monitoring'))\n",
    "from drift_type_classifier import classify_drift_at_detection, DriftTypeConfig\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONFIGURATION SUMMARY\n",
      "================================================================================\n",
      "Stream size: 10000 samples\n",
      "Drift position: 5000\n",
      "Initial training: 500 samples\n",
      "Detection parameters: Buffer=1000, Chunk=150\n",
      "Adaptation delay: 50 samples\n",
      "Adaptation window: 800 samples\n",
      "Window-based detectors: 4\n",
      "Streaming detectors: 6\n",
      "Total detectors: 10\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Data stream configuration\n",
    "STREAM_SIZE = 10000\n",
    "DRIFT_POSITION = 5000\n",
    "RANDOM_SEED = random.randint(0, 1000000)\n",
    "\n",
    "# Model training configuration\n",
    "INITIAL_TRAINING_SIZE = 500\n",
    "TRAINING_WARMUP = 100\n",
    "\n",
    "# Adaptation parameters\n",
    "ADAPTATION_DELAY = 50\n",
    "ADAPTATION_WINDOW = 800\n",
    "PREQUENTIAL_WINDOW = 100\n",
    "\n",
    "# Detection parameters (top-level for easy access)\n",
    "BUFFER_SIZE = 1000      # ShapeDD rolling buffer size\n",
    "CHUNK_SIZE = 150        # Detection frequency (samples between checks)\n",
    "SHAPE_L1 = 50          # ShapeDD reference window\n",
    "SHAPE_L2 = 150         # ShapeDD test window\n",
    "SHAPE_N_PERM = 2500    # ShapeDD permutation count\n",
    "\n",
    "# Detector-specific parameters (detailed configuration)\n",
    "DETECTOR_CONFIG = {\n",
    "    'window_based': {\n",
    "        'chunk_size': CHUNK_SIZE,\n",
    "        'overlap': 100,\n",
    "        'cooldown': 75,\n",
    "        'd3_threshold': 0.5,\n",
    "        'dawidd_alpha': 0.05,\n",
    "        'shape_alpha': 0.05,\n",
    "        'shape_L1': SHAPE_L1,\n",
    "        'shape_L2': SHAPE_L2,\n",
    "        'shape_n_perm': SHAPE_N_PERM,\n",
    "        'shapedd_batch_size': BUFFER_SIZE\n",
    "    },\n",
    "    'streaming': {\n",
    "        'detection_cooldown': 50,\n",
    "        'warm_start_window': 200,\n",
    "        'accuracy_window_size': 50\n",
    "    }\n",
    "}\n",
    "\n",
    "# Drift type classification config\n",
    "DRIFT_TYPE_CONFIG = DriftTypeConfig(\n",
    "    w_ref=250,\n",
    "    w_basic=100,\n",
    "    sudden_len_thresh=250\n",
    ")\n",
    "\n",
    "# Methods to evaluate\n",
    "WINDOW_METHODS = ['D3', 'DAWIDD', 'ShapeDD', 'ShapeDD_Improved']\n",
    "STREAMING_METHODS = ['ADWIN', 'DDM', 'EDDM', 'HDDM_A', 'HDDM_W', 'FHDDM']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Stream size: {STREAM_SIZE} samples\")\n",
    "print(f\"Drift position: {DRIFT_POSITION}\")\n",
    "print(f\"Initial training: {INITIAL_TRAINING_SIZE} samples\")\n",
    "print(f\"Detection parameters: Buffer={BUFFER_SIZE}, Chunk={CHUNK_SIZE}\")\n",
    "print(f\"Adaptation delay: {ADAPTATION_DELAY} samples\")\n",
    "print(f\"Adaptation window: {ADAPTATION_WINDOW} samples\")\n",
    "print(f\"Window-based detectors: {len(WINDOW_METHODS)}\")\n",
    "print(f\"Streaming detectors: {len(STREAMING_METHODS)}\")\n",
    "print(f\"Total detectors: {len(WINDOW_METHODS) + len(STREAMING_METHODS)}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Flexible dataset generation system defined\n",
      "  Available datasets: ['standard_sea', 'enhanced_sea', 'stagger', 'hyperplane', 'gen_random']\n",
      "  Enabled datasets: ['standard_sea', 'enhanced_sea', 'stagger', 'hyperplane', 'gen_random']\n",
      "\n",
      "Drift events per dataset:\n",
      "  â€¢ standard_sea: 2 drift events\n",
      "  â€¢ enhanced_sea: 2 drift events\n",
      "  â€¢ stagger: 1 drift events\n",
      "  â€¢ hyperplane: 2 drift events\n",
      "  â€¢ gen_random: 3 drift events\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: FLEXIBLE DATASET GENERATION - Multiple Benchmark Options with Multi-Drift Support\n",
    "# ============================================================================\n",
    "\n",
    "def generate_standard_sea_stream_multi(total_size, drift_positions, seed=42):\n",
    "    \"\"\"\n",
    "    Standard SEA benchmark with multiple drifts.\n",
    "    Creates sudden drifts by switching between SEA variants.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    variants = [0, 1, 2, 3]  # SEA has 4 variants\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "    \n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "        variant = variants[seg_idx % len(variants)]\n",
    "        \n",
    "        stream = synth.SEA(seed=seed + seg_idx * 100, variant=variant)\n",
    "        for i, (x, y) in enumerate(stream.take(size)):\n",
    "            X_list.append(list(x.values()))\n",
    "            y_list.append(y)\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f\"  âœ“ Standard SEA: {X.shape[0]} samples, {X.shape[1]} features, {len(drift_positions)} drifts\")\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_enhanced_sea_stream_multi(total_size, drift_positions, seed=42,\n",
    "                                        scale_factors=(1.8, 1.5, 2.0),\n",
    "                                        shift_amounts=(5.0, 4.0, 8.0)):\n",
    "    \"\"\"Enhanced SEA with multiple drifts and transformations.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    variants = [0, 1, 2, 3]\n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "    \n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "        variant = variants[seg_idx % len(variants)]\n",
    "        \n",
    "        stream = synth.SEA(seed=seed + seg_idx * 100, variant=variant)\n",
    "        \n",
    "        for i, (x, y) in enumerate(stream.take(size)):\n",
    "            x_vals = list(x.values())\n",
    "            \n",
    "            if seg_idx % 2 == 1:\n",
    "                x_vals = [x_vals[j] * scale_factors[j] + shift_amounts[j] \n",
    "                         for j in range(len(x_vals))]\n",
    "            \n",
    "            X_list.append(x_vals)\n",
    "            y_list.append(y)\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f\"  âœ“ Enhanced SEA: {X.shape[0]} samples, {X.shape[1]} features, {len(drift_positions)} drifts\")\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_stagger_stream_multi(total_size, drift_positions, seed=42):\n",
    "    \"\"\"STAGGER concepts with multiple sudden drifts.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "    X_segments, y_segments = [], []\n",
    "    \n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "        \n",
    "        X_seg = np.random.randn(size, 5)\n",
    "        \n",
    "        # Different concepts for each segment\n",
    "        if seg_idx % 3 == 0:\n",
    "            X_seg[:, 0] += 2.0\n",
    "            y_seg = (X_seg[:, 0] + X_seg[:, 1] > 1.5).astype(int)\n",
    "        elif seg_idx % 3 == 1:\n",
    "            X_seg[:, 0] -= 2.0\n",
    "            y_seg = (X_seg[:, 0] * X_seg[:, 1] > 0).astype(int)\n",
    "        else:\n",
    "            X_seg[:, 1] += 1.5\n",
    "            y_seg = (X_seg[:, 1] + X_seg[:, 2] > 0.5).astype(int)\n",
    "        \n",
    "        X_segments.append(X_seg)\n",
    "        y_segments.append(y_seg)\n",
    "    \n",
    "    X = np.vstack(X_segments)\n",
    "    y = np.hstack(y_segments)\n",
    "    \n",
    "    print(f\"  âœ“ STAGGER: {X.shape[0]} samples, {X.shape[1]} features, {len(drift_positions)} drifts\")\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_hyperplane_stream_multi(total_size, drift_positions, seed=42, n_features=10):\n",
    "    \"\"\"Rotating Hyperplane with multiple drifts.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    segments = [0] + drift_positions + [total_size]\n",
    "    mag_changes = [0.0001, 0.01, 0.005, 0.015]  # Alternate speeds\n",
    "    \n",
    "    for seg_idx in range(len(segments) - 1):\n",
    "        start, end = segments[seg_idx], segments[seg_idx + 1]\n",
    "        size = end - start\n",
    "        mag_change = mag_changes[seg_idx % len(mag_changes)]\n",
    "        \n",
    "        stream = synth.Hyperplane(seed=seed + seg_idx * 100, n_features=n_features,\n",
    "                                   n_drift_features=2, mag_change=mag_change,\n",
    "                                   noise_percentage=0.05)\n",
    "        \n",
    "        for i, (x, y) in enumerate(stream.take(size)):\n",
    "            X_list.append(list(x.values()))\n",
    "            y_list.append(y)\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    print(f\"  âœ“ Hyperplane: {X.shape[0]} samples, {n_features} features, {len(drift_positions)} drifts\")\n",
    "    return X, y, drift_positions\n",
    "\n",
    "\n",
    "def generate_genrandom_stream_multi(total_size, n_drifts, seed=42,\n",
    "                                     dims=5, intens=0.125, dist=\"unif\", alt=False):\n",
    "    \"\"\"Custom synthetic data using gen_random with multiple drifts.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X, y_drift_labels = gen_random(\n",
    "        number=n_drifts,\n",
    "        dims=dims,\n",
    "        intens=intens,\n",
    "        dist=dist,\n",
    "        alt=alt,\n",
    "        length=total_size,\n",
    "        min_dist=10,\n",
    "        min_dist_border=100\n",
    "    )\n",
    "    \n",
    "    # Find actual drift positions\n",
    "    drift_indices = np.where(np.diff(y_drift_labels) != 0)[0] + 1\n",
    "    drift_positions = drift_indices.tolist()\n",
    "    \n",
    "    # Create binary labels with class balance check\n",
    "    feature_sum = X[:, 0] + X[:, 1]\n",
    "    threshold = np.percentile(feature_sum, 50)\n",
    "    y_class = (feature_sum > threshold).astype(int)\n",
    "    \n",
    "    # Check class balance and adjust if needed\n",
    "    unique_classes, class_counts = np.unique(y_class, return_counts=True)\n",
    "    \n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"  âš  Single class detected, using balanced fallback\")\n",
    "        y_class = np.zeros(len(X), dtype=int)\n",
    "        sorted_indices = np.argsort(feature_sum)\n",
    "        y_class[sorted_indices[::2]] = 1\n",
    "    else:\n",
    "        min_class_ratio = min(class_counts) / len(y_class)\n",
    "        if min_class_ratio < 0.1:\n",
    "            print(f\"  âš  Class imbalance detected ({min_class_ratio:.1%}), rebalancing\")\n",
    "            threshold = np.percentile(feature_sum, 40)\n",
    "            y_class = (feature_sum > threshold).astype(int)\n",
    "    \n",
    "    unique_classes_final, class_counts_final = np.unique(y_class, return_counts=True)\n",
    "    class_dist = {int(c): int(cnt) for c, cnt in zip(unique_classes_final, class_counts_final)}\n",
    "    \n",
    "    print(f\"  âœ“ gen_random: {X.shape[0]} samples, {dims} features, {len(drift_positions)} drifts\")\n",
    "    print(f\"    Class distribution: {class_dist}\")\n",
    "    \n",
    "    return X, y_class, drift_positions\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UNIFIED DATASET GENERATOR - Select Your Dataset Type\n",
    "# ============================================================================\n",
    "\n",
    "def generate_drift_stream(dataset_type=\"standard_sea\", total_size=10000, n_drift_events=1,\n",
    "                          seed=42, **kwargs):\n",
    "    \"\"\"\n",
    "    Unified dataset generator with multiple benchmark options and multi-drift support.\n",
    "    \n",
    "    Args:\n",
    "        dataset_type: Type of dataset to generate\n",
    "        total_size: Total number of samples\n",
    "        n_drift_events: Number of drift events to generate\n",
    "        seed: Random seed\n",
    "        **kwargs: Additional parameters for specific dataset types\n",
    "    \n",
    "    Returns:\n",
    "        X, y, drift_positions, dataset_info\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Generating {dataset_type} with {n_drift_events} drift events...\")\n",
    "    \n",
    "    # Calculate drift positions (evenly spaced)\n",
    "    if n_drift_events <= 0:\n",
    "        n_drift_events = 1\n",
    "    \n",
    "    segment_size = total_size // (n_drift_events + 1)\n",
    "    drift_positions = [(i + 1) * segment_size for i in range(n_drift_events)]\n",
    "    \n",
    "    print(f\"  Drift positions: {drift_positions}\")\n",
    "    \n",
    "    if dataset_type == \"standard_sea\":\n",
    "        X, y, drift_pos = generate_standard_sea_stream_multi(total_size, drift_positions, seed)\n",
    "        info = {'name': 'Standard SEA', 'academic_status': 'âœ… Standard', 'features': 3}\n",
    "    \n",
    "    elif dataset_type == \"enhanced_sea\":\n",
    "        scale_factors = kwargs.get('scale_factors', (1.8, 1.5, 2.0))\n",
    "        shift_amounts = kwargs.get('shift_amounts', (5.0, 4.0, 8.0))\n",
    "        X, y, drift_pos = generate_enhanced_sea_stream_multi(total_size, drift_positions, seed,\n",
    "                                                              scale_factors, shift_amounts)\n",
    "        info = {'name': 'Enhanced SEA', 'academic_status': 'âš ï¸ Non-standard', 'features': 3}\n",
    "    \n",
    "    elif dataset_type == \"stagger\":\n",
    "        X, y, drift_pos = generate_stagger_stream_multi(total_size, drift_positions, seed)\n",
    "        info = {'name': 'STAGGER', 'academic_status': 'âœ… Standard', 'features': 5}\n",
    "    \n",
    "    elif dataset_type == \"hyperplane\":\n",
    "        n_features = kwargs.get('n_features', 10)\n",
    "        X, y, drift_pos = generate_hyperplane_stream_multi(total_size, drift_positions, seed, n_features)\n",
    "        info = {'name': 'Hyperplane', 'academic_status': 'âœ… Standard', 'features': n_features}\n",
    "    \n",
    "    elif dataset_type == \"gen_random\":\n",
    "        dims = kwargs.get('dims', 5)\n",
    "        intens = kwargs.get('intens', 0.125)\n",
    "        dist = kwargs.get('dist', 'unif')\n",
    "        alt = kwargs.get('alt', False)\n",
    "        X, y, drift_pos = generate_genrandom_stream_multi(total_size, n_drift_events, seed,\n",
    "                                                          dims, intens, dist, alt)\n",
    "        info = {'name': 'gen_random', 'academic_status': 'ðŸ”§ Custom', 'features': dims}\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset_type: {dataset_type}\")\n",
    "    \n",
    "    info['type'] = dataset_type\n",
    "    info['drift_positions'] = drift_pos\n",
    "    info['n_drift_events'] = len(drift_pos)\n",
    "    info['total_samples'] = len(X)\n",
    "    \n",
    "    return X, y, drift_pos, info\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CATALOG - Configure All Datasets to Benchmark\n",
    "# ============================================================================\n",
    "\n",
    "DATASET_CATALOG = {\n",
    "    'standard_sea': {\n",
    "        'enabled': True,\n",
    "        'n_drift_events': 2,\n",
    "        'params': {}\n",
    "    },\n",
    "    'enhanced_sea': {\n",
    "        'enabled': True,\n",
    "        'n_drift_events': 2,\n",
    "        'params': {\n",
    "            'scale_factors': (1.8, 1.5, 2.0),\n",
    "            'shift_amounts': (5.0, 4.0, 8.0)\n",
    "        }\n",
    "    },\n",
    "    'stagger': {\n",
    "        'enabled': True,\n",
    "        'n_drift_events': 1,\n",
    "        'params': {}\n",
    "    },\n",
    "    'hyperplane': {\n",
    "        'enabled': True,\n",
    "        'n_drift_events': 2,\n",
    "        'params': {\n",
    "            'n_features': 10\n",
    "        }\n",
    "    },\n",
    "    'gen_random': {\n",
    "        'enabled': True,\n",
    "        'n_drift_events': 3,\n",
    "        'params': {\n",
    "            'number': 3,\n",
    "            'dims': 5,\n",
    "            'intens': 0.125,\n",
    "            'dist': 'unif',\n",
    "            'alt': False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ“ Flexible dataset generation system defined\")\n",
    "print(f\"  Available datasets: {list(DATASET_CATALOG.keys())}\")\n",
    "print(f\"  Enabled datasets: {[k for k, v in DATASET_CATALOG.items() if v['enabled']]}\")\n",
    "print(\"\\nDrift events per dataset:\")\n",
    "for key, cfg in DATASET_CATALOG.items():\n",
    "    if cfg['enabled']:\n",
    "        print(f\"  â€¢ {key}: {cfg['n_drift_events']} drift events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_sklearn_model():\n",
    "    \"\"\"Create a fresh scikit-learn model for batch training.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000, random_state=RANDOM_SEED))\n",
    "    ])\n",
    "\n",
    "def create_river_model():\n",
    "    \"\"\"Create a fresh River model for streaming.\"\"\"\n",
    "    return compose.Pipeline(\n",
    "        preprocessing.StandardScaler(),\n",
    "        linear_model.LogisticRegression()\n",
    "    )\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor memory usage.\"\"\"\n",
    "    process = psutil.Process()\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return memory_mb\n",
    "\n",
    "def calculate_mttd_metrics(detections, true_drift, acceptable_delta=100):\n",
    "    \"\"\"Calculate detection performance metrics.\"\"\"\n",
    "    detections = sorted([int(d) for d in detections])\n",
    "    \n",
    "    if true_drift is None:\n",
    "        return {\n",
    "            'tp': 0, 'fp': len(detections), 'fn': 0,\n",
    "            'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0,\n",
    "            'mttd': float('inf'), 'detection_delay': None\n",
    "        }\n",
    "    \n",
    "    # Check if drift was detected within acceptable window\n",
    "    valid_detections = [d for d in detections \n",
    "                        if abs(d - true_drift) <= acceptable_delta]\n",
    "    \n",
    "    if valid_detections:\n",
    "        # True positive: detected within window\n",
    "        closest_detection = min(valid_detections, key=lambda x: abs(x - true_drift))\n",
    "        tp = 1\n",
    "        fp = len(detections) - 1  # Other detections are false positives\n",
    "        fn = 0\n",
    "        detection_delay = closest_detection - true_drift\n",
    "        mttd = abs(detection_delay)\n",
    "    else:\n",
    "        # False negative: no detection in window\n",
    "        tp = 0\n",
    "        fp = len(detections)\n",
    "        fn = 1\n",
    "        detection_delay = None\n",
    "        mttd = float('inf')\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'tp': tp, 'fp': fp, 'fn': fn,\n",
    "        'precision': precision, 'recall': recall, 'f1_score': f1_score,\n",
    "        'mttd': mttd, 'detection_delay': detection_delay\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Utility functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Window-based evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: WINDOW-BASED DETECTOR EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_window_detector_with_adaptation(method_name, X, y, true_drift):\n",
    "    \"\"\"\n",
    "    Evaluate window-based detector with full adaptation lifecycle.\n",
    "    \n",
    "    Returns comprehensive metrics including detection and model performance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING: {method_name} (Window-based)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    cfg = DETECTOR_CONFIG['window_based']\n",
    "    start_time = time.time()\n",
    "    start_mem = monitor_resources()\n",
    "    \n",
    "    # Phase 1: Initial model training\n",
    "    model = create_sklearn_model()\n",
    "    training_end = INITIAL_TRAINING_SIZE\n",
    "    model.fit(X[:training_end], y[:training_end])\n",
    "    \n",
    "    # Warmup evaluation\n",
    "    warmup_end = training_end + TRAINING_WARMUP\n",
    "    warmup_correct = []\n",
    "    for idx in range(training_end, warmup_end):\n",
    "        y_pred = model.predict(X[idx].reshape(1, -1))[0]\n",
    "        warmup_correct.append(int(y_pred == y[idx]))\n",
    "    baseline_accuracy = np.mean(warmup_correct)\n",
    "    \n",
    "    print(f\"  Initial training: {INITIAL_TRAINING_SIZE} samples\")\n",
    "    print(f\"  Baseline accuracy: {baseline_accuracy:.4f}\")\n",
    "    \n",
    "    # Phase 2: Detection setup\n",
    "    chunk_size = cfg['chunk_size']\n",
    "    overlap = cfg['overlap']\n",
    "    shift = chunk_size - overlap\n",
    "    buffer_size = cfg['shapedd_batch_size']  # Rolling buffer size (1000 samples)\n",
    "    \n",
    "    # Tracking structures\n",
    "    accuracy_tracker = []\n",
    "    recent_correct = deque(maxlen=PREQUENTIAL_WINDOW)\n",
    "    detections = []\n",
    "    last_detection = -10**9\n",
    "    \n",
    "    # Rolling buffer for drift detection (NO PRE-COMPUTATION!)\n",
    "    detection_buffer = deque(maxlen=buffer_size)\n",
    "    \n",
    "    drift_detected = False\n",
    "    drift_detected_at = None\n",
    "    drift_type = None\n",
    "    planned_adaptation_idx = None\n",
    "    adaptation_applied = False\n",
    "    adaptation_idx = None\n",
    "    \n",
    "    print(f\"  Detection buffer size: {buffer_size} samples\")\n",
    "    print(f\"  Drift check frequency: every {chunk_size} samples\")\n",
    "    \n",
    "    # Phase 3: Streaming evaluation with detection\n",
    "    print(f\"  Processing stream...\")\n",
    "    for idx in range(warmup_end, len(X)):\n",
    "        # Prediction\n",
    "        y_pred = model.predict(X[idx].reshape(1, -1))[0]\n",
    "        is_correct = (y_pred == y[idx])\n",
    "        recent_correct.append(is_correct)\n",
    "        accuracy = np.mean(recent_correct) if len(recent_correct) > 0 else 0.0\n",
    "        accuracy_tracker.append({'idx': idx, 'accuracy': accuracy})\n",
    "        \n",
    "        # Check for planned adaptation\n",
    "        if planned_adaptation_idx is not None and idx >= planned_adaptation_idx:\n",
    "            print(f\"  [Sample {idx}] Applying adaptation - retraining model\")\n",
    "            \n",
    "            # Get post-drift data for retraining\n",
    "            adapt_start = max(drift_detected_at, idx - ADAPTATION_WINDOW)\n",
    "            adapt_X = X[adapt_start:idx]\n",
    "            adapt_y = y[adapt_start:idx]\n",
    "            \n",
    "            # Full model reset\n",
    "            model = create_sklearn_model()\n",
    "            model.fit(adapt_X, adapt_y)\n",
    "            \n",
    "            adaptation_applied = True\n",
    "            adaptation_idx = idx\n",
    "            planned_adaptation_idx = None\n",
    "            print(f\"  Model retrained on {len(adapt_X)} samples\")\n",
    "        \n",
    "        # Drift detection (continues throughout the stream)\n",
    "        # Add current sample to rolling buffer\n",
    "        detection_buffer.append({'idx': idx, 'x': X[idx]})\n",
    "        \n",
    "        # Periodic drift detection when buffer is full\n",
    "        if len(detection_buffer) >= buffer_size and idx % chunk_size == 0:\n",
    "            # Extract current buffer state\n",
    "            buffer_list = list(detection_buffer)\n",
    "            buffer_X = np.array([item['x'] for item in buffer_list])\n",
    "            buffer_indices = np.array([item['idx'] for item in buffer_list])\n",
    "            \n",
    "            try:\n",
    "                # Method-specific detection\n",
    "                if 'ShapeDD' in method_name:\n",
    "                    # ShapeDD: Run on entire rolling buffer\n",
    "                    if method_name == 'ShapeDD':\n",
    "                        shp_results = shape(buffer_X, cfg['shape_L1'], cfg['shape_L2'], cfg['shape_n_perm'])\n",
    "                    else:  # ShapeDD_Improved\n",
    "                        L1 = int(0.05 * len(buffer_X))  # 5% of the stream\n",
    "                        L2 = 2 * L1              # double the size of the first window\n",
    "                        shp_results = shape_adaptive(buffer_X, L1, L2, cfg['shape_n_perm'], sensitivity='none')\n",
    "                    \n",
    "                    # Check recent chunk within buffer for drift\n",
    "                    chunk_start = max(0, len(buffer_X) - chunk_size)\n",
    "                    chunk_pvals = shp_results[chunk_start:, 2]\n",
    "                    pval_min = chunk_pvals.min()\n",
    "                    trigger = pval_min < cfg['shape_alpha']\n",
    "                    \n",
    "                    if trigger:\n",
    "                        # Find drift position within buffer\n",
    "                        det_pos_in_chunk = int(np.argmin(chunk_pvals))\n",
    "                        det_pos = int(buffer_indices[chunk_start + det_pos_in_chunk])\n",
    "                        \n",
    "                        if det_pos - last_detection >= cfg['cooldown']:\n",
    "                            detections.append(det_pos)\n",
    "                            last_detection = det_pos\n",
    "                            \n",
    "                            # Only trigger adaptation for first detection\n",
    "                            if not drift_detected:\n",
    "                                drift_detected = True\n",
    "                                drift_detected_at = det_pos\n",
    "                                \n",
    "                                # Classify drift type using buffer context\n",
    "                                drift_pos_in_buffer = np.where(buffer_indices == drift_detected_at)[0]\n",
    "                                if len(drift_pos_in_buffer) > 0:\n",
    "                                    drift_idx_in_buffer = int(drift_pos_in_buffer[0])\n",
    "                                    drift_result = classify_drift_at_detection(buffer_X, drift_idx_in_buffer, DRIFT_TYPE_CONFIG)\n",
    "                                    drift_type = drift_result['subcategory']\n",
    "                                else:\n",
    "                                    # Fallback if position not found in buffer\n",
    "                                    drift_result = classify_drift_at_detection(X, drift_detected_at, DRIFT_TYPE_CONFIG)\n",
    "                                    drift_type = drift_result['subcategory']\n",
    "                                \n",
    "                                # Schedule adaptation\n",
    "                                planned_adaptation_idx = idx + ADAPTATION_DELAY\n",
    "                                \n",
    "                                print(f\"  [Sample {idx}] DRIFT DETECTED at {drift_detected_at}\")\n",
    "                                print(f\"    Drift type: {drift_type}\")\n",
    "                                print(f\"    p-value: {pval_min:.6f}\")\n",
    "                                print(f\"    Scheduling adaptation at sample {planned_adaptation_idx}\")\n",
    "                            else:\n",
    "                                print(f\"  [Sample {idx}] Additional drift detected at {det_pos} (p-value: {pval_min:.6f})\")\n",
    "                \n",
    "                elif method_name in ['D3', 'DAWIDD']:\n",
    "                    # D3/DAWIDD: Use sliding window approach\n",
    "                    # Extract recent window from buffer\n",
    "                    window_start = max(0, len(buffer_X) - chunk_size)\n",
    "                    window_X = buffer_X[window_start:]\n",
    "                    \n",
    "                    if method_name == 'D3':\n",
    "                        score = d3(window_X)\n",
    "                        trigger = score > cfg['d3_threshold']\n",
    "                        det_pos = int(buffer_indices[-1])\n",
    "                        \n",
    "                    elif method_name == 'DAWIDD':\n",
    "                        _, pval = dawidd(window_X, 'rbf')\n",
    "                        trigger = pval < cfg['dawidd_alpha']\n",
    "                        det_pos = int(buffer_indices[-1])\n",
    "                    else:\n",
    "                        trigger = False\n",
    "                        det_pos = idx\n",
    "                    \n",
    "                    if trigger and (det_pos - last_detection >= cfg['cooldown']):\n",
    "                        detections.append(det_pos)\n",
    "                        last_detection = det_pos\n",
    "                        \n",
    "                        # Only trigger adaptation for first detection\n",
    "                        if not drift_detected:\n",
    "                            drift_detected = True\n",
    "                            drift_detected_at = det_pos\n",
    "                            \n",
    "                            # Classify drift type\n",
    "                            drift_result = classify_drift_at_detection(X, drift_detected_at, DRIFT_TYPE_CONFIG)\n",
    "                            drift_type = drift_result['subcategory']\n",
    "                            \n",
    "                            # Schedule adaptation\n",
    "                            planned_adaptation_idx = idx + ADAPTATION_DELAY\n",
    "                            \n",
    "                            print(f\"  [Sample {idx}] DRIFT DETECTED at {drift_detected_at}\")\n",
    "                            print(f\"    Drift type: {drift_type}\")\n",
    "                            print(f\"    Scheduling adaptation at sample {planned_adaptation_idx}\")\n",
    "                        else:\n",
    "                            print(f\"  [Sample {idx}] Additional drift detected at {det_pos}\")\n",
    "                            \n",
    "            except Exception as e:\n",
    "                pass  # Skip detection errors\n",
    "    \n",
    "    # Phase 4: Calculate metrics\n",
    "    end_time = time.time()\n",
    "    end_mem = monitor_resources()\n",
    "    \n",
    "    # Detection metrics\n",
    "    detection_metrics = calculate_mttd_metrics(detections, true_drift)\n",
    "    \n",
    "    # Performance metrics\n",
    "    acc_indices = np.array([item['idx'] for item in accuracy_tracker])\n",
    "    acc_values = np.array([item['accuracy'] for item in accuracy_tracker])\n",
    "    \n",
    "    if drift_detected and adaptation_applied:\n",
    "        pre_mask = acc_indices < drift_detected_at\n",
    "        deg_mask = (acc_indices >= drift_detected_at) & (acc_indices < adaptation_idx)\n",
    "        rec_mask = (acc_indices >= adaptation_idx) & (acc_indices < min(adaptation_idx + 300, len(X)))\n",
    "        \n",
    "        pre_acc = np.mean(acc_values[pre_mask]) if np.any(pre_mask) else baseline_accuracy\n",
    "        deg_min = np.min(acc_values[deg_mask]) if np.any(deg_mask) else 0.0\n",
    "        rec_acc = np.mean(acc_values[rec_mask]) if np.any(rec_mask) else 0.0\n",
    "        \n",
    "        accuracy_drop = pre_acc - deg_min\n",
    "        recovery_rate = (rec_acc - deg_min) / accuracy_drop * 100 if accuracy_drop > 0 else 0.0\n",
    "        \n",
    "        # Recovery time\n",
    "        target_acc = pre_acc * 0.95\n",
    "        recovery_time = None\n",
    "        for i, (idx_val, acc_val) in enumerate(zip(acc_indices[rec_mask], acc_values[rec_mask])):\n",
    "            if acc_val >= target_acc:\n",
    "                recovery_time = idx_val - adaptation_idx\n",
    "                break\n",
    "        if recovery_time is None:\n",
    "            recovery_time = len(acc_values[rec_mask])\n",
    "    else:\n",
    "        pre_acc = baseline_accuracy\n",
    "        deg_min = np.min(acc_values) if len(acc_values) > 0 else 0.0\n",
    "        rec_acc = 0.0\n",
    "        accuracy_drop = 0.0\n",
    "        recovery_rate = 0.0\n",
    "        recovery_time = 0\n",
    "    \n",
    "    print(f\"  âœ“ Evaluation complete ({end_time - start_time:.2f}s)\")\n",
    "    print(f\"    Detection: {detection_metrics['tp']} TP, {detection_metrics['fp']} FP, {detection_metrics['fn']} FN\")\n",
    "    print(f\"    F1-score: {detection_metrics['f1_score']:.3f}, MTTD: {detection_metrics['mttd']}\")\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'paradigm': 'window',\n",
    "        'detections': detections,\n",
    "        'drift_detected': drift_detected,\n",
    "        'drift_detected_at': drift_detected_at,\n",
    "        'drift_type': drift_type,\n",
    "        'adaptation_applied': adaptation_applied,\n",
    "        'adaptation_idx': adaptation_idx,\n",
    "        'baseline_accuracy': pre_acc,\n",
    "        'min_accuracy': deg_min,\n",
    "        'recovery_accuracy': rec_acc,\n",
    "        'accuracy_drop': accuracy_drop,\n",
    "        'recovery_rate': recovery_rate,\n",
    "        'recovery_time': recovery_time,\n",
    "        'runtime_total_s': end_time - start_time,\n",
    "        'runtime_per_instance_ms': (end_time - start_time) * 1000 / len(X),\n",
    "        'memory_mb': max(0.0, end_mem - start_mem),\n",
    "        'accuracy_timeline': accuracy_tracker,\n",
    "        **detection_metrics\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Window-based evaluation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Streaming evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: STREAMING DETECTOR EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_streaming_detector_with_adaptation(method_name, X, y, true_drift):\n",
    "    \"\"\"\n",
    "    Evaluate streaming detector with full adaptation lifecycle.\n",
    "    \n",
    "    Uses River's online learning framework with frozen model approach.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING: {method_name} (Streaming)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    cfg = DETECTOR_CONFIG['streaming']\n",
    "    start_time = time.time()\n",
    "    start_mem = monitor_resources()\n",
    "    \n",
    "    # Create drift detector\n",
    "    if method_name == 'ADWIN':\n",
    "        detector = ADWIN(delta=0.002)\n",
    "    elif method_name == 'DDM':\n",
    "        detector = DDM()\n",
    "    elif method_name == 'EDDM':\n",
    "        detector = EDDM(alpha=0.95, beta=0.9)\n",
    "    elif method_name == 'HDDM_A':\n",
    "        detector = HDDM_A()\n",
    "    elif method_name == 'HDDM_W':\n",
    "        detector = HDDM_W()\n",
    "    elif method_name == 'FHDDM':\n",
    "        detector = FHDDM(short_window_size=20)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown streaming detector: {method_name}\")\n",
    "    \n",
    "    # Phase 1: Initial batch training (using sklearn for consistency)\n",
    "    sklearn_model = create_sklearn_model()\n",
    "    training_end = INITIAL_TRAINING_SIZE\n",
    "    sklearn_model.fit(X[:training_end], y[:training_end])\n",
    "    \n",
    "    # Warmup evaluation\n",
    "    warmup_end = training_end + TRAINING_WARMUP\n",
    "    warmup_correct = []\n",
    "    for idx in range(training_end, warmup_end):\n",
    "        y_pred = sklearn_model.predict(X[idx].reshape(1, -1))[0]\n",
    "        warmup_correct.append(int(y_pred == y[idx]))\n",
    "    baseline_accuracy = np.mean(warmup_correct)\n",
    "    \n",
    "    print(f\"  Initial training: {INITIAL_TRAINING_SIZE} samples\")\n",
    "    print(f\"  Baseline accuracy: {baseline_accuracy:.4f}\")\n",
    "    \n",
    "    # Tracking structures\n",
    "    accuracy_tracker = []\n",
    "    recent_correct = deque(maxlen=PREQUENTIAL_WINDOW)\n",
    "    detections = []\n",
    "    last_detection = -10**9\n",
    "    accuracy_buffer = deque(maxlen=cfg['accuracy_window_size'])\n",
    "    \n",
    "    drift_detected = False\n",
    "    drift_detected_at = None\n",
    "    drift_type = None\n",
    "    planned_adaptation_idx = None\n",
    "    adaptation_applied = False\n",
    "    adaptation_idx = None\n",
    "    \n",
    "    # Determine signal type (continuous vs binary)\n",
    "    continuous_detectors = {'ADWIN', 'HDDM_A'}\n",
    "    signal_type = 'continuous' if method_name in continuous_detectors else 'binary'\n",
    "    \n",
    "    # Phase 3: Streaming evaluation with detection\n",
    "    print(f\"  Processing stream (signal type: {signal_type})...\")\n",
    "    for idx in range(warmup_end, len(X)):\n",
    "        # Prediction\n",
    "        y_pred = sklearn_model.predict(X[idx].reshape(1, -1))[0]\n",
    "        is_correct = (y_pred == y[idx])\n",
    "        recent_correct.append(is_correct)\n",
    "        accuracy = np.mean(recent_correct) if len(recent_correct) > 0 else 0.0\n",
    "        accuracy_tracker.append({'idx': idx, 'accuracy': accuracy})\n",
    "        \n",
    "        # Check for planned adaptation\n",
    "        if planned_adaptation_idx is not None and idx >= planned_adaptation_idx:\n",
    "            print(f\"  [Sample {idx}] Applying adaptation - retraining model\")\n",
    "            \n",
    "            # Get post-drift data for retraining\n",
    "            adapt_start = max(drift_detected_at, idx - ADAPTATION_WINDOW)\n",
    "            adapt_X = X[adapt_start:idx]\n",
    "            adapt_y = y[adapt_start:idx]\n",
    "            \n",
    "            # Full model reset\n",
    "            sklearn_model = create_sklearn_model()\n",
    "            sklearn_model.fit(adapt_X, adapt_y)\n",
    "            \n",
    "            # Reset detector\n",
    "            if method_name == 'ADWIN':\n",
    "                detector = ADWIN(delta=0.002)\n",
    "            elif method_name == 'DDM':\n",
    "                detector = DDM()\n",
    "            elif method_name == 'EDDM':\n",
    "                detector = EDDM(alpha=0.95, beta=0.9)\n",
    "            elif method_name == 'HDDM_A':\n",
    "                detector = HDDM_A()\n",
    "            elif method_name == 'HDDM_W':\n",
    "                detector = HDDM_W()\n",
    "            elif method_name == 'FHDDM':\n",
    "                detector = FHDDM(short_window_size=20)\n",
    "            \n",
    "            accuracy_buffer.clear()\n",
    "            \n",
    "            adaptation_applied = True\n",
    "            adaptation_idx = idx\n",
    "            planned_adaptation_idx = None\n",
    "            print(f\"  Model retrained on {len(adapt_X)} samples\")\n",
    "        \n",
    "        # Update drift detector\n",
    "        accuracy_buffer.append(is_correct)\n",
    "        \n",
    "        if signal_type == 'continuous':\n",
    "            if len(accuracy_buffer) >= 10:\n",
    "                signal = float(np.mean(accuracy_buffer))\n",
    "            else:\n",
    "                signal = float(is_correct)\n",
    "        else:\n",
    "            signal = bool(1 - is_correct)  # Error signal\n",
    "        \n",
    "        detector.update(signal)\n",
    "        \n",
    "        # Check for drift\n",
    "        if detector.drift_detected and (idx - last_detection >= cfg['detection_cooldown']) and not drift_detected:\n",
    "            drift_detected = True\n",
    "            drift_detected_at = idx\n",
    "            detections.append(idx)\n",
    "            last_detection = idx\n",
    "            \n",
    "            # Classify drift type\n",
    "            drift_result = classify_drift_at_detection(X, drift_detected_at, DRIFT_TYPE_CONFIG)\n",
    "            drift_type = drift_result['subcategory']\n",
    "            \n",
    "            # Schedule adaptation\n",
    "            planned_adaptation_idx = idx + ADAPTATION_DELAY\n",
    "            \n",
    "            print(f\"  [Sample {idx}] DRIFT DETECTED\")\n",
    "            print(f\"    Drift type: {drift_type}\")\n",
    "            print(f\"    Scheduling adaptation at sample {planned_adaptation_idx}\")\n",
    "    \n",
    "    # Phase 4: Calculate metrics\n",
    "    end_time = time.time()\n",
    "    end_mem = monitor_resources()\n",
    "    \n",
    "    # Detection metrics\n",
    "    detection_metrics = calculate_mttd_metrics(detections, true_drift)\n",
    "    \n",
    "    # Performance metrics\n",
    "    acc_indices = np.array([item['idx'] for item in accuracy_tracker])\n",
    "    acc_values = np.array([item['accuracy'] for item in accuracy_tracker])\n",
    "    \n",
    "    if drift_detected and adaptation_applied:\n",
    "        pre_mask = acc_indices < drift_detected_at\n",
    "        deg_mask = (acc_indices >= drift_detected_at) & (acc_indices < adaptation_idx)\n",
    "        rec_mask = (acc_indices >= adaptation_idx) & (acc_indices < min(adaptation_idx + 300, len(X)))\n",
    "        \n",
    "        pre_acc = np.mean(acc_values[pre_mask]) if np.any(pre_mask) else baseline_accuracy\n",
    "        deg_min = np.min(acc_values[deg_mask]) if np.any(deg_mask) else 0.0\n",
    "        rec_acc = np.mean(acc_values[rec_mask]) if np.any(rec_mask) else 0.0\n",
    "        \n",
    "        accuracy_drop = pre_acc - deg_min\n",
    "        recovery_rate = (rec_acc - deg_min) / accuracy_drop * 100 if accuracy_drop > 0 else 0.0\n",
    "        \n",
    "        # Recovery time\n",
    "        target_acc = pre_acc * 0.95\n",
    "        recovery_time = None\n",
    "        for idx_val, acc_val in zip(acc_indices[rec_mask], acc_values[rec_mask]):\n",
    "            if acc_val >= target_acc:\n",
    "                recovery_time = idx_val - adaptation_idx\n",
    "                break\n",
    "        if recovery_time is None:\n",
    "            recovery_time = len(acc_values[rec_mask])\n",
    "    else:\n",
    "        pre_acc = baseline_accuracy\n",
    "        deg_min = np.min(acc_values) if len(acc_values) > 0 else 0.0\n",
    "        rec_acc = 0.0\n",
    "        accuracy_drop = 0.0\n",
    "        recovery_rate = 0.0\n",
    "        recovery_time = 0\n",
    "    \n",
    "    print(f\"  âœ“ Evaluation complete ({end_time - start_time:.2f}s)\")\n",
    "    print(f\"    Detection: {detection_metrics['tp']} TP, {detection_metrics['fp']} FP, {detection_metrics['fn']} FN\")\n",
    "    print(f\"    F1-score: {detection_metrics['f1_score']:.3f}, MTTD: {detection_metrics['mttd']}\")\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'paradigm': 'streaming',\n",
    "        'detections': detections,\n",
    "        'drift_detected': drift_detected,\n",
    "        'drift_detected_at': drift_detected_at,\n",
    "        'drift_type': drift_type,\n",
    "        'adaptation_applied': adaptation_applied,\n",
    "        'adaptation_idx': adaptation_idx,\n",
    "        'baseline_accuracy': pre_acc,\n",
    "        'min_accuracy': deg_min,\n",
    "        'recovery_accuracy': rec_acc,\n",
    "        'accuracy_drop': accuracy_drop,\n",
    "        'recovery_rate': recovery_rate,\n",
    "        'recovery_time': recovery_time,\n",
    "        'runtime_total_s': end_time - start_time,\n",
    "        'runtime_per_instance_ms': (end_time - start_time) * 1000 / len(X),\n",
    "        'memory_mb': max(0.0, end_mem - start_mem),\n",
    "        'accuracy_timeline': accuracy_tracker,\n",
    "        **detection_metrics\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Streaming evaluation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MULTI-DATASET BENCHMARK\n",
      "================================================================================\n",
      "Datasets to evaluate: 5\n",
      "Detectors to evaluate: 10\n",
      "Total experiments: 50\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DATASET 1/5: STANDARD_SEA\n",
      "================================================================================\n",
      "\n",
      "  Generating standard_sea with 2 drift events...\n",
      "  Drift positions: [3333, 6666]\n",
      "  âœ“ Standard SEA: 10000 samples, 3 features, 2 drifts\n",
      "\n",
      "  Dataset info:\n",
      "    Name: Standard SEA\n",
      "    Academic status: âœ… Standard\n",
      "    Samples: 10000\n",
      "    Features: 3\n",
      "    Drift positions: [3333, 6666]\n",
      "    Number of drift events: 2\n",
      "\n",
      "  Window-based detectors (4 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: D3 (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 1650] DRIFT DETECTED at 1650\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 1700\n",
      "  [Sample 1700] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  [Sample 1800] Additional drift detected at 1800\n",
      "  [Sample 1950] Additional drift detected at 1950\n",
      "  [Sample 2700] Additional drift detected at 2700\n",
      "  [Sample 3150] Additional drift detected at 3150\n",
      "  [Sample 3600] Additional drift detected at 3600\n",
      "  [Sample 3900] Additional drift detected at 3900\n",
      "  [Sample 4050] Additional drift detected at 4050\n",
      "  [Sample 4200] Additional drift detected at 4200\n",
      "  [Sample 4350] Additional drift detected at 4350\n",
      "  [Sample 4500] Additional drift detected at 4500\n",
      "  [Sample 4800] Additional drift detected at 4800\n",
      "  [Sample 4950] Additional drift detected at 4950\n",
      "  [Sample 5100] Additional drift detected at 5100\n",
      "  [Sample 5250] Additional drift detected at 5250\n",
      "  [Sample 5550] Additional drift detected at 5550\n",
      "  [Sample 6000] Additional drift detected at 6000\n",
      "  [Sample 6150] Additional drift detected at 6150\n",
      "  [Sample 6300] Additional drift detected at 6300\n",
      "  [Sample 6750] Additional drift detected at 6750\n",
      "  [Sample 7200] Additional drift detected at 7200\n",
      "  [Sample 7500] Additional drift detected at 7500\n",
      "  [Sample 7950] Additional drift detected at 7950\n",
      "  [Sample 8400] Additional drift detected at 8400\n",
      "  [Sample 8550] Additional drift detected at 8550\n",
      "  [Sample 8700] Additional drift detected at 8700\n",
      "  [Sample 9000] Additional drift detected at 9000\n",
      "  [Sample 9150] Additional drift detected at 9150\n",
      "  [Sample 9600] Additional drift detected at 9600\n",
      "  [Sample 9900] Additional drift detected at 9900\n",
      "  âœ“ Evaluation complete (1.66s)\n",
      "    Detection: 0 TP, 30 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DAWIDD (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 2550] DRIFT DETECTED at 2550\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 2600\n",
      "  [Sample 2600] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  [Sample 7350] Additional drift detected at 7350\n",
      "  [Sample 8100] Additional drift detected at 8100\n",
      "  âœ“ Evaluation complete (6.84s)\n",
      "    Detection: 0 TP, 3 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 3600] DRIFT DETECTED at 3464\n",
      "    Drift type: blip\n",
      "    p-value: 0.008000\n",
      "    Scheduling adaptation at sample 3650\n",
      "  [Sample 3650] Applying adaptation - retraining model\n",
      "  Model retrained on 186 samples\n",
      "  [Sample 4350] Additional drift detected at 4239 (p-value: 0.042400)\n",
      "  [Sample 5850] Additional drift detected at 5756 (p-value: 0.019200)\n",
      "  [Sample 6000] Additional drift detected at 5873 (p-value: 0.012000)\n",
      "  [Sample 6450] Additional drift detected at 6336 (p-value: 0.007600)\n",
      "  [Sample 7050] Additional drift detected at 6948 (p-value: 0.031600)\n",
      "  âœ“ Evaluation complete (5.99s)\n",
      "    Detection: 0 TP, 6 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD_Improved (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 3000] DRIFT DETECTED at 2851\n",
      "    Drift type: blip\n",
      "    p-value: 0.016800\n",
      "    Scheduling adaptation at sample 3050\n",
      "  [Sample 3050] Applying adaptation - retraining model\n",
      "  Model retrained on 199 samples\n",
      "  [Sample 3300] Additional drift detected at 3163 (p-value: 0.007200)\n",
      "  [Sample 4350] Additional drift detected at 4238 (p-value: 0.038400)\n",
      "  [Sample 5250] Additional drift detected at 5147 (p-value: 0.022000)\n",
      "  [Sample 6000] Additional drift detected at 5874 (p-value: 0.019600)\n",
      "  [Sample 7950] Additional drift detected at 7808 (p-value: 0.046400)\n",
      "  âœ“ Evaluation complete (4.90s)\n",
      "    Detection: 0 TP, 6 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "  Streaming detectors (6 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ADWIN (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: continuous)...\n",
      "  [Sample 3479] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 3529\n",
      "  [Sample 3529] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.50s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 862] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 912\n",
      "  [Sample 912] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.44s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: EDDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 3441] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 3491\n",
      "  [Sample 3491] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.32s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_A (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: continuous)...\n",
      "  âœ“ Evaluation complete (1.47s)\n",
      "    Detection: 0 TP, 0 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_W (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  âœ“ Evaluation complete (1.24s)\n",
      "    Detection: 0 TP, 0 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: FHDDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  âœ“ Evaluation complete (1.22s)\n",
      "    Detection: 0 TP, 0 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "  Dataset summary:\n",
      "    Detection rate: 70.0%\n",
      "    Average F1: 0.000\n",
      "    Average recovery: 31.0%\n",
      "    Best detector: D3 (F1=0.000)\n",
      "\n",
      "================================================================================\n",
      "DATASET 2/5: ENHANCED_SEA\n",
      "================================================================================\n",
      "\n",
      "  Generating enhanced_sea with 2 drift events...\n",
      "  Drift positions: [3333, 6666]\n",
      "  âœ“ Enhanced SEA: 10000 samples, 3 features, 2 drifts\n",
      "\n",
      "  Dataset info:\n",
      "    Name: Enhanced SEA\n",
      "    Academic status: âš ï¸ Non-standard\n",
      "    Samples: 10000\n",
      "    Features: 3\n",
      "    Drift positions: [3333, 6666]\n",
      "    Number of drift events: 2\n",
      "\n",
      "  Window-based detectors (4 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: D3 (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 1650] DRIFT DETECTED at 1650\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 1700\n",
      "  [Sample 1700] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  [Sample 1800] Additional drift detected at 1800\n",
      "  [Sample 1950] Additional drift detected at 1950\n",
      "  [Sample 2700] Additional drift detected at 2700\n",
      "  [Sample 3150] Additional drift detected at 3150\n",
      "  [Sample 3600] Additional drift detected at 3600\n",
      "  [Sample 3900] Additional drift detected at 3900\n",
      "  [Sample 4050] Additional drift detected at 4050\n",
      "  [Sample 4200] Additional drift detected at 4200\n",
      "  [Sample 4350] Additional drift detected at 4350\n",
      "  [Sample 4500] Additional drift detected at 4500\n",
      "  [Sample 4950] Additional drift detected at 4950\n",
      "  [Sample 5100] Additional drift detected at 5100\n",
      "  [Sample 5250] Additional drift detected at 5250\n",
      "  [Sample 5550] Additional drift detected at 5550\n",
      "  [Sample 6000] Additional drift detected at 6000\n",
      "  [Sample 6300] Additional drift detected at 6300\n",
      "  [Sample 7200] Additional drift detected at 7200\n",
      "  [Sample 7500] Additional drift detected at 7500\n",
      "  [Sample 7950] Additional drift detected at 7950\n",
      "  [Sample 8400] Additional drift detected at 8400\n",
      "  [Sample 8550] Additional drift detected at 8550\n",
      "  [Sample 8700] Additional drift detected at 8700\n",
      "  [Sample 9000] Additional drift detected at 9000\n",
      "  [Sample 9150] Additional drift detected at 9150\n",
      "  [Sample 9600] Additional drift detected at 9600\n",
      "  [Sample 9900] Additional drift detected at 9900\n",
      "  âœ“ Evaluation complete (1.54s)\n",
      "    Detection: 0 TP, 27 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DAWIDD (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 2550] DRIFT DETECTED at 2550\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 2600\n",
      "  [Sample 2600] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  [Sample 3450] Additional drift detected at 3450\n",
      "  [Sample 6750] Additional drift detected at 6750\n",
      "  [Sample 7350] Additional drift detected at 7350\n",
      "  [Sample 8100] Additional drift detected at 8100\n",
      "  âœ“ Evaluation complete (3.12s)\n",
      "    Detection: 0 TP, 5 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 3450] DRIFT DETECTED at 3335\n",
      "    Drift type: sudden\n",
      "    p-value: 0.000000\n",
      "    Scheduling adaptation at sample 3500\n",
      "  [Sample 3500] Applying adaptation - retraining model\n",
      "  Model retrained on 165 samples\n",
      "  [Sample 3600] Additional drift detected at 3465 (p-value: 0.006000)\n",
      "  [Sample 6000] Additional drift detected at 5872 (p-value: 0.015200)\n",
      "  [Sample 6750] Additional drift detected at 6654 (p-value: 0.000000)\n",
      "  [Sample 7050] Additional drift detected at 6948 (p-value: 0.031600)\n",
      "  âœ“ Evaluation complete (5.00s)\n",
      "    Detection: 1 TP, 4 FP, 0 FN\n",
      "    F1-score: 0.333, MTTD: 2\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD_Improved (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 3000] DRIFT DETECTED at 2851\n",
      "    Drift type: blip\n",
      "    p-value: 0.016800\n",
      "    Scheduling adaptation at sample 3050\n",
      "  [Sample 3050] Applying adaptation - retraining model\n",
      "  Model retrained on 199 samples\n",
      "  [Sample 3300] Additional drift detected at 3163 (p-value: 0.007200)\n",
      "  [Sample 3450] Additional drift detected at 3334 (p-value: 0.000000)\n",
      "  [Sample 6000] Additional drift detected at 5874 (p-value: 0.030800)\n",
      "  [Sample 6600] Additional drift detected at 6453 (p-value: 0.026800)\n",
      "  [Sample 6750] Additional drift detected at 6661 (p-value: 0.000000)\n",
      "  [Sample 6900] Additional drift detected at 6769 (p-value: 0.010400)\n",
      "  [Sample 7200] Additional drift detected at 7055 (p-value: 0.011600)\n",
      "  [Sample 7950] Additional drift detected at 7808 (p-value: 0.046400)\n",
      "  âœ“ Evaluation complete (4.13s)\n",
      "    Detection: 1 TP, 8 FP, 0 FN\n",
      "    F1-score: 0.200, MTTD: 1\n",
      "\n",
      "  Streaming detectors (6 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ADWIN (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: continuous)...\n",
      "  [Sample 3415] DRIFT DETECTED\n",
      "    Drift type: blip\n",
      "    Scheduling adaptation at sample 3465\n",
      "  [Sample 3465] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.54s)\n",
      "    Detection: 1 TP, 0 FP, 0 FN\n",
      "    F1-score: 1.000, MTTD: 82\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 862] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 912\n",
      "  [Sample 912] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.43s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: EDDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 3360] DRIFT DETECTED\n",
      "    Drift type: blip\n",
      "    Scheduling adaptation at sample 3410\n",
      "  [Sample 3410] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.45s)\n",
      "    Detection: 1 TP, 0 FP, 0 FN\n",
      "    F1-score: 1.000, MTTD: 27\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_A (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: continuous)...\n",
      "  [Sample 6853] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 6903\n",
      "  [Sample 6903] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.61s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_W (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 3379] DRIFT DETECTED\n",
      "    Drift type: blip\n",
      "    Scheduling adaptation at sample 3429\n",
      "  [Sample 3429] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.42s)\n",
      "    Detection: 1 TP, 0 FP, 0 FN\n",
      "    F1-score: 1.000, MTTD: 46\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: FHDDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 3549] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 3599\n",
      "  [Sample 3599] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.33s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "  Dataset summary:\n",
      "    Detection rate: 100.0%\n",
      "    Average F1: 0.353\n",
      "    Average recovery: 49.9%\n",
      "    Best detector: ADWIN (F1=1.000)\n",
      "\n",
      "================================================================================\n",
      "DATASET 3/5: STAGGER\n",
      "================================================================================\n",
      "\n",
      "  Generating stagger with 1 drift events...\n",
      "  Drift positions: [5000]\n",
      "  âœ“ STAGGER: 10000 samples, 5 features, 1 drifts\n",
      "\n",
      "  Dataset info:\n",
      "    Name: STAGGER\n",
      "    Academic status: âœ… Standard\n",
      "    Samples: 10000\n",
      "    Features: 5\n",
      "    Drift positions: [5000]\n",
      "    Number of drift events: 1\n",
      "\n",
      "  Window-based detectors (4 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: D3 (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 1650] DRIFT DETECTED at 1650\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 1700\n",
      "  [Sample 1700] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  [Sample 1800] Additional drift detected at 1800\n",
      "  [Sample 2250] Additional drift detected at 2250\n",
      "  [Sample 2850] Additional drift detected at 2850\n",
      "  [Sample 3300] Additional drift detected at 3300\n",
      "  [Sample 3450] Additional drift detected at 3450\n",
      "  [Sample 3600] Additional drift detected at 3600\n",
      "  [Sample 3750] Additional drift detected at 3750\n",
      "  [Sample 3900] Additional drift detected at 3900\n",
      "  [Sample 5250] Additional drift detected at 5250\n",
      "  [Sample 5850] Additional drift detected at 5850\n",
      "  [Sample 6000] Additional drift detected at 6000\n",
      "  [Sample 6150] Additional drift detected at 6150\n",
      "  [Sample 6600] Additional drift detected at 6600\n",
      "  [Sample 6900] Additional drift detected at 6900\n",
      "  [Sample 7050] Additional drift detected at 7050\n",
      "  [Sample 7350] Additional drift detected at 7350\n",
      "  [Sample 7500] Additional drift detected at 7500\n",
      "  [Sample 7800] Additional drift detected at 7800\n",
      "  [Sample 8700] Additional drift detected at 8700\n",
      "  [Sample 9150] Additional drift detected at 9150\n",
      "  [Sample 9450] Additional drift detected at 9450\n",
      "  [Sample 9600] Additional drift detected at 9600\n",
      "  [Sample 9750] Additional drift detected at 9750\n",
      "  âœ“ Evaluation complete (1.45s)\n",
      "    Detection: 0 TP, 24 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DAWIDD (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 3000] DRIFT DETECTED at 3000\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 3050\n",
      "  [Sample 3050] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  [Sample 5100] Additional drift detected at 5100\n",
      "  [Sample 6750] Additional drift detected at 6750\n",
      "  [Sample 7050] Additional drift detected at 7050\n",
      "  âœ“ Evaluation complete (3.05s)\n",
      "    Detection: 1 TP, 3 FP, 0 FN\n",
      "    F1-score: 0.400, MTTD: 100\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 1650] DRIFT DETECTED at 1522\n",
      "    Drift type: blip\n",
      "    p-value: 0.009200\n",
      "    Scheduling adaptation at sample 1700\n",
      "  [Sample 1700] Applying adaptation - retraining model\n",
      "  Model retrained on 178 samples\n",
      "  [Sample 3150] Additional drift detected at 3002 (p-value: 0.036400)\n",
      "  [Sample 3600] Additional drift detected at 3468 (p-value: 0.005200)\n",
      "  [Sample 5100] Additional drift detected at 4998 (p-value: 0.000000)\n",
      "  [Sample 5250] Additional drift detected at 5115 (p-value: 0.017600)\n",
      "  [Sample 5700] Additional drift detected at 5600 (p-value: 0.034400)\n",
      "  [Sample 7050] Additional drift detected at 6960 (p-value: 0.008800)\n",
      "  [Sample 7800] Additional drift detected at 7699 (p-value: 0.047200)\n",
      "  [Sample 9450] Additional drift detected at 9338 (p-value: 0.036000)\n",
      "  âœ“ Evaluation complete (5.28s)\n",
      "    Detection: 1 TP, 8 FP, 0 FN\n",
      "    F1-score: 0.200, MTTD: 2\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD_Improved (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 1650] DRIFT DETECTED at 1516\n",
      "    Drift type: blip\n",
      "    p-value: 0.001600\n",
      "    Scheduling adaptation at sample 1700\n",
      "  [Sample 1700] Applying adaptation - retraining model\n",
      "  Model retrained on 184 samples\n",
      "  [Sample 3150] Additional drift detected at 3002 (p-value: 0.030400)\n",
      "  [Sample 5100] Additional drift detected at 4992 (p-value: 0.000000)\n",
      "  [Sample 7650] Additional drift detected at 7530 (p-value: 0.018400)\n",
      "  âœ“ Evaluation complete (4.45s)\n",
      "    Detection: 1 TP, 3 FP, 0 FN\n",
      "    F1-score: 0.400, MTTD: 8\n",
      "\n",
      "  Streaming detectors (6 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ADWIN (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: continuous)...\n",
      "  [Sample 5047] DRIFT DETECTED\n",
      "    Drift type: sudden\n",
      "    Scheduling adaptation at sample 5097\n",
      "  [Sample 5097] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.41s)\n",
      "    Detection: 1 TP, 0 FP, 0 FN\n",
      "    F1-score: 1.000, MTTD: 47\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 782] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 832\n",
      "  [Sample 832] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.37s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: EDDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 3201] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 3251\n",
      "  [Sample 3251] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.35s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_A (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: continuous)...\n",
      "  âœ“ Evaluation complete (1.55s)\n",
      "    Detection: 0 TP, 0 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_W (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 5070] DRIFT DETECTED\n",
      "    Drift type: gradual\n",
      "    Scheduling adaptation at sample 5120\n",
      "  [Sample 5120] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.37s)\n",
      "    Detection: 1 TP, 0 FP, 0 FN\n",
      "    F1-score: 1.000, MTTD: 70\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: FHDDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 1.0000\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 5633] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 5683\n",
      "  [Sample 5683] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.29s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "  Dataset summary:\n",
      "    Detection rate: 90.0%\n",
      "    Average F1: 0.300\n",
      "    Average recovery: -22.4%\n",
      "    Best detector: ADWIN (F1=1.000)\n",
      "\n",
      "================================================================================\n",
      "DATASET 4/5: HYPERPLANE\n",
      "================================================================================\n",
      "\n",
      "  Generating hyperplane with 2 drift events...\n",
      "  Drift positions: [3333, 6666]\n",
      "  âœ“ Hyperplane: 10000 samples, 10 features, 2 drifts\n",
      "\n",
      "  Dataset info:\n",
      "    Name: Hyperplane\n",
      "    Academic status: âœ… Standard\n",
      "    Samples: 10000\n",
      "    Features: 10\n",
      "    Drift positions: [3333, 6666]\n",
      "    Number of drift events: 2\n",
      "\n",
      "  Window-based detectors (4 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: D3 (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 1950] DRIFT DETECTED at 1950\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 2000\n",
      "  [Sample 2000] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  [Sample 2700] Additional drift detected at 2700\n",
      "  [Sample 2850] Additional drift detected at 2850\n",
      "  [Sample 3000] Additional drift detected at 3000\n",
      "  [Sample 3300] Additional drift detected at 3300\n",
      "  [Sample 3750] Additional drift detected at 3750\n",
      "  [Sample 3900] Additional drift detected at 3900\n",
      "  [Sample 4050] Additional drift detected at 4050\n",
      "  [Sample 4350] Additional drift detected at 4350\n",
      "  [Sample 5100] Additional drift detected at 5100\n",
      "  [Sample 5400] Additional drift detected at 5400\n",
      "  [Sample 5550] Additional drift detected at 5550\n",
      "  [Sample 5700] Additional drift detected at 5700\n",
      "  [Sample 6600] Additional drift detected at 6600\n",
      "  [Sample 7200] Additional drift detected at 7200\n",
      "  [Sample 7350] Additional drift detected at 7350\n",
      "  [Sample 7500] Additional drift detected at 7500\n",
      "  [Sample 7800] Additional drift detected at 7800\n",
      "  [Sample 8100] Additional drift detected at 8100\n",
      "  [Sample 8700] Additional drift detected at 8700\n",
      "  [Sample 8850] Additional drift detected at 8850\n",
      "  [Sample 9000] Additional drift detected at 9000\n",
      "  [Sample 9450] Additional drift detected at 9450\n",
      "  [Sample 9750] Additional drift detected at 9750\n",
      "  âœ“ Evaluation complete (1.43s)\n",
      "    Detection: 1 TP, 23 FP, 0 FN\n",
      "    F1-score: 0.080, MTTD: 33\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DAWIDD (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 6900] DRIFT DETECTED at 6900\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 6950\n",
      "  [Sample 6950] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  [Sample 9900] Additional drift detected at 9900\n",
      "  âœ“ Evaluation complete (2.83s)\n",
      "    Detection: 0 TP, 2 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  [Sample 1650] DRIFT DETECTED at 1563\n",
      "    Drift type: sudden\n",
      "    p-value: 0.020800\n",
      "    Scheduling adaptation at sample 1700\n",
      "  [Sample 1700] Applying adaptation - retraining model\n",
      "  Model retrained on 137 samples\n",
      "  [Sample 4800] Additional drift detected at 4684 (p-value: 0.032000)\n",
      "  [Sample 6000] Additional drift detected at 5869 (p-value: 0.010000)\n",
      "  [Sample 6150] Additional drift detected at 6009 (p-value: 0.010800)\n",
      "  [Sample 6900] Additional drift detected at 6825 (p-value: 0.044400)\n",
      "  [Sample 7050] Additional drift detected at 6916 (p-value: 0.000000)\n",
      "  [Sample 7500] Additional drift detected at 7379 (p-value: 0.036400)\n",
      "  [Sample 8700] Additional drift detected at 8612 (p-value: 0.027600)\n",
      "  [Sample 9900] Additional drift detected at 9814 (p-value: 0.000800)\n",
      "  âœ“ Evaluation complete (4.71s)\n",
      "    Detection: 0 TP, 9 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD_Improved (Window-based)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Detection buffer size: 1000 samples\n",
      "  Drift check frequency: every 150 samples\n",
      "  Processing stream...\n",
      "  âœ“ Evaluation complete (4.91s)\n",
      "    Detection: 0 TP, 0 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "  Streaming detectors (6 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ADWIN (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Processing stream (signal type: continuous)...\n",
      "  [Sample 2199] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 2249\n",
      "  [Sample 2249] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.43s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 3785] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 3835\n",
      "  [Sample 3835] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.37s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: EDDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 971] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 1021\n",
      "  [Sample 1021] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.37s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_A (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Processing stream (signal type: continuous)...\n",
      "  âœ“ Evaluation complete (1.45s)\n",
      "    Detection: 0 TP, 0 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_W (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 3397] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 3447\n",
      "  [Sample 3447] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.37s)\n",
      "    Detection: 1 TP, 0 FP, 0 FN\n",
      "    F1-score: 1.000, MTTD: 64\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: FHDDM (Streaming)\n",
      "================================================================================\n",
      "  Initial training: 500 samples\n",
      "  Baseline accuracy: 0.9700\n",
      "  Processing stream (signal type: binary)...\n",
      "  [Sample 3885] DRIFT DETECTED\n",
      "    Drift type: recurrent\n",
      "    Scheduling adaptation at sample 3935\n",
      "  [Sample 3935] Applying adaptation - retraining model\n",
      "  Model retrained on 50 samples\n",
      "  âœ“ Evaluation complete (1.32s)\n",
      "    Detection: 0 TP, 1 FP, 1 FN\n",
      "    F1-score: 0.000, MTTD: inf\n",
      "\n",
      "  Dataset summary:\n",
      "    Detection rate: 80.0%\n",
      "    Average F1: 0.108\n",
      "    Average recovery: 40.6%\n",
      "    Best detector: HDDM_W (F1=1.000)\n",
      "\n",
      "================================================================================\n",
      "DATASET 5/5: GEN_RANDOM\n",
      "================================================================================\n",
      "\n",
      "  Generating gen_random with 3 drift events...\n",
      "  Drift positions: [2500, 5000, 7500]\n",
      "  âœ“ gen_random: 10000 samples, 5 features, 3 drifts\n",
      "    Class distribution: {0: 5000, 1: 5000}\n",
      "\n",
      "  Dataset info:\n",
      "    Name: gen_random\n",
      "    Academic status: ðŸ”§ Custom\n",
      "    Samples: 10000\n",
      "    Features: 5\n",
      "    Drift positions: [2143, 3955, 9676]\n",
      "    Number of drift events: 3\n",
      "\n",
      "  Window-based detectors (4 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: D3 (Window-based)\n",
      "================================================================================\n",
      "    âœ— D3 failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DAWIDD (Window-based)\n",
      "================================================================================\n",
      "    âœ— DAWIDD failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD (Window-based)\n",
      "================================================================================\n",
      "    âœ— ShapeDD failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ShapeDD_Improved (Window-based)\n",
      "================================================================================\n",
      "    âœ— ShapeDD_Improved failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "  Streaming detectors (6 methods):\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: ADWIN (Streaming)\n",
      "================================================================================\n",
      "    âœ— ADWIN failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: DDM (Streaming)\n",
      "================================================================================\n",
      "    âœ— DDM failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: EDDM (Streaming)\n",
      "================================================================================\n",
      "    âœ— EDDM failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_A (Streaming)\n",
      "================================================================================\n",
      "    âœ— HDDM_A failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: HDDM_W (Streaming)\n",
      "================================================================================\n",
      "    âœ— HDDM_W failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
      "\n",
      "================================================================================\n",
      "EVALUATING: FHDDM (Streaming)\n",
      "================================================================================\n",
      "    âœ— FHDDM failed: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'F1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 105\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Calculate dataset-level summary\u001b[39;00m\n\u001b[1;32m     91\u001b[0m dataset_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m'\u001b[39m: r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m'\u001b[39m: r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecovery_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m: r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecovery_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDetected\u001b[39m\u001b[38;5;124m'\u001b[39m: r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrift_detected\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     96\u001b[0m } \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m dataset_results])\n\u001b[1;32m     98\u001b[0m dataset_summary \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_name,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_drift_events\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_drift_events\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrift_positions\u001b[39m\u001b[38;5;124m'\u001b[39m: true_drift_positions,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetection_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDetected\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset_df) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdataset_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_recovery\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecovery_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_method\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_df\u001b[38;5;241m.\u001b[39mloc[dataset_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmax(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    109\u001b[0m }\n\u001b[1;32m    110\u001b[0m dataset_summaries\u001b[38;5;241m.\u001b[39mappend(dataset_summary)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Dataset summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sandboxes/One-or-Two-Things-We-Know-about-Concept-Drift/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/sandboxes/One-or-Two-Things-We-Know-about-Concept-Drift/.venv/lib/python3.10/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'F1'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: COMPREHENSIVE MULTI-DATASET BENCHMARK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MULTI-DATASET BENCHMARK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get enabled datasets\n",
    "enabled_datasets = [(name, config) for name, config in DATASET_CATALOG.items() \n",
    "                    if config['enabled']]\n",
    "\n",
    "print(f\"Datasets to evaluate: {len(enabled_datasets)}\")\n",
    "print(f\"Detectors to evaluate: {len(WINDOW_METHODS) + len(STREAMING_METHODS)}\")\n",
    "print(f\"Total experiments: {len(enabled_datasets) * (len(WINDOW_METHODS) + len(STREAMING_METHODS))}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Storage for all results\n",
    "all_comprehensive_results = []\n",
    "dataset_summaries = []\n",
    "\n",
    "# Run experiments for each dataset\n",
    "for dataset_idx, (dataset_name, dataset_config) in enumerate(enabled_datasets, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATASET {dataset_idx}/{len(enabled_datasets)}: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Generate dataset with multiple drifts\n",
    "    X_stream, y_stream, true_drift_positions, dataset_info = generate_drift_stream(\n",
    "        dataset_type=dataset_name,\n",
    "        total_size=STREAM_SIZE,\n",
    "        n_drift_events=dataset_config['n_drift_events'],\n",
    "        seed=RANDOM_SEED,\n",
    "        **dataset_config['params']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  Dataset info:\")\n",
    "    print(f\"    Name: {dataset_info['name']}\")\n",
    "    print(f\"    Academic status: {dataset_info['academic_status']}\")\n",
    "    print(f\"    Samples: {dataset_info['total_samples']}\")\n",
    "    print(f\"    Features: {dataset_info['features']}\")\n",
    "    print(f\"    Drift positions: {dataset_info['drift_positions']}\")\n",
    "    print(f\"    Number of drift events: {dataset_info['n_drift_events']}\")\n",
    "    \n",
    "    # Store dataset for this iteration\n",
    "    dataset_results = []\n",
    "    \n",
    "    # Note: For multi-drift evaluation, we use the FIRST drift position as the primary drift point\n",
    "    # This maintains compatibility with detection metrics while still evaluating multi-drift scenarios\n",
    "    primary_drift_point = true_drift_positions[0] if len(true_drift_positions) > 0 else DRIFT_POSITION\n",
    "    \n",
    "    # Evaluate window-based methods\n",
    "    print(f\"\\n  Window-based detectors ({len(WINDOW_METHODS)} methods):\")\n",
    "    print(f\"  {'-'*76}\")\n",
    "    \n",
    "    for method in WINDOW_METHODS:\n",
    "        try:\n",
    "            result = evaluate_window_detector_with_adaptation(method, X_stream, y_stream, primary_drift_point)\n",
    "            result['dataset'] = dataset_name\n",
    "            result['dataset_name'] = dataset_info['name']\n",
    "            result['dataset_features'] = dataset_info['features']\n",
    "            result['true_drift_point'] = primary_drift_point\n",
    "            result['all_drift_positions'] = true_drift_positions\n",
    "            result['n_drift_events'] = dataset_info['n_drift_events']\n",
    "            dataset_results.append(result)\n",
    "            all_comprehensive_results.append(result)\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— {method} failed: {str(e)}\")\n",
    "    \n",
    "    # Evaluate streaming methods\n",
    "    print(f\"\\n  Streaming detectors ({len(STREAMING_METHODS)} methods):\")\n",
    "    print(f\"  {'-'*76}\")\n",
    "    \n",
    "    for method in STREAMING_METHODS:\n",
    "        try:\n",
    "            result = evaluate_streaming_detector_with_adaptation(method, X_stream, y_stream, primary_drift_point)\n",
    "            result['dataset'] = dataset_name\n",
    "            result['dataset_name'] = dataset_info['name']\n",
    "            result['dataset_features'] = dataset_info['features']\n",
    "            result['true_drift_point'] = primary_drift_point\n",
    "            result['all_drift_positions'] = true_drift_positions\n",
    "            result['n_drift_events'] = dataset_info['n_drift_events']\n",
    "            dataset_results.append(result)\n",
    "            all_comprehensive_results.append(result)\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— {method} failed: {str(e)}\")\n",
    "    \n",
    "    # Calculate dataset-level summary\n",
    "    dataset_df = pd.DataFrame([{\n",
    "        'Method': r['method'],\n",
    "        'F1': r['f1_score'],\n",
    "        'Recovery_%': r['recovery_rate'],\n",
    "        'Detected': r['drift_detected']\n",
    "    } for r in dataset_results])\n",
    "    \n",
    "    dataset_summary = {\n",
    "        'dataset': dataset_name,\n",
    "        'dataset_name': dataset_info['name'],\n",
    "        'features': dataset_info['features'],\n",
    "        'n_drift_events': dataset_info['n_drift_events'],\n",
    "        'drift_positions': true_drift_positions,\n",
    "        'detection_rate': dataset_df['Detected'].sum() / len(dataset_df) if len(dataset_df) > 0 else 0,\n",
    "        'avg_f1': dataset_df['F1'].mean(),\n",
    "        'avg_recovery': dataset_df['Recovery_%'].mean(),\n",
    "        'best_f1': dataset_df['F1'].max() if len(dataset_df) > 0 else 0,\n",
    "        'best_method': dataset_df.loc[dataset_df['F1'].idxmax(), 'Method'] if len(dataset_df) > 0 else 'N/A'\n",
    "    }\n",
    "    dataset_summaries.append(dataset_summary)\n",
    "    \n",
    "    print(f\"\\n  Dataset summary:\")\n",
    "    print(f\"    Detection rate: {dataset_summary['detection_rate']*100:.1f}%\")\n",
    "    print(f\"    Average F1: {dataset_summary['avg_f1']:.3f}\")\n",
    "    print(f\"    Average recovery: {dataset_summary['avg_recovery']:.1f}%\")\n",
    "    print(f\"    Best detector: {dataset_summary['best_method']} (F1={dataset_summary['best_f1']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE BENCHMARK COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ Evaluated {len(all_comprehensive_results)} detector-dataset combinations\")\n",
    "print(f\"âœ“ Total datasets: {len(enabled_datasets)}\")\n",
    "print(f\"âœ“ Total detectors: {len(WINDOW_METHODS) + len(STREAMING_METHODS)}\")\n",
    "print(f\"âœ“ Total drift events across all datasets: {sum(s['n_drift_events'] for s in dataset_summaries)}\")\n",
    "print(f\"âœ“ Total execution time: {sum(r['runtime_total_s'] for r in all_comprehensive_results):.2f}s\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: COMPREHENSIVE RESULTS AGGREGATION\n",
    "# ============================================================================\n",
    "\n",
    "# Create comprehensive DataFrame\n",
    "comprehensive_df = pd.DataFrame([{\n",
    "    'Dataset': r['dataset'],\n",
    "    'Dataset_Name': r['dataset_name'],\n",
    "    'Features': r['dataset_features'],\n",
    "    'Method': r['method'],\n",
    "    'Paradigm': r['paradigm'],\n",
    "    'Detected': r['drift_detected'],\n",
    "    'N_Detections': len(r['detections']),\n",
    "    'Detection_Delay': r['detection_delay'] if r['detection_delay'] is not None else np.nan,\n",
    "    'Drift_Type': r['drift_type'] if r['drift_type'] else 'N/A',\n",
    "    'TP': r['tp'],\n",
    "    'FP': r['fp'],\n",
    "    'FN': r['fn'],\n",
    "    'Precision': r['precision'],\n",
    "    'Recall': r['recall'],\n",
    "    'F1': r['f1_score'],\n",
    "    'MTTD': r['mttd'] if r['mttd'] != float('inf') else np.nan,\n",
    "    'Baseline_Acc': r['baseline_accuracy'],\n",
    "    'Min_Acc': r['min_accuracy'],\n",
    "    'Recovery_Acc': r['recovery_accuracy'],\n",
    "    'Acc_Drop': r['accuracy_drop'],\n",
    "    'Recovery_Rate_%': r['recovery_rate'],\n",
    "    'Recovery_Time': r['recovery_time'],\n",
    "    'Runtime_ms': r['runtime_per_instance_ms'],\n",
    "    'Memory_MB': r['memory_mb']\n",
    "} for r in all_comprehensive_results])\n",
    "\n",
    "# Dataset summaries DataFrame\n",
    "dataset_summary_df = pd.DataFrame(dataset_summaries)\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"COMPREHENSIVE RESULTS SUMMARY - ALL DATASETS\")\n",
    "print(\"=\"*140)\n",
    "print(f\"\\nTotal experiments: {len(comprehensive_df)}\")\n",
    "print(f\"Datasets: {comprehensive_df['Dataset'].nunique()}\")\n",
    "print(f\"Methods: {comprehensive_df['Method'].nunique()}\")\n",
    "print(f\"Paradigms: {comprehensive_df['Paradigm'].nunique()}\")\n",
    "\n",
    "# Display per-dataset summary\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"DATASET-LEVEL SUMMARY\")\n",
    "print(\"=\"*140)\n",
    "print(dataset_summary_df.to_string(index=False))\n",
    "\n",
    "# Method performance across all datasets\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"METHOD PERFORMANCE ACROSS ALL DATASETS (Averages)\")\n",
    "print(\"=\"*140)\n",
    "\n",
    "method_avg = comprehensive_df.groupby('Method').agg({\n",
    "    'F1': ['mean', 'std', 'min', 'max'],\n",
    "    'Recovery_Rate_%': ['mean', 'std'],\n",
    "    'Runtime_ms': ['mean', 'std'],\n",
    "    'Detected': 'sum'\n",
    "}).round(3)\n",
    "\n",
    "print(method_avg)\n",
    "\n",
    "# Best performer per dataset\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"BEST PERFORMER PER DATASET\")\n",
    "print(\"=\"*140)\n",
    "\n",
    "for dataset in comprehensive_df['Dataset'].unique():\n",
    "    dataset_data = comprehensive_df[comprehensive_df['Dataset'] == dataset]\n",
    "    best = dataset_data.loc[dataset_data['F1'].idxmax()]\n",
    "    print(f\"\\n{best['Dataset_Name']}:\")\n",
    "    print(f\"  Best method: {best['Method']} ({best['Paradigm']})\")\n",
    "    print(f\"  F1-Score: {best['F1']:.3f}\")\n",
    "    print(f\"  Recovery Rate: {best['Recovery_Rate_%']:.1f}%\")\n",
    "    print(f\"  Runtime: {best['Runtime_ms']:.4f} ms/instance\")\n",
    "\n",
    "# Overall best detector\n",
    "comprehensive_df['Composite_Score'] = (\n",
    "    comprehensive_df['F1'] * 40 + \n",
    "    comprehensive_df['Recovery_Rate_%'] * 0.4 + \n",
    "    comprehensive_df['Runtime_ms'].apply(lambda x: 100 / (1 + x)) * 20\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"OVERALL BEST DETECTORS (Across All Datasets)\")\n",
    "print(\"=\"*140)\n",
    "\n",
    "top_overall = comprehensive_df.groupby('Method')['Composite_Score'].mean().sort_values(ascending=False).head(5)\n",
    "print(\"\\nTop 5 by Composite Score:\")\n",
    "for rank, (method, score) in enumerate(top_overall.items(), 1):\n",
    "    method_data = comprehensive_df[comprehensive_df['Method'] == method]\n",
    "    print(f\"{rank}. {method}: Score={score:.2f}, Avg F1={method_data['F1'].mean():.3f}, \"\n",
    "          f\"Avg Recovery={method_data['Recovery_Rate_%'].mean():.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "\n",
    "# Calculate detection rate by dataset difficulty\n",
    "print(\"\\nDETECTION RATE BY DATASET:\")\n",
    "detection_by_dataset = comprehensive_df.groupby('Dataset_Name')['Detected'].agg(['sum', 'count', 'mean'])\n",
    "detection_by_dataset['detection_rate_%'] = detection_by_dataset['mean'] * 100\n",
    "print(detection_by_dataset[['sum', 'count', 'detection_rate_%']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 9: VISUALIZATION - PER-DATASET ACCURACY TIMELINE (ALL DATASETS)\n",
    "# ============================================================================\n",
    "\n",
    "if len(comprehensive_df) == 0 or len(all_comprehensive_results) == 0:\n",
    "    print(\"âš  No results to visualize - run experiments first\")\n",
    "else:\n",
    "    # Create visualizations for EACH dataset separately\n",
    "    unique_datasets = comprehensive_df['Dataset'].unique()\n",
    "    \n",
    "    print(f\"Generating timeline visualizations for {len(unique_datasets)} datasets...\")\n",
    "    \n",
    "    for dataset_name in unique_datasets:\n",
    "        dataset_data = [r for r in all_comprehensive_results if r['dataset'] == dataset_name]\n",
    "        \n",
    "        if len(dataset_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get dataset info\n",
    "        first_result = dataset_data[0]\n",
    "        dataset_display_name = first_result['dataset_name']\n",
    "        true_drift_point = first_result.get('true_drift_point', DRIFT_POSITION)\n",
    "        all_drift_positions = first_result.get('all_drift_positions', [true_drift_point])\n",
    "        n_drift_events = first_result.get('n_drift_events', 1)\n",
    "        \n",
    "        print(f\"\\n  Visualizing {dataset_display_name} ({n_drift_events} drift events)...\")\n",
    "        \n",
    "        n_detectors = len(dataset_data)\n",
    "        n_cols = 2\n",
    "        n_rows = (n_detectors + 1) // 2\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 4 * n_rows))\n",
    "        gs = fig.add_gridspec(n_rows, n_cols, hspace=0.35, wspace=0.25)\n",
    "        fig.suptitle(f'Accuracy Timeline Comparison - {dataset_display_name} ({n_drift_events} drifts)', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        \n",
    "        for idx, result in enumerate(dataset_data):\n",
    "            row = idx // n_cols\n",
    "            col = idx % n_cols\n",
    "            ax = fig.add_subplot(gs[row, col])\n",
    "            \n",
    "            # Extract accuracy timeline\n",
    "            acc_timeline = result.get('accuracy_timeline', [])\n",
    "            if len(acc_timeline) > 0:\n",
    "                # Handle both 'acc' and 'accuracy' keys for compatibility\n",
    "                acc_indices = np.array([item['idx'] for item in acc_timeline])\n",
    "                acc_values = np.array([item.get('acc', item.get('accuracy', 0)) for item in acc_timeline])\n",
    "                \n",
    "                # Plot accuracy\n",
    "                ax.plot(acc_indices, acc_values, linewidth=2, color='navy', alpha=0.8)\n",
    "                ax.fill_between(acc_indices, 0, acc_values, alpha=0.2, color='navy')\n",
    "                \n",
    "                # Mark ALL true drift positions\n",
    "                for drift_idx, true_drift in enumerate(all_drift_positions):\n",
    "                    ax.axvline(true_drift, color='red', linestyle='--', linewidth=2, \n",
    "                              label=f'True Drift {drift_idx+1}' if drift_idx == 0 else '', \n",
    "                              alpha=0.6)\n",
    "                \n",
    "                if result.get('drift_detected', False):\n",
    "                    detection_idx = result['detections'][0] if len(result.get('detections', [])) > 0 else None\n",
    "                    if detection_idx:\n",
    "                        ax.axvline(detection_idx, color='orange', linestyle=':', linewidth=2,\n",
    "                                  label=f'Detection (delay={detection_idx - true_drift_point})', alpha=0.7)\n",
    "                    \n",
    "                    if result.get('adaptation_idx'):\n",
    "                        ax.axvline(result['adaptation_idx'], color='green', linestyle='-', linewidth=2,\n",
    "                                  label='Adaptation', alpha=0.7)\n",
    "                \n",
    "                # Annotations\n",
    "                method_name = result['method']\n",
    "                paradigm = result['paradigm']\n",
    "                f1 = result.get('f1_score', 0)\n",
    "                recovery = result.get('recovery_rate', 0)\n",
    "                \n",
    "                title = f\"{method_name} ({paradigm})\\\\nF1={f1:.3f}, Recovery={recovery:.1f}%\"\n",
    "                ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "                ax.set_xlabel('Sample Index', fontsize=10)\n",
    "                ax.set_ylabel('Accuracy', fontsize=10)\n",
    "                ax.set_ylim([0, 1.05])\n",
    "                ax.legend(loc='lower left', fontsize=8)\n",
    "                ax.grid(alpha=0.3)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'{result[\"method\"]}\\\\nNo timeline data',\n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title(result['method'], fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ“ Per-dataset timeline visualizations complete ({len(unique_datasets)} datasets)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 10: VISUALIZATION - OVERALL DETECTION PERFORMANCE HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "if len(comprehensive_df) == 0:\n",
    "    print(\"âš  No results to visualize - run experiments first\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Prepare data\n",
    "    metrics_to_plot = ['Precision', 'Recall', 'F1', 'Recovery_Rate_%']\n",
    "    metric_labels = ['Precision', 'Recall', 'F1-Score', 'Recovery Rate (%)']\n",
    "    \n",
    "    # Group by method and take mean across datasets\n",
    "    method_metrics = comprehensive_df.groupby('Method')[metrics_to_plot].mean()\n",
    "    methods = method_metrics.index.tolist()\n",
    "    \n",
    "    if len(methods) == 0:\n",
    "        print(\"âš  No methods to visualize\")\n",
    "    else:\n",
    "        # Normalize recovery rate to 0-1 scale\n",
    "        data_matrix = method_metrics.values.copy()\n",
    "        data_matrix[:, 3] = data_matrix[:, 3] / 100\n",
    "        \n",
    "        # Create heatmap\n",
    "        im = ax.imshow(data_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "        \n",
    "        # Set ticks and labels\n",
    "        ax.set_xticks(range(len(metric_labels)))\n",
    "        ax.set_xticklabels(metric_labels, fontsize=11)\n",
    "        ax.set_yticks(range(len(methods)))\n",
    "        ax.set_yticklabels(methods, fontsize=10)\n",
    "        \n",
    "        # Add values as text\n",
    "        for i in range(len(methods)):\n",
    "            for j in range(len(metric_labels)):\n",
    "                value = data_matrix[i, j]\n",
    "                display_value = f'{value:.3f}' if j < 3 else f'{value*100:.1f}%'\n",
    "                text_color = 'white' if value < 0.5 else 'black'\n",
    "                ax.text(j, i, display_value, ha='center', va='center',\n",
    "                       color=text_color, fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Labels and title\n",
    "        ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Detection Methods', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Detection Performance Heatmap (Averaged Across All Datasets)',\n",
    "                    fontsize=14, fontweight='bold', pad=15)\n",
    "        \n",
    "        # Colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Score (0-1)', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ“ Detection performance heatmap created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 11: VISUALIZATION - RECOVERY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "if len(comprehensive_df) == 0:\n",
    "    print(\"âš  No results to visualize - run experiments first\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    colors_paradigm = {'window': '#2E86AB', 'streaming': '#A23B72'}\n",
    "    \n",
    "    # Average across datasets for each method\n",
    "    method_stats = comprehensive_df.groupby(['Method', 'Paradigm']).agg({\n",
    "        'Acc_Drop': 'mean',\n",
    "        'Recovery_Rate_%': 'mean', \n",
    "        'Recovery_Time': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    if len(method_stats) == 0:\n",
    "        print(\"âš  No method statistics available\")\n",
    "    else:\n",
    "        methods = method_stats['Method'].tolist()\n",
    "        paradigms = method_stats['Paradigm'].tolist()\n",
    "        bar_colors = [colors_paradigm.get(p, '#2E86AB') for p in paradigms]\n",
    "        \n",
    "        # Plot 1: Accuracy Drop\n",
    "        ax = axes[0]\n",
    "        bars = ax.bar(range(len(methods)), method_stats['Acc_Drop'], \n",
    "                     color=bar_colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_xlabel('Methods', fontsize=11)\n",
    "        ax.set_ylabel('Accuracy Drop', fontsize=11)\n",
    "        ax.set_title('Avg Accuracy Drop During Drift', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(range(len(methods)))\n",
    "        ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            if not np.isnan(height):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Plot 2: Recovery Rate\n",
    "        ax = axes[1]\n",
    "        bars = ax.bar(range(len(methods)), method_stats['Recovery_Rate_%'],\n",
    "                     color=bar_colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_xlabel('Methods', fontsize=11)\n",
    "        ax.set_ylabel('Recovery Rate (%)', fontsize=11)\n",
    "        ax.set_title('Avg Recovery Rate', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(range(len(methods)))\n",
    "        ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "        ax.axhline(100, color='green', linestyle='--', alpha=0.5, label='Full Recovery')\n",
    "        ax.set_ylim([0, 110])\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            if not np.isnan(height):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Plot 3: Recovery Time\n",
    "        ax = axes[2]\n",
    "        bars = ax.bar(range(len(methods)), method_stats['Recovery_Time'],\n",
    "                     color=bar_colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_xlabel('Methods', fontsize=11)\n",
    "        ax.set_ylabel('Recovery Time (samples)', fontsize=11)\n",
    "        ax.set_title('Avg Recovery Time (to 95% baseline)', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(range(len(methods)))\n",
    "        ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            if not np.isnan(height):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{int(height)}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=colors_paradigm['window'], label='Window-based', alpha=0.7),\n",
    "            Patch(facecolor=colors_paradigm['streaming'], label='Streaming', alpha=0.7)\n",
    "        ]\n",
    "        fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.98),\n",
    "                  ncol=2, fontsize=10, frameon=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ“ Recovery analysis visualization created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 12: VISUALIZATION - DETECTION TIMELINE COMPARISON (ALL DATASETS)\n",
    "# ============================================================================\n",
    "\n",
    "if len(comprehensive_df) == 0 or len(all_comprehensive_results) == 0:\n",
    "    print(\"âš  No results to visualize - run experiments first\")\n",
    "else:\n",
    "    # Show detection timelines for ALL datasets\n",
    "    unique_datasets = comprehensive_df['Dataset'].unique()\n",
    "    \n",
    "    print(f\"Generating detection timelines for {len(unique_datasets)} datasets...\")\n",
    "    \n",
    "    for dataset_name in unique_datasets:\n",
    "        dataset_results = [r for r in all_comprehensive_results if r['dataset'] == dataset_name]\n",
    "        \n",
    "        if len(dataset_results) == 0:\n",
    "            continue\n",
    "        \n",
    "        first_result = dataset_results[0]\n",
    "        true_drift_point = first_result.get('true_drift_point', DRIFT_POSITION)\n",
    "        all_drift_positions = first_result.get('all_drift_positions', [true_drift_point])\n",
    "        dataset_display_name = first_result['dataset_name']\n",
    "        n_drift_events = first_result.get('n_drift_events', 1)\n",
    "        \n",
    "        print(f\"\\n  Visualizing {dataset_display_name} ({n_drift_events} drift events)...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Plot ALL true drift positions as vertical lines\n",
    "        for drift_idx, drift_pos in enumerate(all_drift_positions):\n",
    "            label = f'True Drift {drift_idx+1}' if drift_idx == 0 else ''\n",
    "            ax.axvline(drift_pos, color='red', linestyle='--', linewidth=3, \n",
    "                      label=label if drift_idx == 0 else '', alpha=0.7, zorder=1)\n",
    "            # Add drift number annotation\n",
    "            ax.text(drift_pos, len(dataset_results) + 0.5, f'D{drift_idx+1}', \n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold', color='red')\n",
    "        \n",
    "        # Plot detections for each method\n",
    "        colors_paradigm = {'window': '#2E86AB', 'streaming': '#A23B72'}\n",
    "        method_names = []\n",
    "        y_positions = []\n",
    "        \n",
    "        for idx, result in enumerate(dataset_results):\n",
    "            method_name = f\"{result['method']}\"\n",
    "            method_names.append(method_name)\n",
    "            y_positions.append(idx)\n",
    "            \n",
    "            detections = result.get('detections', [])\n",
    "            paradigm = result.get('paradigm', 'window')\n",
    "            color = colors_paradigm.get(paradigm, '#2E86AB')\n",
    "            marker = 'o' if paradigm == 'window' else 's'\n",
    "            \n",
    "            if len(detections) > 0:\n",
    "                # Plot first detection (triggers adaptation)\n",
    "                ax.scatter(detections[0], idx, c=color, s=200, marker=marker, \n",
    "                          edgecolors='black', linewidth=2, alpha=0.8, zorder=3,\n",
    "                          label=paradigm if idx < 2 else '')\n",
    "                \n",
    "                # Plot additional detections (smaller)\n",
    "                if len(detections) > 1:\n",
    "                    ax.scatter(detections[1:], [idx]*len(detections[1:]), c=color, s=80, \n",
    "                              marker=marker, edgecolors='black', linewidth=1, alpha=0.5, zorder=2)\n",
    "                \n",
    "                # Calculate delay relative to PRIMARY drift (first drift)\n",
    "                delay = detections[0] - true_drift_point\n",
    "                delay_text = f\"delay: {delay:+d}\"\n",
    "                ax.text(detections[0] + 100, idx, delay_text, fontsize=8, \n",
    "                       va='center', color=color, fontweight='bold')\n",
    "        \n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(method_names, fontsize=10)\n",
    "        ax.set_xlabel('Sample Index', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Detection Method', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'Detection Timeline Comparison - {dataset_display_name} ({n_drift_events} drifts)', \n",
    "                    fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.grid(alpha=0.3, axis='x')\n",
    "        \n",
    "        # Legend\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        ax.legend(by_label.values(), by_label.keys(), loc='upper right', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ“ Detection timeline comparisons complete ({len(unique_datasets)} datasets)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NEW VISUALIZATION: CROSS-DATASET PERFORMANCE HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "if len(comprehensive_df) == 0:\n",
    "    print(\"âš  No results to visualize - run experiments first\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    \n",
    "    # Pivot data for heatmaps\n",
    "    datasets = comprehensive_df['Dataset_Name'].unique()\n",
    "    methods = comprehensive_df['Method'].unique()\n",
    "    \n",
    "    # Plot 1: F1-Score Heatmap\n",
    "    ax = axes[0, 0]\n",
    "    f1_pivot = comprehensive_df.pivot_table(values='F1', index='Method', columns='Dataset_Name', aggfunc='mean')\n",
    "    sns.heatmap(f1_pivot, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0, vmax=1, \n",
    "               cbar_kws={'label': 'F1-Score'}, ax=ax, linewidths=0.5)\n",
    "    ax.set_title('F1-Score Across Datasets', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Dataset', fontsize=11)\n",
    "    ax.set_ylabel('Method', fontsize=11)\n",
    "    \n",
    "    # Plot 2: Recovery Rate Heatmap\n",
    "    ax = axes[0, 1]\n",
    "    recovery_pivot = comprehensive_df.pivot_table(values='Recovery_Rate_%', index='Method', \n",
    "                                                 columns='Dataset_Name', aggfunc='mean')\n",
    "    sns.heatmap(recovery_pivot, annot=True, fmt='.1f', cmap='RdYlGn', vmin=0, vmax=100,\n",
    "               cbar_kws={'label': 'Recovery Rate (%)'}, ax=ax, linewidths=0.5)\n",
    "    ax.set_title('Recovery Rate Across Datasets', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Dataset', fontsize=11)\n",
    "    ax.set_ylabel('Method', fontsize=11)\n",
    "    \n",
    "    # Plot 3: Detection Rate by Dataset (Bar plot)\n",
    "    ax = axes[1, 0]\n",
    "    detection_by_dataset = comprehensive_df.groupby('Dataset_Name')['Detected'].agg(['sum', 'count'])\n",
    "    detection_by_dataset['rate'] = detection_by_dataset['sum'] / detection_by_dataset['count'] * 100\n",
    "    \n",
    "    bars = ax.bar(range(len(detection_by_dataset)), detection_by_dataset['rate'], \n",
    "                 color='steelblue', alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    ax.set_xlabel('Dataset', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Detection Rate (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Detection Success Rate by Dataset', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(range(len(detection_by_dataset)))\n",
    "    ax.set_xticklabels(detection_by_dataset.index, rotation=45, ha='right')\n",
    "    ax.axhline(100, color='green', linestyle='--', linewidth=2, alpha=0.5, label='Perfect Detection')\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, detection_by_dataset['rate'])):\n",
    "        ax.text(i, val + 2, f'{val:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Method Performance Radar (Average across datasets)\n",
    "    ax = axes[1, 1]\n",
    "    ax.remove()\n",
    "    ax = fig.add_subplot(2, 2, 4, projection='polar')\n",
    "    \n",
    "    # Select top 5 methods by composite score (handle if fewer methods exist)\n",
    "    composite_scores = comprehensive_df.groupby('Method')['Composite_Score'].mean().sort_values(ascending=False)\n",
    "    n_top = min(5, len(composite_scores))\n",
    "    top_methods = composite_scores.head(n_top).index\n",
    "    \n",
    "    if len(top_methods) > 0:\n",
    "        # Metrics for radar chart\n",
    "        metrics = ['F1', 'Recovery_Rate_%', 'Precision', 'Recall']\n",
    "        metric_labels = ['F1-Score', 'Recovery', 'Precision', 'Recall']\n",
    "        \n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        colors_radar = plt.cm.tab10(np.linspace(0, 1, len(top_methods)))\n",
    "        \n",
    "        for method, color in zip(top_methods, colors_radar):\n",
    "            method_data = comprehensive_df[comprehensive_df['Method'] == method]\n",
    "            \n",
    "            values = [\n",
    "                method_data['F1'].mean(),\n",
    "                method_data['Recovery_Rate_%'].mean() / 100,  # Normalize to 0-1\n",
    "                method_data['Precision'].mean(),\n",
    "                method_data['Recall'].mean()\n",
    "            ]\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=method, color=color, alpha=0.7)\n",
    "            ax.fill(angles, values, alpha=0.15, color=color)\n",
    "        \n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metric_labels, fontsize=10)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(f'Top {n_top} Methods Performance Profile\\n(Averaged Across All Datasets)', \n",
    "                    fontsize=12, fontweight='bold', pad=20)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No data for radar chart', ha='center', va='center',\n",
    "               transform=ax.transAxes, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Cross-dataset performance heatmap created\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
