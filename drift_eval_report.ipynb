{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Drift Detector Evaluation — Report Notebook\n", "\n", "_VN_: Notebook này **tự động** tạo pipeline, chạy thí nghiệm và tạo bảng/biểu đồ **đủ cho báo cáo**.\n", "- Detectors: ADWIN, DDM, EDDM, HDDM_A, HDDM_W, KSWIN, Page-Hinkley (River)\n", "- Datasets: SEA (synthetic, có ground-truth drift), Elec2 (real-world), RandomRBFDrift (gradual)\n", "- Chỉ số: Accuracy, F1, #alarms, β-score (SEA), delay (samples & %window), **F1@AR**, **Accuracy@AR** + Global scores\n", "\n", "_EN_: One-click River-based benchmark that outputs report-ready tables and plots."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 0) Install dependencies (safe to re-run)\n", "import sys, subprocess\n", "pkgs = ['river>=0.17','pandas>=2.0','numpy>=1.23','pyyaml>=6.0','pyarrow>=14.0','rich>=13.0']\n", "subprocess.check_call([sys.executable,'-m','pip','install','--quiet', *pkgs])\n", "print('Installed:', pkgs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 1) Bootstrap helper package: driftlab/\n", "from pathlib import Path\n", "import textwrap, os\n", "root = Path('driftlab')\n", "for p in [root/'src/driftlab', root/'results', root/'configs']: p.mkdir(parents=True, exist_ok=True)\n", "(root/'src/__init__.py').write_text('')\n", "(root/'src/driftlab/__init__.py').write_text('from . import datasets, detectors, metrics, runner\\n')\n", "\n", "(root/'src/driftlab/datasets.py').write_text(textwrap.dedent('''\\\n", "from __future__ import annotations\n", "from typing import Iterator, Dict, Any, List, Optional\n", "from dataclasses import dataclass\n", "from river import datasets as r_datasets\n", "\n", "@dataclass\n", "class StreamBatch:\n", "    x: dict\n", "    y: Any\n", "    t: int\n", "\n", "def iter_elec2(limit: Optional[int] = None) -> Iterator[StreamBatch]:\n", "    ds = r_datasets.Elec2()\n", "    for i,(x,y) in enumerate(ds):\n", "        if limit is not None and i >= limit: break\n", "        yield StreamBatch(x=x, y=y, t=i+1)\n", "\n", "def iter_sea(n_samples: int = 100000, drift_positions: List[int] = [30000, 60000], seed: int = 42):\n", "    positions = sorted(drift_positions)\n", "    segments = [positions[0]] + [positions[i]-positions[i-1] for i in range(1, len(positions))] + [n_samples-positions[-1]]\n", "    variants = [0, 1, 2, 3]\n", "    t = 0\n", "    for seg_len, var in zip(segments, variants):\n", "        sea = r_datasets.synth.SEA(variant=var, seed=seed + var)\n", "        for (x, y) in sea.take(seg_len):\n", "            t += 1\n", "            yield StreamBatch(x=x, y=y, t=t)\n", "\n", "def sea_ground_truth(n_samples: int, drift_positions: List[int]) -> Dict[str, Any]:\n", "    positions = sorted(drift_positions)\n", "    return {\"drift_times\": positions, \"window_len\": positions[0] if positions else n_samples}\n", "\n", "def iter_randomrbf(n_samples: int = 100000, speed: float = 0.87, n_centroids: int = 50, seed: int = 42):\n", "    gen = r_datasets.synth.RandomRBFDrift(seed_model=seed, seed_sample=seed+1, n_classes=2, n_features=10, n_centroids=n_centroids, change_speed=speed)\n", "    for i, (x, y) in enumerate(gen.take(n_samples)):\n", "        yield StreamBatch(x=x, y=y, t=i+1)\n", "\n", "def get_stream(name: str, **kwargs):\n", "    name = name.lower()\n", "    if name in (\"elec2\",\"electricity\"):\n", "        limit = kwargs.get(\"limit\", None)\n", "        return iter_elec2(limit=limit), {\"drift_times\": None, \"window_len\": None}\n", "    elif name == \"sea\":\n", "        n = int(kwargs.get(\"n_samples\", 100000))\n", "        drifts = kwargs.get(\"drift_positions\", [30000, 60000])\n", "        iterator = iter_sea(n_samples=n, drift_positions=drifts)\n", "        gt = sea_ground_truth(n, drifts)\n", "        return iterator, gt\n", "    elif name in (\"rbf\",\"randomrbf\",\"randomrbfdrift\"):\n", "        n = int(kwargs.get(\"n_samples\", 100000))\n", "        speed = float(kwargs.get(\"speed\", 0.87))\n", "        it = iter_randomrbf(n_samples=n, speed=speed)\n", "        return it, {\"drift_times\": None, \"window_len\": None}\n", "    else:\n", "        raise ValueError(f\"Unknown dataset: {name}\")\n", "'''))\n", "\n", "(root/'src/driftlab/detectors.py').write_text(textwrap.dedent('''\\\n", "from river.drift import ADWIN, DDM, EDDM, HDDM_A, HDDM_W, KSWIN, PageHinkley\n", "\n", "def make_detector(name: str, **kwargs):\n", "    n = name.lower()\n", "    return {\n", "        'adwin': ADWIN, 'ddm': DDM, 'eddm': EDDM,\n", "        'hddm_a': HDDM_A, 'hddm_w': HDDM_W, 'kswin': KSWIN,\n", "        'pagehinkley': PageHinkley,\n", "    }[n](**kwargs)\n", "\n", "def update_supervised(det, error01: int) -> bool:\n", "    det.update(error01)\n", "    return bool(det.change_detected)\n", "\n", "def update_univariate(det, value: float) -> bool:\n", "    det.update(value)\n", "    return bool(det.change_detected)\n", "'''))\n", "\n", "(root/'src/driftlab/metrics.py').write_text(textwrap.dedent('''\\\n", "from __future__ import annotations\n", "from typing import List, Optional, Dict, Any\n", "import numpy as np\n", "\n", "def match_alarms_to_drifts(alarms: List[int], drifts: List[int], max_delay: Optional[int]=None) -> Dict[str, Any]:\n", "    alarms = sorted(alarms); drifts = sorted(drifts)\n", "    tp=fp=fn=0; delays=[]; used=set(); j=0\n", "    for dt in drifts:\n", "        while j < len(alarms) and alarms[j] < dt: j+=1\n", "        if j < len(alarms):\n", "            d = alarms[j]-dt\n", "            if max_delay is None or d <= max_delay:\n", "                tp+=1; delays.append(d); used.add(j); j+=1\n", "            else:\n", "                fn+=1\n", "        else:\n", "            fn+=1\n", "    fp = len([a for idx,a in enumerate(alarms) if idx not in used])\n", "    return {\"tp\":tp,\"fp\":fp,\"fn\":fn,\"delays\":delays}\n", "\n", "def beta_score(tp:int, fp:int, p:int, beta:float=0.5) -> float:\n", "    return float('nan') if p<=0 else tp / (p + beta*fp)\n", "\n", "def delay_stats(delays: List[int]) -> Dict[str,float]:\n", "    if not delays: return {\"mean\":float('nan'),\"median\":float('nan'),\"min\":float('nan'),\"max\":float('nan')}\n", "    arr = np.array(delays, dtype=float)\n", "    return {\"mean\":float(arr.mean()), \"median\":float(np.median(arr)), \"min\":float(arr.min()), \"max\":float(arr.max())}\n", "'''))\n", "\n", "(root/'src/driftlab/runner.py').write_text(textwrap.dedent('''\\\n", "from __future__ import annotations\n", "from dataclasses import dataclass\n", "from typing import Dict, Any, List, Optional\n", "import math\n", "from river import tree, metrics as r_metrics\n", "from .datasets import get_stream\n", "from .detectors import make_detector, update_supervised, update_univariate\n", "from .metrics import match_alarms_to_drifts, beta_score, delay_stats\n", "\n", "@dataclass\n", "class RunConfig:\n", "    dataset: str\n", "    detector: str\n", "    classifier: str = 'HT'\n", "    n_samples: int = 100000\n", "    sea_drifts: Optional[List[int]] = None\n", "    rbf_speed: float = 0.87\n", "    seed: int = 42\n", "    max_delay: Optional[int] = None\n", "    beta: float = 0.5\n", "\n", "def make_classifier(name:str):\n", "    return tree.HoeffdingTreeClassifier()\n", "\n", "def run_once(cfg: RunConfig) -> Dict[str, Any]:\n", "    if cfg.dataset.lower()=='sea':\n", "        iterator, gt = get_stream('sea', n_samples=cfg.n_samples, drift_positions=cfg.sea_drifts or [30000,60000])\n", "    elif cfg.dataset.lower() in ('rbf','randomrbf','randomrbfdrift'):\n", "        iterator, gt = get_stream('randomrbf', n_samples=cfg.n_samples, speed=cfg.rbf_speed)\n", "    elif cfg.dataset.lower() in ('elec2','electricity'):\n", "        iterator, gt = get_stream('elec2', limit=cfg.n_samples)\n", "    else:\n", "        raise ValueError('Unsupported dataset')\n", "\n", "    model = make_classifier(cfg.classifier)\n", "    det = make_detector(cfg.detector)\n", "\n", "    acc = r_metrics.Accuracy(); f1 = r_metrics.F1()\n", "    alarms: List[int] = []; n=0\n", "\n", "    for batch in iterator:\n", "        n += 1\n", "        x, y = batch.x, batch.y\n", "\n", "        y_pred = model.predict_one(x)\n", "        prob_pred = None\n", "        if hasattr(model,'predict_proba_one') and y_pred is not None:\n", "            proba_dict = model.predict_proba_one(x)\n", "            prob_pred = proba_dict.get(y_pred, None)\n", "\n", "        if cfg.detector.lower()=='kswin' and prob_pred is not None:\n", "            if update_univariate(det, float(prob_pred)): alarms.append(n)\n", "        else:\n", "            err01 = 1 if (y_pred is None or y_pred != y) else 0\n", "            if update_supervised(det, err01): alarms.append(n)\n", "\n", "        if y_pred is not None:\n", "            acc.update(y_true=y, y_pred=y)\n", "            f1.update(y_true=y, y_pred=y)\n", "\n", "        model.learn_one(x, y)\n", "        if cfg.n_samples and n >= cfg.n_samples: break\n", "\n", "    out: Dict[str, Any] = {\n", "        'dataset': cfg.dataset,\n", "        'detector': cfg.detector,\n", "        'classifier': cfg.classifier,\n", "        'n_seen': n,\n", "        'alarms': len(alarms),\n", "        'acc': float(acc.get() or 0.0),\n", "        'f1': float(f1.get() or 0.0),\n", "        'alarm_times': alarms,\n", "    }\n", "\n", "    if gt.get('drift_times'):\n", "        res = match_alarms_to_drifts(alarms, gt['drift_times'], cfg.max_delay)\n", "        out.update({'tp':res['tp'],'fp':res['fp'],'fn':res['fn'],'delays':res['delays']})\n", "        p = len(gt['drift_times'])\n", "        out['beta_score'] = float(beta_score(res['tp'], res['fp'], p, beta=cfg.beta))\n", "        dstats = delay_stats(res['delays']) if res['delays'] else {'mean': float('nan')}\n", "        out.update({f'delay_{k}': v for k,v in dstats.items()})\n", "        if gt.get('window_len') and 'mean' in dstats and not math.isnan(dstats['mean']):\n", "            out['delay_percent_mean'] = 100.0 * dstats['mean'] / float(gt['window_len'])\n", "    return out\n", "'''))\n", "\n", "print('Bootstrap done in', root.resolve())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 2) Run batch experiments (SEA + Elec2 + RBF)\n", "import sys\n", "sys.path.append('driftlab/src')\n", "from driftlab.runner import RunConfig, run_once\n", "import pandas as pd\n", "\n", "jobs=[]\n", "detectors=['adwin','ddm','eddm','hddm_a','hddm_w','kswin','pagehinkley']\n", "\n", "# SEA (ground-truth drifts)\n", "for det in detectors:\n", "    res = run_once(RunConfig(dataset='sea', detector=det, n_samples=60000, sea_drifts=[20000,40000], beta=0.5, max_delay=5000))\n", "    res['context']='sea'; jobs.append(res)\n", "\n", "# Elec2 (real-world)\n", "for det in detectors:\n", "    res = run_once(RunConfig(dataset='elec2', detector=det, n_samples=30000))\n", "    res['context']='elec2'; jobs.append(res)\n", "\n", "# RandomRBFDrift (gradual)\n", "for det in detectors:\n", "    res = run_once(RunConfig(dataset='rbf', detector=det, n_samples=50000, rbf_speed=0.8))\n", "    res['context']='rbf'; jobs.append(res)\n", "\n", "df = pd.DataFrame(jobs)\n", "df.head(10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 3) Save raw results (CSV/Parquet)\n", "from pathlib import Path\n", "out = Path('driftlab/results/notebook_runs')\n", "out.mkdir(parents=True, exist_ok=True)\n", "df.to_csv(out/'combined.csv', index=False)\n", "try:\n", "    df.to_parquet(out/'combined.parquet', index=False)\n", "except Exception as e:\n", "    print('Parquet skipped:', e)\n", "out.resolve()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Visual summaries (matplotlib)\n", "- SEA: β-score, mean delay (% of first segment)\n", "- Elec2: Accuracy\n", "- RBF: F1\n", "\n", "_Rule_: one chart per figure; no manual colors."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "sea = df[df['context']=='sea'].copy().sort_values('beta_score', ascending=False)\n", "plt.figure(); plt.bar(sea['detector'], sea['beta_score'].fillna(0)); plt.title('SEA: β-score'); plt.xlabel('Detector'); plt.ylabel('β-score'); plt.xticks(rotation=45); plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(); plt.bar(sea['detector'], sea['delay_percent_mean'].fillna(0)); plt.title('SEA: mean delay (%)'); plt.xlabel('Detector'); plt.ylabel('delay %'); plt.xticks(rotation=45); plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["elec = df[df['context']=='elec2'].copy().sort_values('acc', ascending=False)\n", "plt.figure(); plt.bar(elec['detector'], elec['acc'].fillna(0)); plt.title('Elec2: Accuracy'); plt.xlabel('Detector'); plt.ylabel('Accuracy'); plt.xticks(rotation=45); plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rbf = df[df['context']=='rbf'].copy().sort_values('f1', ascending=False)\n", "plt.figure(); plt.bar(rbf['detector'], rbf['f1'].fillna(0)); plt.title('RBF: F1'); plt.xlabel('Detector'); plt.ylabel('F1'); plt.xticks(rotation=45); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Aggregate metrics: F1@AR & Accuracy@AR (plus normalized Global Scores)\n", "**Alarm Rate** per 10k samples: $AR = (\\#\\text{alarms}/N)\\times 10^4$.  \n", "**F1@AR**: $F1 - \\lambda \\cdot AR$ (default $\\lambda=0.01$).  \n", "**Accuracy@AR**: $Acc - \\lambda_{acc} \\cdot AR$ (default $\\lambda_{acc}=0.01$).  \n", "Chuẩn hoá min–max riêng trên từng dataset, sau đó macro-average qua datasets → GlobalScore."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "LAMBDA_F1 = 0.01\n", "LAMBDA_ACC = 0.01\n", "\n", "dfm = df.copy()\n", "dfm['AR_per10k'] = dfm['alarms'] / dfm['n_seen'] * 1e4\n", "dfm['F1@AR'] = dfm['f1'] - LAMBDA_F1 * dfm['AR_per10k']\n", "dfm['Acc@AR'] = dfm['acc'] - LAMBDA_ACC * dfm['AR_per10k']\n", "\n", "def _minmax(s):\n", "    mx, mn = s.max(), s.min()\n", "    return (s - mn) / (mx - mn) if mx > mn else 1.0\n", "\n", "dfm['F1@AR_norm']  = dfm.groupby('dataset')['F1@AR'].transform(_minmax)\n", "dfm['Acc@AR_norm'] = dfm.groupby('dataset')['Acc@AR'].transform(_minmax)\n", "\n", "global_f1ar  = dfm.groupby('detector')['F1@AR_norm'].mean().sort_values(ascending=False).rename('GlobalScore_F1@AR')\n", "global_accar = dfm.groupby('detector')['Acc@AR_norm'].mean().sort_values(ascending=False).rename('GlobalScore_Acc@AR')\n", "\n", "display(global_f1ar.to_frame())\n", "display(global_accar.to_frame())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 6) Export report tables\n", "from pathlib import Path\n", "report_dir = Path('driftlab/results/report')\n", "report_dir.mkdir(parents=True, exist_ok=True)\n", "df.to_csv(report_dir/'raw_combined.csv', index=False)\n", "dfm.to_csv(report_dir/'with_F1@AR_and_Acc@AR.csv', index=False)\n", "global_f1ar.to_csv(report_dir/'global_F1@AR_scores.csv')\n", "global_accar.to_csv(report_dir/'global_Acc@AR_scores.csv')\n", "print('Saved to', report_dir.resolve())"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}