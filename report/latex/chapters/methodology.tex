\chapter{PHƯƠNG PHÁP NGHIÊN CỨU}

\section{Cách tiếp cận nghiên cứu}

This chapter presents the methodological framework for investigating concept drift detection using the Shape Drift Detector (ShapeDD) method. Our approach focuses on understanding the theoretical foundations of ShapeDD, implementing the algorithm for various drift scenarios, and conducting comprehensive empirical evaluation to assess its effectiveness across different types of concept drift.

The research methodology consists of three main components: (1) theoretical analysis of the ShapeDD algorithm and its underlying Maximum Mean Discrepancy (MMD) foundations, (2) implementation and optimization of the detection system for synthetic and real-world datasets, and (3) comprehensive experimental evaluation across different drift patterns and parameter configurations.

\section{Theoretical Foundations of Shape Drift Detector}

\subsection{Maximum Mean Discrepancy (MMD)}

The Shape Drift Detector (ShapeDD) is fundamentally based on Maximum Mean Discrepancy (MMD), a statistical measure used to compare two probability distributions $P$ and $Q$. The core idea is to map data from the original space into a high-dimensional feature space where the comparison becomes more sensitive to distributional differences.

MMD is formally defined as:
\begin{equation}
\text{MMD}(P, Q) = \sup_{f \in \mathcal{F}} \left| \mathbb{E}_{X \sim P}[f(X)] - \mathbb{E}_{Y \sim Q}[f(Y)] \right|
\end{equation}

where:
\begin{itemize}
    \item $P$ and $Q$ are the two distributions to be compared
    \item $X \sim P$ represents a random variable sampled from distribution $P$
    \item $Y \sim Q$ represents a random variable sampled from distribution $Q$
    \item $\mathcal{F}$ is a class of functions $f$ such that $\|f\|_{\mathcal{H}} \leq 1$ in the Reproducing Kernel Hilbert Space (RKHS)
    \item $\sup$ denotes the supremum (least upper bound)
\end{itemize}

In practice, finding the supremum over $\mathcal{F}$ is computationally intractable. Therefore, MMD is typically implemented in RKHS using the kernel trick with a kernel function $k(x, y)$:

\begin{equation}
k(x, y) = \langle \phi(x), \phi(y) \rangle_{\mathcal{H}}
\end{equation}

where $\phi(x)$ maps point $x$ into RKHS $\mathcal{H}$ and $\langle \cdot, \cdot \rangle_{\mathcal{H}}$ is the inner product in $\mathcal{H}$.

The squared MMD in RKHS becomes:
\begin{equation}
\text{MMD}^2(P, Q) = \mathbb{E}_{X, X' \sim P}[k(X, X')] + \mathbb{E}_{Y, Y' \sim Q}[k(Y, Y')] - 2\mathbb{E}_{X \sim P, Y \sim Q}[k(X, Y)]
\end{equation}

For empirical estimation with samples $\{x_i\}_{i=1}^n$ from $P$ and $\{y_j\}_{j=1}^m$ from $Q$:

\begin{equation}
\widehat{\text{MMD}}^2 = \frac{1}{n(n-1)} \sum_{i \neq j} k(x_i, x_j) + \frac{1}{m(m-1)} \sum_{i \neq j} k(y_i, y_j) - \frac{2}{nm} \sum_{i,j} k(x_i, y_j)
\end{equation}

\subsection{Shape Drift Detector Algorithm}

The Shape Drift Detector (ShapeDD) is a meta-statistic-based drift detector that operates through a multi-stage process to identify concept drift in data streams. The algorithm employs MMD as its core statistical measure and follows a systematic approach consisting of four main stages.

\subsubsection{Stage 1: Data Collection}

The first stage involves collecting data using sliding window techniques. Multiple window strategies can be employed:

\begin{itemize}
    \item \textbf{Fixed-size sliding windows}: Maintain a constant window size $w$ that slides over the data stream
    \item \textbf{Adaptive windows}: Dynamically adjust window size based on data characteristics
    \item \textbf{Overlapping windows}: Use overlapping segments to ensure smooth transitions
\end{itemize}

For a data stream $\mathcal{S} = \{x_1, x_2, \ldots, x_n\}$, we maintain a sliding window $W_t$ of size $l_1$ at time $t$:
\begin{equation}
W_t = \{x_{t-l_1+1}, x_{t-l_1+2}, \ldots, x_t\}
\end{equation}

\subsubsection{Stage 2: Feature Construction}

In this stage, we construct a similarity matrix using a kernel function to capture the relationships between data points. The Gaussian RBF kernel is commonly used:

\begin{equation}
k(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)
\end{equation}

This results in a kernel matrix $K \in \mathbb{R}^{n \times n}$ where $K_{ij} = k(x_i, x_j)$ represents the similarity between data points $x_i$ and $x_j$.

\subsubsection{Stage 3: Difference Computation}

The core of ShapeDD involves computing the statistical difference between consecutive data segments using MMD. We define a weight function $w(t)$ that creates contrasting weights for different halves of the sliding window:

\begin{equation}
w(t) = \begin{cases}
\frac{1}{l_1} & \text{if } t \in [1, l_1] \\
-\frac{1}{l_1} & \text{if } t \in [l_1+1, 2l_1]
\end{cases}
\end{equation}

The MMD statistic is then computed as:
\begin{equation}
\text{MMD}^2_t = \sum_{i,j=1}^{2l_1} w_i w_j K_{ij}
\end{equation}

This computation is performed across the entire data stream using a sliding window approach, resulting in a sequence of MMD values $\{\text{MMD}^2_1, \text{MMD}^2_2, \ldots, \text{MMD}^2_T\}$.

\subsubsection{Stage 4: Statistical Validation}

The final stage involves normalizing the MMD statistics and identifying potential change points through zero-crossing detection. The shape values are computed using convolution:

\begin{equation}
\text{shape\_values}_t = \sum_{i} \text{MMD}^2_{t+i} \cdot h_i
\end{equation}

where $h$ is a convolution kernel (typically $[1, -1]$ for simple edge detection).

Potential change points are identified where consecutive shape values have opposite signs. These candidates are then validated using permutation tests to compute p-values and determine statistical significance.

\section{Enhanced Drift Detection Algorithms}

\subsection{ShapeDD Implementation Details}

The complete ShapeDD algorithm can be summarized in the following steps:

\textbf{Algorithm: Shape Drift Detector (ShapeDD)}
\begin{enumerate}
    \item \textbf{Initialize parameters:} Set window size $l_1$, kernel bandwidth $\sigma$, significance threshold $\alpha$
    \item \textbf{Data collection:} Maintain sliding window $W_t$ of size $2l_1$
    \item \textbf{Kernel computation:} Compute similarity matrix $K$ using Gaussian RBF kernel
    \item \textbf{MMD calculation:} Apply weight function and compute MMD statistics across the sliding window
    \item \textbf{Shape analysis:} Apply convolution to identify potential change points via zero-crossing
    \item \textbf{Statistical validation:} Use permutation tests to validate detected change points with p-values
    \item \textbf{Drift signal:} Output drift detection signal when p-value $< \alpha$
\end{enumerate}

\textbf{Computational Complexity:}
\begin{itemize}
    \item Kernel matrix computation: $O(n^2)$ where $n$ is window size
    \item MMD calculation: $O(n^2)$ per sliding window position
    \item Permutation testing: $O(k \cdot n^2)$ where $k$ is number of permutations
    \item Overall complexity: $O(T \cdot n^2)$ for stream of length $T$
\end{itemize}

\section{Adaptation Strategy Framework}

\subsection{Meta-Learning Approach}

We develop a meta-learning framework that automatically selects adaptation strategies based on detected drift characteristics:

\textbf{Feature Extraction:} For each detected drift episode, we extract features describing:
\begin{itemize}
    \item Drift magnitude: $|\Delta(t_1, t_2)|$
    \item Drift speed: $\frac{|\Delta(t_1, t_2)|}{t_2 - t_1}$
    \item Affected dimensions: Number of features showing significant change
    \item Historical context: Previous drift patterns and adaptation outcomes
\end{itemize}

\textbf{Strategy Selection:} A meta-classifier trained on historical drift episodes predicts the most suitable adaptation strategy:

\begin{equation}
s^* = \arg\max_{s \in \mathcal{S}} P(s|\mathbf{f}_{\text{drift}})
\end{equation}

where $\mathbf{f}_{\text{drift}}$ represents the extracted drift features and $\mathcal{S}$ is the set of available adaptation strategies.

\subsection{Adaptive Window Management}

We propose an adaptive window management strategy that adjusts window size based on drift characteristics:

\begin{equation}
w_{\text{size}}(t) = w_{\text{base}} \cdot \exp(-\lambda \cdot \Delta(t))
\end{equation}

where $w_{\text{base}}$ is the base window size, $\lambda$ is a decay parameter, and $\Delta(t)$ is the detected drift magnitude.

\section{Experimental Design}

\subsection{Synthetic Dataset Generation}

To evaluate the effectiveness of ShapeDD across different drift scenarios, we generate controlled synthetic datasets with precisely defined drift characteristics. This approach allows us to assess algorithm performance under known conditions and analyze the impact of various parameters.

\textbf{Abrupt Drift Dataset:} We generate datasets with sudden, instantaneous changes in data distribution. The dataset consists of 10,000 data points with uniform sampling within a unit space. Drift is introduced by shifting the distribution parameters at randomly selected time points:

\begin{itemize}
    \item Dataset size: 10,000 data points
    \item Drift magnitude: 0.5 (standard deviation shift)
    \item Number of drift points: 10 randomly distributed locations
    \item Distribution type: Uniform distribution with sudden parameter shifts
\end{itemize}

\textbf{Incremental Drift Dataset:} We create datasets exhibiting gradual, continuous changes in distribution parameters over time. This represents slow-evolving drift patterns commonly found in real-world applications:

\begin{itemize}
    \item Dataset size: 10,000 data points  
    \item Drift progression: Continuous linear parameter evolution
    \item Distribution type: Gaussian or uniform with gradually changing parameters
    \item Drift speed: Configurable rate of parameter change per time unit
\end{itemize}

The synthetic data generation process ensures reproducibility and allows for systematic evaluation of detection performance across varying drift intensities and patterns.

\textbf{Parameter Variations:} For each drift type, we generate multiple dataset variants with different:
\begin{itemize}
    \item Drift magnitudes (0.1, 0.3, 0.5, 0.7, 0.9)
    \item Dimensionalities (2D, 5D, 10D, 20D)
    \item Noise levels (0\%, 5\%, 10\%, 15\%, 20\%)
    \item Drift frequencies (sparse vs. frequent drift occurrences)
\end{itemize}

\subsection{Evaluation Protocol}

\textbf{Prequential Evaluation:} We use the test-then-train approach for realistic streaming evaluation:

\textbf{Evaluation Protocol Steps:}
\begin{enumerate}
    \item Initialize model $M$ and performance metrics
    \item For each data point $(x_t, y_t)$ in the stream:
    \begin{enumerate}
        \item Make prediction: $\hat{y}_t \leftarrow M.\text{predict}(x_t)$
        \item Update performance metrics with $(\hat{y}_t, y_t)$
        \item Update model: $M.\text{update}(x_t, y_t)$
        \item Apply drift detection and adaptation if needed
    \end{enumerate}
    \item Report cumulative performance metrics
\end{enumerate}

\textbf{Performance Metrics:} We employ multiple metrics to assess different aspects of performance:

\begin{itemize}
    \item \textit{Classification accuracy}: Overall prediction accuracy
    \item \textit{Detection delay}: Time between drift occurrence and detection
    \item \textit{False alarm rate}: Frequency of incorrect drift signals
    \item \textit{Adaptation efficiency}: Performance recovery after drift
    \item \textit{Computational cost}: Runtime and memory requirements
\end{itemize}

\subsection{Statistical Analysis}

\textbf{Significance Testing:} We use appropriate statistical tests to verify the significance of performance differences:

\begin{itemize}
    \item Friedman test for comparing multiple algorithms across datasets
    \item Nemenyi post-hoc test for pairwise comparisons
    \item McNemar test for comparing classification accuracies
\end{itemize}

\textbf{Effect Size Analysis:} Beyond statistical significance, we compute effect sizes to assess practical significance:

\begin{equation}
\text{Cohen's d} = \frac{\mu_1 - \mu_2}{\sqrt{\frac{\sigma_1^2 + \sigma_2^2}{2}}}
\end{equation}

\section{Implementation Details}

\subsection{Software Framework}

We develop a comprehensive software framework for concept drift experimentation:

\textbf{Core Components:}
\begin{itemize}
    \item Stream simulation engine for synthetic data generation
    \item Modular drift detection library with pluggable algorithms
    \item Adaptation strategy framework with multiple implemented strategies
    \item Comprehensive evaluation suite with multiple metrics
\end{itemize}

\textbf{Technical Stack:}
\begin{itemize}
    \item Python 3.8+ with NumPy, SciPy, and scikit-learn
    \item Apache Kafka for stream processing simulation
    \item PostgreSQL for result storage and analysis
    \item Jupyter notebooks for visualization and analysis
\end{itemize}

\subsection{Computational Infrastructure}

\textbf{Hardware Requirements:}
\begin{itemize}
    \item Multi-core processors for parallel experiment execution
    \item Sufficient RAM for large-scale dataset processing
    \item Storage capacity for experimental results and datasets
\end{itemize}

\textbf{Experiment Management:}
\begin{itemize}
    \item Version control for experimental configurations
    \item Automated experiment scheduling and execution
    \item Result reproducibility through random seed management
\end{itemize}

\section{Validation Strategy}

\subsection{Cross-validation for Drift Detection}

Traditional cross-validation is not suitable for temporal data with drift. We employ temporal cross-validation:

\begin{itemize}
    \item \textit{Sliding window validation}: Use overlapping temporal windows
    \item \textit{Blocked cross-validation}: Respect temporal ordering in folds
    \item \textit{Prequential validation}: Test-then-train approach
\end{itemize}

\subsection{Robustness Testing}

\textbf{Noise Sensitivity:} Test algorithm performance under different noise levels:

\begin{equation}
x_{\text{noisy}} = x + \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
\end{equation}

\textbf{Parameter Sensitivity:} Analyze performance across different parameter settings using grid search and sensitivity analysis.

\textbf{Scalability Testing:} Evaluate computational performance on datasets of varying sizes and dimensionalities.

\section{Ethical Considerations}

\subsection{Data Privacy}

All real-world datasets used in this research are either publicly available or properly anonymized. We ensure compliance with relevant data protection regulations.

\subsection{Reproducibility}

We commit to making our code, datasets, and experimental configurations publicly available to enable reproducibility and facilitate future research.

\section{Summary}

This chapter has outlined the comprehensive methodology employed in this research. Our approach combines theoretical development with practical algorithm design and rigorous empirical evaluation. The next chapter presents the results of applying this methodology to investigate concept drift detection and adaptation across multiple domains and scenarios. 
