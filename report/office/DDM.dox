The Drift Detection Method (DDM), introduced by Gama et al., provides a fundamental approach to address concept drift in online learning environments where the underlying class-probability distribution of examples can change over time. A central idea in DDM is the concept of a "context," defined as a set of contiguous examples where the data distribution is stationary. The method's core principle is to monitor the online error-rate (p_i) of the learning algorithm. Statistical theory dictates that for a stationary distribution, this error rate should consistently decrease as more examples are processed. Therefore, a significant increase in the error rate signals a change in the class distribution, indicating that the current predictive model may no longer be appropriate. DDM maintains two key registers: p_min and s_min (minimum error rate and its standard deviation, respectively), updating them whenever p_i + s_i is lower than their current minimum sum. To facilitate adaptation, DDM defines two thresholds: a warning level (e.g., p_i + s_i >= p_min + 2*s_min) and a drift level (e.g., p_i + s_i >= p_min + 3*s_min), corresponding to specified confidence levels (e.g., 95% and 99% respectively).

When the error rate sequence reaches the warning level at an example k_w and subsequently the drift level at k_d, DDM declares a new context beginning at k_w and triggers the induction of a new decision model using only the examples from k_w to k_d. Importantly, transient increases in error that return below the warning level are treated as false alarms, preventing unnecessary model revisions. A key strength of DDM is its independence from the specific learning algorithm being used; it can be implemented as a wrapper around batch learners or integrated directly into online/incremental algorithms. The method essentially automates the selection of the most appropriate training set to reflect the current class-distribution. Experimental evaluations across a variety of artificial datasets (simulating abrupt and gradual drift, noise, and irrelevant attributes) and a real-world electricity dataset demonstrated DDM's effectiveness in improving the generalization capacity of diverse learning algorithms, including Perceptrons, Neural Networks, and Decision Trees. Notably, on the Electricity dataset, DDM's 1-day prediction error rate matched the optimal lower bound, showcasing its robust performance in real-world scenarios. Furthermore, DDM proved resilient to false alarms when applied to a stable concept dataset like ADULT.
