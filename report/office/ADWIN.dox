In the domain of concept drift detection, the Adaptive Windowing (ADWIN) algorithm, introduced by Bifet and Gavaldà, offers a novel approach to learning from time-changing data by employing sliding windows whose size is recomputed online based on the observed rate of change. This adaptive nature is a significant departure from traditional fixed-size windows, which force users into a dilemma of choosing between rapid adaptability to changes (small window) or higher accuracy during stable periods (large window). ADWIN aims to eliminate the need for users to pre-guess the time-scale of change. The core mechanism involves maintaining a window of bits or real numbers and dynamically adjusting its length. When two "large enough" subwindows within the main window exhibit "distinct enough" averages, it signifies that the underlying expected values are different, leading to the older portion of the window being dropped. This process ensures that the window primarily contains data consistent with the current underlying distribution. The algorithm provides rigorous theoretical guarantees on its performance, including bounds on false positive and false negative rates, and for certain change structures, it can be formally shown to adjust its window size to a statistically optimal length. The statistical test employed for comparing subwindow averages initially uses the Hoeffding bound and later a more sensitive test based on the normal approximation (derived from the Bernstein bound) for calculating the ε_cut threshold. This threshold ensures robust detection of significant differences while managing multiple hypothesis testing issues.

To address the computational inefficiency of the initial ADWIN version, ADWIN2 was developed, leveraging ideas from data-stream algorithmics to operate with low memory and time requirements. ADWIN2 maintains a window of length W with O(log W) memory and amortized O(1) (worst-case O(log W)) update time, making it suitable for high-speed data streams. It achieves this by using a variation of exponential histograms, allowing exact counts for approximately O(log W) subwindows. ADWIN can be flexibly incorporated into learning algorithms in two ways: externally, by monitoring the model's error rate to trigger revision, or internally, by directly maintaining updated statistics (like counts or averages) for the learning algorithm itself. Experimental validation on synthetic data streams, including abrupt and gradual changes, demonstrates ADWIN2's superior ability to track unknown quantities and estimate probabilities, often outperforming fixed-size windows and other variable-size window methods. When combined with learning algorithms like Naive Bayes and k-means, ADWIN2 significantly enhances their accuracy and adaptability to time-changing data, particularly when embedded internally to provide continuously updated statistics. Its robustness against false alarms in stable concepts has also been demonstrated.
