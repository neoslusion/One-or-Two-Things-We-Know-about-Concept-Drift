Addressing the compounded challenges of class imbalance and concept drift in streaming data, Mirza et al. propose the Ensemble of Subset Online Sequential Extreme Learning Machine (ESOS-ELM). This computationally efficient framework is specifically designed to handle dynamic data streams where both the class distribution and underlying concepts may change over time. ESOS-ELM features a modular architecture comprising a main ensemble, an ELM-Store module, and a change detection mechanism. The main ensemble acts as a short-term memory, consisting of multiple Online Sequential Extreme Learning Machine (OS-ELM) networks. To counter class imbalance, each OS-ELM in the ensemble is trained with balanced subsets of the data stream: minority class samples are processed by a number of classifiers proportional to the imbalance ratio, while majority class samples are distributed in a round-robin fashion. The ensemble utilizes dynamic weighted majority voting, where classifier weights are updated based on Gmean, a performance measure designed to provide a balanced evaluation for imbalanced datasets, ensuring appropriate reaction to changes in both classes.

The ELM-Store module serves as the long-term memory, implementing a novel and computationally efficient information storage scheme based on ELM theory to leverage prior knowledge of recurring concepts. Unlike methods that store old minority class samples or entire hypotheses, ELM-Store retains information in fixed-size temporary matrices whose dimensions depend only on the number of hidden neurons, not the number of samples, thus making it highly memory-efficient. Batch classifiers (WELMs) are trained and stored in ELM-Store only when a concept drift is detected, preventing rapid growth in the number of stored hypotheses. If a stored hypothesis later outperforms the current ensemble, it can be re-introduced, allowing the system to adapt faster to recurring concepts without relearning from scratch. The change detection mechanism within ESOS-ELM promptly reacts to both sudden and gradual drifts. Sudden drifts trigger a potential pause in older classifier voting and the training of a new WELM if ensemble Gmean falls below a threshold, while gradual drifts are detected using a statistical decision theory approach based on Gmean values. Experiments show that ESOS-ELM significantly improves performance over state-of-the-art methods like WOS-ELM in non-stationary environments and Learn++:NIE in chunk-by-chunk learning, demonstrating faster adaptation to recurring concepts with fewer stored hypotheses. Furthermore, ESOS-ELM also outperforms other methods in stationary imbalanced environments, showcasing its versatility and robustness in handling class imbalance in both one-by-one and chunk-by-chunk learning modes.
